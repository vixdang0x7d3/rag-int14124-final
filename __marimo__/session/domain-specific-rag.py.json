{
  "version": "1",
  "metadata": {
    "marimo_version": "0.13.15"
  },
  "cells": [
    {
      "id": "Hbol",
      "code_hash": "b03ae3d1a8c7e35b57a5d71425064bb8",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "MJUe",
      "code_hash": "96379d387ddac03d5f7158a0722bdd34",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"machine-specs\">Machine Specs</h1></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "vblA",
      "code_hash": "3b9a883b366809ce5b72bdfd72d20f4e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "bkHC",
      "code_hash": "ccc69ea4b1a214f77cfa56ce9a9b5eb2",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><span>CUDA is available</span><span>GPU: NVIDIA GeForce GTX 1660 Ti</span></div>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "lEQa",
      "code_hash": "782498b0da05787845794c82603cda96",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"ray-docs-pre-processing\">Ray docs Pre-processing</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "PKri",
      "code_hash": "276c0a3fae8e4db7785817271cdf68ff",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "Xref",
      "code_hash": "84b2291335c815f8431f61bba583cecf",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"define-shared-utilities\">Define shared utilities</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "SFPL",
      "code_hash": "207cdd512927a573bcb09c70108f1698",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "BYtC",
      "code_hash": "e087442121df9caacfdf7fdfdf13c6db",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "RGSE",
      "code_hash": "51856b8b619ddb395f9d72dd6ad1cf87",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-11 16:13:34\nRunning for: 00:00:01.87        \nMemory:      22.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name                     status    loc                    eta  max_depth  min_child_weight  subsample     acc  iter  total time (s)\n\n\ntrain_breast_cancer_31c9f_00000TERMINATED127.0.0.1:897350.0434196            8                 1   0.5303510.909091     1      0.0114911 \ntrain_breast_cancer_31c9f_00001TERMINATED127.0.0.1:897340.0115669            6                 2   0.9965190.615385     1      0.01138   \ntrain_breast_cancer_31c9f_00002TERMINATED127.0.0.1:897400.00124339           7                 3   0.5360780.629371     1      0.0096581 \ntrain_breast_cancer_31c9f_00003TERMINATED127.0.0.1:897420.000400434          6                 3   0.90014 0.601399     1      0.0103199 \ntrain_breast_cancer_31c9f_00004TERMINATED127.0.0.1:897380.0121308            6                 3   0.8431560.629371     1      0.00843   \ntrain_breast_cancer_31c9f_00005TERMINATED127.0.0.1:897330.0344144            2                 3   0.5130710.895105     1      0.00800109\ntrain_breast_cancer_31c9f_00006TERMINATED127.0.0.1:897370.0530037            7                 2   0.9208010.965035     1      0.0117419 \ntrain_breast_cancer_31c9f_00007TERMINATED127.0.0.1:897410.000230442          3                 3   0.9468520.608392     1      0.00917387\ntrain_breast_cancer_31c9f_00008TERMINATED127.0.0.1:897390.00166323           4                 1   0.5888790.636364     1      0.011095  \ntrain_breast_cancer_31c9f_00009TERMINATED127.0.0.1:897360.0753618            3                 3   0.55103 0.909091     1      0.00776482\n\n\n\n\n\n2025-02-11 16:13:34,649\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-31' in 0.0057s.\n2025-02-11 16:13:34,652\tINFO tune.py:1041 -- Total run time: 1.88 seconds (1.86 seconds for the tuning loop).\n\n\n\n\n\nAs you can see, the changes in the actual training function are minimal. Instead of\nreturning the accuracy value, we report it back to Tune using session.report().\nOur config dictionary only changed slightly. Instead of passing hard-coded\nparameters, we tell Tune to choose values from a range of valid options. There are\na number of options we have here, all of which are explained in\nthe Tune docs.\nFor a brief explanation, this is what they do:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial74757_00000:\nlr : 0.0654 --- (* 0.8) --> 0.052320000000000005\nh0 : 0.7739237385426097 --- (* 1.2) --> 0.9287084862511316\nh1 : 0.15770319740458727 --- (resample) --> 0.4279796053289977\n\n2025-02-24 16:22:03,081\tINFO pbt.py:878 -- \n\n[PopulationBasedTraining] [Exploit] Cloning trial 74757_00000 (score = 1.199790) into trial 74757_00001 (score = 1.199772)\n\n2025-02-24 16:22:03,082\tINFO pbt.py:905 -- \n\n[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial74757_00001:\nlr : 0.052320000000000005 --- (* 0.8) --> 0.041856000000000004\nh0 : 0.9287084862511316 --- (resample) --> 0.579167003721271\nh1 : 0.4279796053289977 --- (* 1.2) --> 0.5135755263947972\n\n2025-02-24 16:22:04,698\tINFO pbt.py:878 -- \n\n[PopulationBasedTraining] [Exploit] Cloning trial 74757_00000 (score = 1.199872) into trial 74757_00001 (score = 1.199847)\n\n2025-02-24 16:22:04,699\tINFO pbt.py:905 -- \n\n[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial74757_00001:\nlr : 0.052320000000000005 --- (* 1.2) --> 0.062784\nh0 : 0.9287084862511316 --- (* 1.2) --> 1.1144501835013578\nh1 : 0.4279796053289977 --- (resample) --> 0.25894972559062557\n\n2025-02-24 16:22:06,309\tINFO pbt.py:878 -- \n\n[PopulationBasedTraining] [Exploit] Cloning trial 74757_00001 (score = 1.199924) into trial 74757_00000 (score = 1.199920)\n\n2025-02-24 16:22:06,310\tINFO pbt.py:905 -- \n\n[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial74757_00000:\nlr : 0.062784 --- (resample) --> 0.006500000000000001\nh0 : 1.1144501835013578 --- (* 0.8) --> 0.8915601468010863\nh1 : 0.25894972559062557 --- (resample) --> 0.4494584110928429\n\n2025-02-24 16:22:07,944\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_func_2025-02-24_16-21-28' in 0.0049s.\n2025-02-24 16:22:07,946\tINFO tune.py:1041 -- Total run time: 39.88 seconds (39.86 seconds for the tuning loop).\n\n\n\n\n\n\n\nVisualize results#\nUsing some helper functions from here, we can create some visuals to help us understand the training progression of PBT.\n\n\nfig, axs = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw=dict(width_ratios=[1.5, 1]))\n\ncolors = [\"red\", \"black\"]\nlabels = [\"h = [1, 0]\", \"h = [0, 1]\"]\n\nplot_parameter_history(\n    pbt_results,\n    colors,\n    labels,\n    perturbation_interval=perturbation_interval,\n    fig=fig,\n    ax=axs[0],\n)\nplot_Q_history(pbt_results, colors, labels, ax=axs[1])': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial74757_00000:\nlr : 0.0654 --- (* 0.8) --> 0.052320000000000005\nh0 : 0.7739237385426097 --- (* 1.2) --> 0.9287084862511316\nh1 : 0.15770319740458727 --- (resample) --> 0.4279796053289977\n\n2025-02-24 16:22:03,081\tINFO pbt.py:878 -- \n\n[PopulationBasedTraining] [Exploit] Cloning trial 74757_00000 (score = 1.199790) into trial 74757_00001 (score = 1.199772)\n\n2025-02-24 16:22:03,082\tINFO pbt.py:905 -- \n\n[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial74757_00001:\nlr : 0.052320000000000005 --- (* 0.8) --> 0.041856000000000004\nh0 : 0.9287084862511316 --- (resample) --> 0.579167003721271\nh1 : 0.4279796053289977 --- (* 1.2) --> 0.5135755263947972\n\n2025-02-24 16:22:04,698\tINFO pbt.py:878 -- \n\n[PopulationBasedTraining] [Exploit] Cloning trial 74757_00000 (score = 1.199872) into trial 74757_00001 (score = 1.199847)\n\n2025-02-24 16:22:04,699\tINFO pbt.py:905 -- \n\n[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial74757_00001:\nlr : 0.052320000000000005 --- (* 1.2) --> 0.062784\nh0 : 0.9287084862511316 --- (* 1.2) --> 1.1144501835013578\nh1 : 0.4279796053289977 --- (resample) --> 0.25894972559062557\n\n2025-02-24 16:22:06,309\tINFO pbt.py:878 -- \n\n[PopulationBasedTraining] [Exploit] Cloning trial 74757_00001 (score = 1.199924) into trial 74757_00000 (score = 1.199920)\n\n2025-02-24 16:22:06,310\tINFO pbt.py:905 -- \n\n[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial74757_00000:\nlr : 0.062784 --- (resample) --> 0.006500000000000001\nh0 : 1.1144501835013578 --- (* 0.8) --> 0.8915601468010863\nh1 : 0.25894972559062557 --- (resample) --> 0.4494584110928429\n\n2025-02-24 16:22:07,944\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_func_2025-02-24_16-21-28' in 0.0049s.\n2025-02-24 16:22:07,946\tINFO tune.py:1041 -- Total run time: 39.88 seconds (39.86 seconds for the tuning loop).\n\n\n\n\n\n\n\nVisualize results#\nUsing some helper functions from here, we can create some visuals to help us understand the training progression of PBT.\n\n\nfig, axs = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw=dict(width_ratios=[1.5, 1]))\n\ncolors = [\"red\", \"black\"]\nlabels = [\"h = [1, 0]\", \"h = [0, 1]\"]\n\nplot_parameter_history(\n    pbt_results,\n    colors,\n    labels,\n    perturbation_interval=perturbation_interval,\n    fig=fig,\n    ax=axs[0],\n)\nplot_Q_history(pbt_results, colors, labels, ax=axs[1])': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'grid_results = tuner.fit()\nif grid_results.errors:\n    raise RuntimeError\n\n\n\n\n\nShow code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-24 16:22:18\nRunning for: 00:00:01.24        \nMemory:      21.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name            status    loc              h0   lr  iter  total time (s)       Q     theta0   theta1\n\n\ntrain_func_91d06_00000TERMINATED127.0.0.1:23610   00.015   100       0.068691 0.5906680.9        0.0427973\ntrain_func_91d06_00001TERMINATED127.0.0.1:23609   10.045   100       0.06599690.3899990.0008300930.9      \n\n\n\n\n\n\n\n\n\nAs we can see, neither trial makes it to the optimum, since the search configs are stuck with their original values. This illustrates a key advantage of PBT: while traditional hyperparameter search methods (like grid search) keep fixed search values throughout training, PBT can adapt the search dynamically, allowing it to find better solutions with the same computational budget.\n\n\nfig, axs = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw=dict(width_ratios=[1.5, 1]))\n\ncolors = [\"red\", \"black\"]\nlabels = [\"h = [1, 0]\", \"h = [0, 1]\"]\n\nplot_parameter_history(\n    grid_results,\n    colors,\n    labels,\n    perturbation_interval=perturbation_interval,\n    fig=fig,\n    ax=axs[0],\n)\nplot_Q_history(grid_results, colors, labels, ax=axs[1])\n\n\n\n\n\n\n\nCompare the two plots we generated with Figure 2 from the PBT paper (in particular, we produced the top-left and bottom-right plots).\n\n\n\nIncrease PBT population size#\nOne last experiment: what does it look like if we increase the PBT population size? Now, low-performing trials will sample one of the multiple high-performing trials to exploit, and it should result in some more interesting behavior.\nWith a larger population:\n\nThere\u2019s more diversity in the exploration space\nMultiple \u201cgood\u201d solutions can be discovered simultaneously\nDifferent exploitation patterns emerge as trials may choose from multiple well-performing configurations\nThe population as a whole can develop more robust strategies for optimization\n\n\n\nif ray.is_initialized():\n    ray.shutdown()\nray.init()\nperturbation_interval = 4\npbt_scheduler = PopulationBasedTraining(\n    time_attr=\"training_iteration\",\n    perturbation_interval=perturbation_interval,\n    quantile_fraction=0.5,\n    resample_probability=0.5,\n    hyperparam_mutations={\n        \"lr\": tune.qloguniform(5e-3, 1e-1, 5e-4),\n        \"h0\": tune.uniform(0.0, 1.0),\n        \"h1\": tune.uniform(0.0, 1.0),\n    },\n    synch=True,\n)\ntuner = Tuner(\n    train_func,\n    param_space={\n        \"lr\": tune.qloguniform(5e-3, 1e-1, 5e-4),\n        \"h0\": tune.grid_search([0.0, 1.0, 0.01, 0.99]),  # 4 trials\n        \"h1\": tune.sample_from(lambda spec: 1.0 - spec.config[\"h0\"]),\n        \"num_training_iterations\": 100,\n        \"checkpoint_interval\": perturbation_interval,\n    },\n    tune_config=TuneConfig(\n        num_samples=1,\n        metric=\"Q\",\n        mode=\"max\",\n        # Set the PBT scheduler in this config\n        scheduler=pbt_scheduler,\n    ),\n    run_config=tune.RunConfig(\n        stop={\"training_iteration\": 100},\n        failure_config=tune.FailureConfig(max_failures=3),\n    ),\n)\npbt_4_results = tuner.fit()\n\n\n\n\n\nShow code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-24 16:23:40\nRunning for: 00:01:18.96        \nMemory:      21.3/36.0 GiB': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial name                status    loc              mean      sd  iter  total time (s)   loss\n\n\ntrain_function_5bf98_00000TERMINATED127.0.0.1:48140     10.405758    30       2.11758  1.02341\ntrain_function_5bf98_00001TERMINATED127.0.0.1:48147     20.647335    30       0.07707311.53993\ntrain_function_5bf98_00002TERMINATED127.0.0.1:48151     30.256568    30       0.07284313.0393 \n\n2022-07-22 15:41:24,693\tINFO plugin_schema_manager.py:52 -- Loading the default runtime env schemas: ['/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/working_dir_schema.json', '/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/pip_schema.json'].\nCOMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\nCOMET ERROR: The given API key abc is invalid, please check it against the dashboard. Your experiment would not be logged \nFor more details, please refer to: https://www.comet.ml/docs/python-sdk/warnings-errors/\nCOMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\nCOMET ERROR: The given API key abc is invalid, please check it against the dashboard. Your experiment would not be logged \nFor more details, please refer to: https://www.comet.ml/docs/python-sdk/warnings-errors/\nCOMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\nCOMET ERROR: The given API key abc is invalid, please check it against the dashboard. Your experiment would not be logged \nFor more details, please refer to: https://www.comet.ml/docs/python-sdk/warnings-errors/\n\n\n2022-07-22 15:41:31,290\tINFO tune.py:738 -- Total run time: 7.36 seconds (6.72 seconds for the tuning loop).\n\n\n{'mean': 1, 'sd': 0.40575843135279466}\n\n\n\n\n\n\nTune Comet Logger#\nRay Tune offers an integration with Comet through the CometLoggerCallback,\nwhich automatically logs metrics and parameters reported to Tune to the Comet UI.\nClick on the following dropdown to see this callback API in detail:\n\n\nclass ray.air.integrations.comet.CometLoggerCallback(online: bool = True, tags: List[str] = None, save_checkpoints: bool = False, **experiment_kwargs)[source]\nCometLoggerCallback for logging Tune results to Comet.\nComet (https://comet.ml/site/) is a tool to manage and optimize the\nentire ML lifecycle, from experiment tracking, model optimization\nand dataset versioning to model production monitoring.\nThis Ray Tune LoggerCallback sends metrics and parameters to\nComet for tracking.\nIn order to use the CometLoggerCallback you must first install Comet\nvia pip install comet_ml\nThen set the following environment variables\nexport COMET_API_KEY=<Your API Key>\nAlternatively, you can also pass in your API Key as an argument to the\nCometLoggerCallback constructor.\nCometLoggerCallback(api_key=<Your API Key>)\n\nParameters:\n\nonline \u2013 Whether to make use of an Online or\nOffline Experiment. Defaults to True.\ntags \u2013 Tags to add to the logged Experiment.\nDefaults to None.\nsave_checkpoints \u2013 If True, model checkpoints will be saved to\nComet ML as artifacts. Defaults to False.\n**experiment_kwargs \u2013 Other keyword arguments will be passed to the\nconstructor for comet_ml.Experiment (or OfflineExperiment if\nonline=False).\n\n\n\nPlease consult the Comet ML documentation for more information on the\nExperiment and OfflineExperiment classes: https://comet.ml/site/\nExample:\nfrom ray.air.integrations.comet import CometLoggerCallback\ntune.run(\n    train,\n    config=config\n    callbacks=[CometLoggerCallback(\n        True,\n        ['tag1', 'tag2'],\n        workspace='my_workspace',\n        project_name='my_project_name'\n        )]\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial name        status    loc            activation    height  steps    width    loss  iter  total time (s)  ts  iterations  neg_mean_loss\n\n\nobjective_2397442cTERMINATED127.0.0.1:45401tanh         32.8422    10012.1847   4.80297    16        2.25951    0          15       -4.80297\nobjective_25b4a998TERMINATED127.0.0.1:45401relu         20.2852    100 2.08202 18.1839      4        0.535426   0           3      -18.1839 \nobjective_25b64488TERMINATED127.0.0.1:45453tanh        -48.4518    10010.1191  -3.74635   100       11.5319     0          99        3.74635\nobjective_25b7dfe6TERMINATED127.0.0.1:45403relu        -18.8439    10019.1277   9.59966     4        0.581903   0           3       -9.59966\nobjective_25cfab4eTERMINATED127.0.0.1:45404relu         17.2057    100 0.31708320.8519      4        0.59468    0           3      -20.8519 \nobjective_278eba4cTERMINATED127.0.0.1:45454relu        -27.0179    10013.577    7.76626    16        2.31198    0          15       -7.76626\nobjective_279d01a6TERMINATED127.0.0.1:45407relu         59.1103    100 2.4466  21.6781      4        0.575556   0           3      -21.6781 \nobjective_27aa31e6TERMINATED127.0.0.1:45409relu         50.058     10017.3776  16.6153      4        0.537561   0           3      -16.6153 \nobjective_27b7e2beTERMINATED127.0.0.1:45455relu        -51.2093    100 8.94948  5.57235    16        2.64238    0          15       -5.57235\nobjective_27c59a80TERMINATED127.0.0.1:45446relu         29.165     100 4.26995 17.3006      4        0.539177   0           3      -17.3006 \n\n\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_2397442c_1_activation=tanh,height=32.8422,steps=100,width=12.1847_2022-07-22_15-11-11/checkpoint_tmpf4b290\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.10108494758605957, '_episodes_total': 0}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial name        status    loc            activation    height  steps    width    loss  iter  total time (s)  ts  iterations  neg_mean_loss\n\n\nobjective_2397442cTERMINATED127.0.0.1:45401tanh         32.8422    10012.1847   4.80297    16        2.25951    0          15       -4.80297\nobjective_25b4a998TERMINATED127.0.0.1:45401relu         20.2852    100 2.08202 18.1839      4        0.535426   0           3      -18.1839 \nobjective_25b64488TERMINATED127.0.0.1:45453tanh        -48.4518    10010.1191  -3.74635   100       11.5319     0          99        3.74635\nobjective_25b7dfe6TERMINATED127.0.0.1:45403relu        -18.8439    10019.1277   9.59966     4        0.581903   0           3       -9.59966\nobjective_25cfab4eTERMINATED127.0.0.1:45404relu         17.2057    100 0.31708320.8519      4        0.59468    0           3      -20.8519 \nobjective_278eba4cTERMINATED127.0.0.1:45454relu        -27.0179    10013.577    7.76626    16        2.31198    0          15       -7.76626\nobjective_279d01a6TERMINATED127.0.0.1:45407relu         59.1103    100 2.4466  21.6781      4        0.575556   0           3      -21.6781 \nobjective_27aa31e6TERMINATED127.0.0.1:45409relu         50.058     10017.3776  16.6153      4        0.537561   0           3      -16.6153 \nobjective_27b7e2beTERMINATED127.0.0.1:45455relu        -51.2093    100 8.94948  5.57235    16        2.64238    0          15       -5.57235\nobjective_27c59a80TERMINATED127.0.0.1:45446relu         29.165     100 4.26995 17.3006      4        0.539177   0           3      -17.3006 \n\n\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_2397442c_1_activation=tanh,height=32.8422,steps=100,width=12.1847_2022-07-22_15-11-11/checkpoint_tmpf4b290\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.10108494758605957, '_episodes_total': 0}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial name        status    loc            activation    height  steps    width    loss  iter  total time (s)  ts  iterations  neg_mean_loss\n\n\nobjective_2397442cTERMINATED127.0.0.1:45401tanh         32.8422    10012.1847   4.80297    16        2.25951    0          15       -4.80297\nobjective_25b4a998TERMINATED127.0.0.1:45401relu         20.2852    100 2.08202 18.1839      4        0.535426   0           3      -18.1839 \nobjective_25b64488TERMINATED127.0.0.1:45453tanh        -48.4518    10010.1191  -3.74635   100       11.5319     0          99        3.74635\nobjective_25b7dfe6TERMINATED127.0.0.1:45403relu        -18.8439    10019.1277   9.59966     4        0.581903   0           3       -9.59966\nobjective_25cfab4eTERMINATED127.0.0.1:45404relu         17.2057    100 0.31708320.8519      4        0.59468    0           3      -20.8519 \nobjective_278eba4cTERMINATED127.0.0.1:45454relu        -27.0179    10013.577    7.76626    16        2.31198    0          15       -7.76626\nobjective_279d01a6TERMINATED127.0.0.1:45407relu         59.1103    100 2.4466  21.6781      4        0.575556   0           3      -21.6781 \nobjective_27aa31e6TERMINATED127.0.0.1:45409relu         50.058     10017.3776  16.6153      4        0.537561   0           3      -16.6153 \nobjective_27b7e2beTERMINATED127.0.0.1:45455relu        -51.2093    100 8.94948  5.57235    16        2.64238    0          15       -5.57235\nobjective_27c59a80TERMINATED127.0.0.1:45446relu         29.165     100 4.26995 17.3006      4        0.539177   0           3      -17.3006 \n\n\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_2397442c_1_activation=tanh,height=32.8422,steps=100,width=12.1847_2022-07-22_15-11-11/checkpoint_tmpf4b290\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.10108494758605957, '_episodes_total': 0}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_2397442c_1_activation=tanh,height=32.8422,steps=100,width=12.1847_2022-07-22_15-11-11/checkpoint_tmpf4b290\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.10108494758605957, '_episodes_total': 0}\n\n\nResult for objective_25b4a998:\n  date: 2022-07-22_15-11-24\n  done: false\n  episodes_total: 0\n  experiment_id: 20a5d76dc18749e4b1c9f15c5d8b43cf\n  hostname: Kais-MacBook-Pro.local\n  iterations: 0\n  iterations_since_restore: 1\n  mean_loss: 22.028519616352035\n  neg_mean_loss: -22.028519616352035\n  node_ip: 127.0.0.1\n  pid: 45401\n  time_since_restore: 0.10445284843444824\n  time_this_iter_s: 0.10445284843444824\n  time_total_s: 0.2087719440460205\n  timestamp: 1658499084\n  timesteps_since_restore: 0\n  timesteps_total: 0\n  training_iteration: 1\n  trial_id: 25b4a998\n  warmup_time: 0.010488033294677734\n  \n\n\n(objective pid=45424) 2022-07-22 15:11:24,383\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_27b7e2be_9_activation=relu,height=-51.2093,steps=100,width=8.9495_2022-07-22_15-11-18/checkpoint_tmp996dec\n(objective pid=45424) 2022-07-22 15:11:24,384\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.10371994972229004, '_episodes_total': 0}\n\n\nResult for objective_27b7e2be:\n  date: 2022-07-22_15-11-24\n  done: false\n  episodes_total: 0\n  experiment_id: fdc43ca37ed44cde857ca150a8f1e84f\n  hostname: Kais-MacBook-Pro.local\n  iterations: 0\n  iterations_since_restore: 1\n  mean_loss: 14.879072389639937\n  neg_mean_loss: -14.879072389639937\n  node_ip: 127.0.0.1\n  pid: 45424\n  time_since_restore: 0.1031639575958252\n  time_this_iter_s: 0.1031639575958252\n  time_total_s: 0.20688390731811523\n  timestamp: 1658499084\n  timesteps_since_restore: 0\n  timesteps_total: 0\n  training_iteration: 1\n  trial_id: 27b7e2be\n  warmup_time: 0.0069200992584228516': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial name        status    loc            activation    height  steps    width    loss  iter  total time (s)  ts  iterations  neg_mean_loss\n\n\nobjective_2397442cTERMINATED127.0.0.1:45401tanh         32.8422    10012.1847   4.80297    16        2.25951    0          15       -4.80297\nobjective_25b4a998TERMINATED127.0.0.1:45401relu         20.2852    100 2.08202 18.1839      4        0.535426   0           3      -18.1839 \nobjective_25b64488TERMINATED127.0.0.1:45453tanh        -48.4518    10010.1191  -3.74635   100       11.5319     0          99        3.74635\nobjective_25b7dfe6TERMINATED127.0.0.1:45403relu        -18.8439    10019.1277   9.59966     4        0.581903   0           3       -9.59966\nobjective_25cfab4eTERMINATED127.0.0.1:45404relu         17.2057    100 0.31708320.8519      4        0.59468    0           3      -20.8519 \nobjective_278eba4cTERMINATED127.0.0.1:45454relu        -27.0179    10013.577    7.76626    16        2.31198    0          15       -7.76626\nobjective_279d01a6TERMINATED127.0.0.1:45407relu         59.1103    100 2.4466  21.6781      4        0.575556   0           3      -21.6781 \nobjective_27aa31e6TERMINATED127.0.0.1:45409relu         50.058     10017.3776  16.6153      4        0.537561   0           3      -16.6153 \nobjective_27b7e2beTERMINATED127.0.0.1:45455relu        -51.2093    100 8.94948  5.57235    16        2.64238    0          15       -5.57235\nobjective_27c59a80TERMINATED127.0.0.1:45446relu         29.165     100 4.26995 17.3006      4        0.539177   0           3      -17.3006 \n\n\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_2397442c_1_activation=tanh,height=32.8422,steps=100,width=12.1847_2022-07-22_15-11-11/checkpoint_tmpf4b290\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.10108494758605957, '_episodes_total': 0}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_2397442c_1_activation=tanh,height=32.8422,steps=100,width=12.1847_2022-07-22_15-11-11/checkpoint_tmpf4b290\n(objective pid=45370) 2022-07-22 15:11:20,826\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.10108494758605957, '_episodes_total': 0}\n\n\nResult for objective_25b4a998:\n  date: 2022-07-22_15-11-24\n  done: false\n  episodes_total: 0\n  experiment_id: 20a5d76dc18749e4b1c9f15c5d8b43cf\n  hostname: Kais-MacBook-Pro.local\n  iterations: 0\n  iterations_since_restore: 1\n  mean_loss: 22.028519616352035\n  neg_mean_loss: -22.028519616352035\n  node_ip: 127.0.0.1\n  pid: 45401\n  time_since_restore: 0.10445284843444824\n  time_this_iter_s: 0.10445284843444824\n  time_total_s: 0.2087719440460205\n  timestamp: 1658499084\n  timesteps_since_restore: 0\n  timesteps_total: 0\n  training_iteration: 1\n  trial_id: 25b4a998\n  warmup_time: 0.010488033294677734\n  \n\n\n(objective pid=45424) 2022-07-22 15:11:24,383\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_27b7e2be_9_activation=relu,height=-51.2093,steps=100,width=8.9495_2022-07-22_15-11-18/checkpoint_tmp996dec\n(objective pid=45424) 2022-07-22 15:11:24,384\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.10371994972229004, '_episodes_total': 0}\n\n\nResult for objective_27b7e2be:\n  date: 2022-07-22_15-11-24\n  done: false\n  episodes_total: 0\n  experiment_id: fdc43ca37ed44cde857ca150a8f1e84f\n  hostname: Kais-MacBook-Pro.local\n  iterations: 0\n  iterations_since_restore: 1\n  mean_loss: 14.879072389639937\n  neg_mean_loss: -14.879072389639937\n  node_ip: 127.0.0.1\n  pid: 45424\n  time_since_restore: 0.1031639575958252\n  time_this_iter_s: 0.1031639575958252\n  time_total_s: 0.20688390731811523\n  timestamp: 1658499084\n  timesteps_since_restore: 0\n  timesteps_total: 0\n  training_iteration: 1\n  trial_id: 27b7e2be\n  warmup_time: 0.0069200992584228516': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45454) 2022-07-22 15:11:29,879\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_278eba4c_6_activation=relu,height=-27.0179,steps=100,width=13.5770_2022-07-22_15-11-18/checkpoint_tmp2835d4\n(objective pid=45454) 2022-07-22 15:11:29,879\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.5807228088378906, '_episodes_total': 0}\n(objective pid=45455) 2022-07-22 15:11:29,909\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_27b7e2be_9_activation=relu,height=-51.2093,steps=100,width=8.9495_2022-07-22_15-11-18/checkpoint_tmpd7ea63\n(objective pid=45455) 2022-07-22 15:11:29,910\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.9150340557098389, '_episodes_total': 0}\n(objective pid=45453) 2022-07-22 15:11:29,930\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_25b64488_3_activation=tanh,height=-48.4518,steps=100,width=10.1191_2022-07-22_15-11-14/checkpoint_tmp11824e\n(objective pid=45453) 2022-07-22 15:11:29,930\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.5960800647735596, '_episodes_total': 0}\n\n\n\n\nHere again are the hyperparameters found to minimize the mean loss of the\ndefined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'activation': 'tanh', 'height': -48.451797714080236, 'steps': 100, 'width': 10.119125894538891}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Returns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring training behavior#\n\n\nAlgorithmConfig.training(*, gamma: float | None = <ray.rllib.utils.from_config._NotProvided object>, lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip: float | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip_by: str | None = <ray.rllib.utils.from_config._NotProvided object>, train_batch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, train_batch_size_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_epochs: int | None = <ray.rllib.utils.from_config._NotProvided object>, minibatch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, shuffle_batch_per_epoch: bool | None = <ray.rllib.utils.from_config._NotProvided object>, model: dict | None = <ray.rllib.utils.from_config._NotProvided object>, optimizer: dict | None = <ray.rllib.utils.from_config._NotProvided object>, learner_class: ~typing.Type[Learner] | None = <ray.rllib.utils.from_config._NotProvided object>, learner_connector: ~typing.Callable[[RLModule], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_learner_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, learner_config_dict: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, num_aggregator_actors_per_learner=-1, max_requests_in_flight_per_aggregator_actor=-1, num_sgd_iter=-1, max_requests_in_flight_per_sampler_worker=-1) \u2192 AlgorithmConfig[source]\nSets the training related configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring EnvRunnerGroup and EnvRunner actors#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring EnvRunnerGroup and EnvRunner actors#\n\n\nAlgorithmConfig.env_runners(*, env_runner_cls: type | None = <ray.rllib.utils.from_config._NotProvided object>, num_env_runners: int | None = <ray.rllib.utils.from_config._NotProvided object>, create_local_env_runner: bool | None = <ray.rllib.utils.from_config._NotProvided object>, create_env_on_local_worker: bool | None = <ray.rllib.utils.from_config._NotProvided object>, num_envs_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, gym_env_vectorize_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_gpus_per_env_runner: float | int | None = <ray.rllib.utils.from_config._NotProvided object>, custom_resources_per_env_runner: dict | None = <ray.rllib.utils.from_config._NotProvided object>, validate_env_runners_after_construction: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sample_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, env_to_module_connector: ~typing.Callable[[~typing.Any | gymnasium.Env], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, module_to_env_connector: ~typing.Callable[[~typing.Any | gymnasium.Env, RLModule], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_env_to_module_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_module_to_env_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episode_lookback_horizon: int | None = <ray.rllib.utils.from_config._NotProvided object>, merge_env_runner_states: str | bool | None = <ray.rllib.utils.from_config._NotProvided object>, broadcast_env_runner_states: bool | None = <ray.rllib.utils.from_config._NotProvided object>, compress_observations: bool | None = <ray.rllib.utils.from_config._NotProvided object>, rollout_fragment_length: int | str | None = <ray.rllib.utils.from_config._NotProvided object>, batch_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, explore: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episodes_to_numpy: bool | None = <ray.rllib.utils.from_config._NotProvided object>, use_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, update_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, exploration_config: dict | None = <ray.rllib.utils.from_config._NotProvided object>, sample_collector: ~typing.Type[~ray.rllib.evaluation.collectors.sample_collector.SampleCollector] | None = <ray.rllib.utils.from_config._NotProvided object>, remote_worker_envs: bool | None = <ray.rllib.utils.from_config._NotProvided object>, remote_env_batch_wait_ms: float | None = <ray.rllib.utils.from_config._NotProvided object>, preprocessor_pref: str | None = <ray.rllib.utils.from_config._NotProvided object>, observation_filter: str | None = <ray.rllib.utils.from_config._NotProvided object>, enable_tf1_exec_eagerly: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sampler_perf_stats_ema_coef: float | None = <ray.rllib.utils.from_config._NotProvided object>, num_rollout_workers=-1, num_envs_per_worker=-1, validate_workers_after_construction=-1, ignore_worker_failures=-1, recreate_failed_workers=-1, restart_failed_sub_environments=-1, num_consecutive_worker_failures_tolerance=-1, worker_health_probe_timeout_s=-1, worker_restore_timeout_s=-1, synchronize_filter=-1, enable_connectors=-1) \u2192 AlgorithmConfig[source]\nSets the rollout worker configuration.\n\nParameters:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Returns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring training behavior#\n\n\nAlgorithmConfig.training(*, gamma: float | None = <ray.rllib.utils.from_config._NotProvided object>, lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip: float | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip_by: str | None = <ray.rllib.utils.from_config._NotProvided object>, train_batch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, train_batch_size_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_epochs: int | None = <ray.rllib.utils.from_config._NotProvided object>, minibatch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, shuffle_batch_per_epoch: bool | None = <ray.rllib.utils.from_config._NotProvided object>, model: dict | None = <ray.rllib.utils.from_config._NotProvided object>, optimizer: dict | None = <ray.rllib.utils.from_config._NotProvided object>, learner_class: ~typing.Type[Learner] | None = <ray.rllib.utils.from_config._NotProvided object>, learner_connector: ~typing.Callable[[RLModule], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_learner_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, learner_config_dict: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, num_aggregator_actors_per_learner=-1, max_requests_in_flight_per_aggregator_actor=-1, num_sgd_iter=-1, max_requests_in_flight_per_sampler_worker=-1) \u2192 AlgorithmConfig[source]\nSets the training related configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring EnvRunnerGroup and EnvRunner actors#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring EnvRunnerGroup and EnvRunner actors#\n\n\nAlgorithmConfig.env_runners(*, env_runner_cls: type | None = <ray.rllib.utils.from_config._NotProvided object>, num_env_runners: int | None = <ray.rllib.utils.from_config._NotProvided object>, create_local_env_runner: bool | None = <ray.rllib.utils.from_config._NotProvided object>, create_env_on_local_worker: bool | None = <ray.rllib.utils.from_config._NotProvided object>, num_envs_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, gym_env_vectorize_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_gpus_per_env_runner: float | int | None = <ray.rllib.utils.from_config._NotProvided object>, custom_resources_per_env_runner: dict | None = <ray.rllib.utils.from_config._NotProvided object>, validate_env_runners_after_construction: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sample_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, env_to_module_connector: ~typing.Callable[[~typing.Any | gymnasium.Env], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, module_to_env_connector: ~typing.Callable[[~typing.Any | gymnasium.Env, RLModule], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_env_to_module_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_module_to_env_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episode_lookback_horizon: int | None = <ray.rllib.utils.from_config._NotProvided object>, merge_env_runner_states: str | bool | None = <ray.rllib.utils.from_config._NotProvided object>, broadcast_env_runner_states: bool | None = <ray.rllib.utils.from_config._NotProvided object>, compress_observations: bool | None = <ray.rllib.utils.from_config._NotProvided object>, rollout_fragment_length: int | str | None = <ray.rllib.utils.from_config._NotProvided object>, batch_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, explore: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episodes_to_numpy: bool | None = <ray.rllib.utils.from_config._NotProvided object>, use_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, update_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, exploration_config: dict | None = <ray.rllib.utils.from_config._NotProvided object>, sample_collector: ~typing.Type[~ray.rllib.evaluation.collectors.sample_collector.SampleCollector] | None = <ray.rllib.utils.from_config._NotProvided object>, remote_worker_envs: bool | None = <ray.rllib.utils.from_config._NotProvided object>, remote_env_batch_wait_ms: float | None = <ray.rllib.utils.from_config._NotProvided object>, preprocessor_pref: str | None = <ray.rllib.utils.from_config._NotProvided object>, observation_filter: str | None = <ray.rllib.utils.from_config._NotProvided object>, enable_tf1_exec_eagerly: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sampler_perf_stats_ema_coef: float | None = <ray.rllib.utils.from_config._NotProvided object>, num_rollout_workers=-1, num_envs_per_worker=-1, validate_workers_after_construction=-1, ignore_worker_failures=-1, recreate_failed_workers=-1, restart_failed_sub_environments=-1, num_consecutive_worker_failures_tolerance=-1, worker_health_probe_timeout_s=-1, worker_restore_timeout_s=-1, synchronize_filter=-1, enable_connectors=-1) \u2192 AlgorithmConfig[source]\nSets the rollout worker configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring LearnerGroup and Learner actors#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring LearnerGroup and Learner actors#\n\n\nAlgorithmConfig.learners(*, num_learners: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_learner: str | float | int | None = <ray.rllib.utils.from_config._NotProvided object>, num_gpus_per_learner: float | int | None = <ray.rllib.utils.from_config._NotProvided object>, num_aggregator_actors_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_aggregator_actor: float | None = <ray.rllib.utils.from_config._NotProvided object>, local_gpu_idx: int | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>)[source]\nSets LearnerGroup and Learner worker related configurations.\n\nParameters:\n\nnum_learners \u2013 Number of Learner workers used for updating the RLModule.\nA value of 0 means training takes place on a local Learner on main\nprocess CPUs or 1 GPU (determined by num_gpus_per_learner).\nFor multi-gpu training, you have to set num_learners to > 1 and set\nnum_gpus_per_learner accordingly (e.g., 4 GPUs total and model fits on\n1 GPU: num_learners=4; num_gpus_per_learner=1 OR 4 GPUs total and\nmodel requires 2 GPUs: num_learners=2; num_gpus_per_learner=2).\nnum_cpus_per_learner \u2013 Number of CPUs allocated per Learner worker.\nIf \u201cauto\u201d (default), use 1 if num_gpus_per_learner=0, otherwise 0.\nOnly necessary for custom processing pipeline inside each Learner\nrequiring multiple CPU cores.\nIf num_learners=0, RLlib creates only one local Learner instance and\nthe number of CPUs on the main process is\nmax(num_cpus_per_learner, num_cpus_for_main_process).\nnum_gpus_per_learner \u2013 Number of GPUs allocated per Learner worker. If\nnum_learners=0, any value greater than 0 runs the\ntraining on a single GPU on the main process, while a value of 0 runs\nthe training on main process CPUs.\nnum_aggregator_actors_per_learner \u2013 The number of aggregator actors per\nLearner (if num_learners=0, one local learner is created). Must be at\nleast 1. Aggregator actors perform the task of a) converting episodes\ninto a train batch and b) move that train batch to the same GPU that\nthe corresponding learner is located on. Good values are 1 or 2, but\nthis strongly depends on your setup and EnvRunner throughput.\nmax_requests_in_flight_per_aggregator_actor \u2013 How many in-flight requests\nare allowed per aggregator actor before new requests are dropped?\nlocal_gpu_idx \u2013 If num_gpus_per_learner > 0, and\nnum_learners < 2, then RLlib uses this GPU index for training. This is\nan index into the available\nCUDA devices. For example if os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nand local_gpu_idx=0, RLlib uses the GPU with ID=1 on the node.\nmax_requests_in_flight_per_learner \u2013 Max number of in-flight requests\nto each Learner (actor). You normally do not have to tune this setting\n(default is 3), however, for asynchronous algorithms, this determines\nthe \u201cqueue\u201d size for incoming batches (or lists of episodes) into each\nLearner worker, thus also determining, how much off-policy\u2019ness would be\nacceptable. The off-policy\u2019ness is the difference between the numbers of\nupdates a policy has undergone on the Learner vs the EnvRunners.\nSee the ray.rllib.utils.actor_manager.FaultTolerantActorManager class\nfor more details.\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring custom callbacks#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Returns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring training behavior#\n\n\nAlgorithmConfig.training(*, gamma: float | None = <ray.rllib.utils.from_config._NotProvided object>, lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip: float | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip_by: str | None = <ray.rllib.utils.from_config._NotProvided object>, train_batch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, train_batch_size_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_epochs: int | None = <ray.rllib.utils.from_config._NotProvided object>, minibatch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, shuffle_batch_per_epoch: bool | None = <ray.rllib.utils.from_config._NotProvided object>, model: dict | None = <ray.rllib.utils.from_config._NotProvided object>, optimizer: dict | None = <ray.rllib.utils.from_config._NotProvided object>, learner_class: ~typing.Type[Learner] | None = <ray.rllib.utils.from_config._NotProvided object>, learner_connector: ~typing.Callable[[RLModule], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_learner_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, learner_config_dict: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, num_aggregator_actors_per_learner=-1, max_requests_in_flight_per_aggregator_actor=-1, num_sgd_iter=-1, max_requests_in_flight_per_sampler_worker=-1) \u2192 AlgorithmConfig[source]\nSets the training related configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring EnvRunnerGroup and EnvRunner actors#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring EnvRunnerGroup and EnvRunner actors#\n\n\nAlgorithmConfig.env_runners(*, env_runner_cls: type | None = <ray.rllib.utils.from_config._NotProvided object>, num_env_runners: int | None = <ray.rllib.utils.from_config._NotProvided object>, create_local_env_runner: bool | None = <ray.rllib.utils.from_config._NotProvided object>, create_env_on_local_worker: bool | None = <ray.rllib.utils.from_config._NotProvided object>, num_envs_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, gym_env_vectorize_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_gpus_per_env_runner: float | int | None = <ray.rllib.utils.from_config._NotProvided object>, custom_resources_per_env_runner: dict | None = <ray.rllib.utils.from_config._NotProvided object>, validate_env_runners_after_construction: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sample_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, env_to_module_connector: ~typing.Callable[[~typing.Any | gymnasium.Env], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, module_to_env_connector: ~typing.Callable[[~typing.Any | gymnasium.Env, RLModule], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_env_to_module_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_module_to_env_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episode_lookback_horizon: int | None = <ray.rllib.utils.from_config._NotProvided object>, merge_env_runner_states: str | bool | None = <ray.rllib.utils.from_config._NotProvided object>, broadcast_env_runner_states: bool | None = <ray.rllib.utils.from_config._NotProvided object>, compress_observations: bool | None = <ray.rllib.utils.from_config._NotProvided object>, rollout_fragment_length: int | str | None = <ray.rllib.utils.from_config._NotProvided object>, batch_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, explore: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episodes_to_numpy: bool | None = <ray.rllib.utils.from_config._NotProvided object>, use_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, update_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, exploration_config: dict | None = <ray.rllib.utils.from_config._NotProvided object>, sample_collector: ~typing.Type[~ray.rllib.evaluation.collectors.sample_collector.SampleCollector] | None = <ray.rllib.utils.from_config._NotProvided object>, remote_worker_envs: bool | None = <ray.rllib.utils.from_config._NotProvided object>, remote_env_batch_wait_ms: float | None = <ray.rllib.utils.from_config._NotProvided object>, preprocessor_pref: str | None = <ray.rllib.utils.from_config._NotProvided object>, observation_filter: str | None = <ray.rllib.utils.from_config._NotProvided object>, enable_tf1_exec_eagerly: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sampler_perf_stats_ema_coef: float | None = <ray.rllib.utils.from_config._NotProvided object>, num_rollout_workers=-1, num_envs_per_worker=-1, validate_workers_after_construction=-1, ignore_worker_failures=-1, recreate_failed_workers=-1, restart_failed_sub_environments=-1, num_consecutive_worker_failures_tolerance=-1, worker_health_probe_timeout_s=-1, worker_restore_timeout_s=-1, synchronize_filter=-1, enable_connectors=-1) \u2192 AlgorithmConfig[source]\nSets the rollout worker configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring LearnerGroup and Learner actors#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring LearnerGroup and Learner actors#\n\n\nAlgorithmConfig.learners(*, num_learners: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_learner: str | float | int | None = <ray.rllib.utils.from_config._NotProvided object>, num_gpus_per_learner: float | int | None = <ray.rllib.utils.from_config._NotProvided object>, num_aggregator_actors_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_aggregator_actor: float | None = <ray.rllib.utils.from_config._NotProvided object>, local_gpu_idx: int | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>)[source]\nSets LearnerGroup and Learner worker related configurations.\n\nParameters:\n\nnum_learners \u2013 Number of Learner workers used for updating the RLModule.\nA value of 0 means training takes place on a local Learner on main\nprocess CPUs or 1 GPU (determined by num_gpus_per_learner).\nFor multi-gpu training, you have to set num_learners to > 1 and set\nnum_gpus_per_learner accordingly (e.g., 4 GPUs total and model fits on\n1 GPU: num_learners=4; num_gpus_per_learner=1 OR 4 GPUs total and\nmodel requires 2 GPUs: num_learners=2; num_gpus_per_learner=2).\nnum_cpus_per_learner \u2013 Number of CPUs allocated per Learner worker.\nIf \u201cauto\u201d (default), use 1 if num_gpus_per_learner=0, otherwise 0.\nOnly necessary for custom processing pipeline inside each Learner\nrequiring multiple CPU cores.\nIf num_learners=0, RLlib creates only one local Learner instance and\nthe number of CPUs on the main process is\nmax(num_cpus_per_learner, num_cpus_for_main_process).\nnum_gpus_per_learner \u2013 Number of GPUs allocated per Learner worker. If\nnum_learners=0, any value greater than 0 runs the\ntraining on a single GPU on the main process, while a value of 0 runs\nthe training on main process CPUs.\nnum_aggregator_actors_per_learner \u2013 The number of aggregator actors per\nLearner (if num_learners=0, one local learner is created). Must be at\nleast 1. Aggregator actors perform the task of a) converting episodes\ninto a train batch and b) move that train batch to the same GPU that\nthe corresponding learner is located on. Good values are 1 or 2, but\nthis strongly depends on your setup and EnvRunner throughput.\nmax_requests_in_flight_per_aggregator_actor \u2013 How many in-flight requests\nare allowed per aggregator actor before new requests are dropped?\nlocal_gpu_idx \u2013 If num_gpus_per_learner > 0, and\nnum_learners < 2, then RLlib uses this GPU index for training. This is\nan index into the available\nCUDA devices. For example if os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nand local_gpu_idx=0, RLlib uses the GPU with ID=1 on the node.\nmax_requests_in_flight_per_learner \u2013 Max number of in-flight requests\nto each Learner (actor). You normally do not have to tune this setting\n(default is 3), however, for asynchronous algorithms, this determines\nthe \u201cqueue\u201d size for incoming batches (or lists of episodes) into each\nLearner worker, thus also determining, how much off-policy\u2019ness would be\nacceptable. The off-policy\u2019ness is the difference between the numbers of\nupdates a policy has undergone on the Learner vs the EnvRunners.\nSee the ray.rllib.utils.actor_manager.FaultTolerantActorManager class\nfor more details.\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring custom callbacks#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Returns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring offline RL specific settings#\n\n\nAlgorithmConfig.offline_data(*, input_: str | ~typing.Callable[[~ray.rllib.offline.io_context.IOContext], ~ray.rllib.offline.input_reader.InputReader] | None = <ray.rllib.utils.from_config._NotProvided object>, offline_data_class: ~typing.Type | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_method: str | ~typing.Callable | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_method_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_schema: ~typing.Dict[str, str] | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_episodes: bool | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_sample_batches: bool | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_batch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, input_filesystem: str | None = <ray.rllib.utils.from_config._NotProvided object>, input_filesystem_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, input_compress_columns: ~typing.List[str] | None = <ray.rllib.utils.from_config._NotProvided object>, materialize_data: bool | None = <ray.rllib.utils.from_config._NotProvided object>, materialize_mapped_data: bool | None = <ray.rllib.utils.from_config._NotProvided object>, map_batches_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, iter_batches_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, ignore_final_observation: bool | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_class: ~typing.Type | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_buffer_class: ~typing.Type | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_buffer_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_module_synch_period: int | None = <ray.rllib.utils.from_config._NotProvided object>, dataset_num_iters_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, input_config: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, actions_in_input_normalized: bool | None = <ray.rllib.utils.from_config._NotProvided object>, postprocess_inputs: bool | None = <ray.rllib.utils.from_config._NotProvided object>, shuffle_buffer_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, output: str | None = <ray.rllib.utils.from_config._NotProvided object>, output_config: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, output_compress_columns: ~typing.List[str] | None = <ray.rllib.utils.from_config._NotProvided object>, output_max_file_size: float | None = <ray.rllib.utils.from_config._NotProvided object>, output_max_rows_per_file: int | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_remaining_data: bool | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_method: str | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_method_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, output_filesystem: str | None = <ray.rllib.utils.from_config._NotProvided object>, output_filesystem_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_episodes: bool | None = <ray.rllib.utils.from_config._NotProvided object>, offline_sampling: str | None = <ray.rllib.utils.from_config._NotProvided object>) \u2192 AlgorithmConfig[source]\nSets the config\u2019s offline data settings.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring evaluation settings#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring evaluation settings#\n\n\nAlgorithmConfig.evaluation(*, evaluation_interval: int | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_duration: int | str | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_duration_unit: str | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_auto_duration_min_env_steps_per_sample: int | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_auto_duration_max_env_steps_per_sample: int | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_sample_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_parallel_to_training: bool | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_force_reset_envs_before_iteration: bool | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_config: ~ray.rllib.algorithms.algorithm_config.AlgorithmConfig | dict | None = <ray.rllib.utils.from_config._NotProvided object>, off_policy_estimation_methods: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, ope_split_batch_by_episode: bool | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_num_env_runners: int | None = <ray.rllib.utils.from_config._NotProvided object>, custom_evaluation_function: ~typing.Callable | None = <ray.rllib.utils.from_config._NotProvided object>, offline_evaluation_interval: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_offline_eval_runners: int | None = <ray.rllib.utils.from_config._NotProvided object>, offline_loss_for_module_fn: ~typing.Callable | None = <ray.rllib.utils.from_config._NotProvided object>, offline_eval_batch_size_per_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, dataset_num_iters_per_offline_eval_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, offline_eval_rl_module_inference_only: bool | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_offline_eval_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, custom_resources_per_offline_eval_runner: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, offline_evaluation_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_offline_eval_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, broadcast_offline_eval_runner_states: bool | None = <ray.rllib.utils.from_config._NotProvided object>, validate_offline_eval_runners_after_construction: bool | None = <ray.rllib.utils.from_config._NotProvided object>, restart_failed_offline_eval_runners: bool | None = <ray.rllib.utils.from_config._NotProvided object>, ignore_offline_eval_runner_failures: bool | None = <ray.rllib.utils.from_config._NotProvided object>, max_num_offline_eval_runner_restarts: int | None = <ray.rllib.utils.from_config._NotProvided object>, offline_eval_runner_health_probe_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, offline_eval_runner_restore_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, always_attach_evaluation_results=-1, evaluation_num_workers=-1) \u2192 AlgorithmConfig[source]\nSets the config\u2019s evaluation settings.\n\nParameters:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Returns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring training behavior#\n\n\nAlgorithmConfig.training(*, gamma: float | None = <ray.rllib.utils.from_config._NotProvided object>, lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip: float | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip_by: str | None = <ray.rllib.utils.from_config._NotProvided object>, train_batch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, train_batch_size_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_epochs: int | None = <ray.rllib.utils.from_config._NotProvided object>, minibatch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, shuffle_batch_per_epoch: bool | None = <ray.rllib.utils.from_config._NotProvided object>, model: dict | None = <ray.rllib.utils.from_config._NotProvided object>, optimizer: dict | None = <ray.rllib.utils.from_config._NotProvided object>, learner_class: ~typing.Type[Learner] | None = <ray.rllib.utils.from_config._NotProvided object>, learner_connector: ~typing.Callable[[RLModule], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_learner_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, learner_config_dict: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, num_aggregator_actors_per_learner=-1, max_requests_in_flight_per_aggregator_actor=-1, num_sgd_iter=-1, max_requests_in_flight_per_sampler_worker=-1) \u2192 AlgorithmConfig[source]\nSets the training related configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring EnvRunnerGroup and EnvRunner actors#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring EnvRunnerGroup and EnvRunner actors#\n\n\nAlgorithmConfig.env_runners(*, env_runner_cls: type | None = <ray.rllib.utils.from_config._NotProvided object>, num_env_runners: int | None = <ray.rllib.utils.from_config._NotProvided object>, create_local_env_runner: bool | None = <ray.rllib.utils.from_config._NotProvided object>, create_env_on_local_worker: bool | None = <ray.rllib.utils.from_config._NotProvided object>, num_envs_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, gym_env_vectorize_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_gpus_per_env_runner: float | int | None = <ray.rllib.utils.from_config._NotProvided object>, custom_resources_per_env_runner: dict | None = <ray.rllib.utils.from_config._NotProvided object>, validate_env_runners_after_construction: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sample_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, env_to_module_connector: ~typing.Callable[[~typing.Any | gymnasium.Env], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, module_to_env_connector: ~typing.Callable[[~typing.Any | gymnasium.Env, RLModule], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_env_to_module_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_module_to_env_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episode_lookback_horizon: int | None = <ray.rllib.utils.from_config._NotProvided object>, merge_env_runner_states: str | bool | None = <ray.rllib.utils.from_config._NotProvided object>, broadcast_env_runner_states: bool | None = <ray.rllib.utils.from_config._NotProvided object>, compress_observations: bool | None = <ray.rllib.utils.from_config._NotProvided object>, rollout_fragment_length: int | str | None = <ray.rllib.utils.from_config._NotProvided object>, batch_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, explore: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episodes_to_numpy: bool | None = <ray.rllib.utils.from_config._NotProvided object>, use_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, update_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, exploration_config: dict | None = <ray.rllib.utils.from_config._NotProvided object>, sample_collector: ~typing.Type[~ray.rllib.evaluation.collectors.sample_collector.SampleCollector] | None = <ray.rllib.utils.from_config._NotProvided object>, remote_worker_envs: bool | None = <ray.rllib.utils.from_config._NotProvided object>, remote_env_batch_wait_ms: float | None = <ray.rllib.utils.from_config._NotProvided object>, preprocessor_pref: str | None = <ray.rllib.utils.from_config._NotProvided object>, observation_filter: str | None = <ray.rllib.utils.from_config._NotProvided object>, enable_tf1_exec_eagerly: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sampler_perf_stats_ema_coef: float | None = <ray.rllib.utils.from_config._NotProvided object>, num_rollout_workers=-1, num_envs_per_worker=-1, validate_workers_after_construction=-1, ignore_worker_failures=-1, recreate_failed_workers=-1, restart_failed_sub_environments=-1, num_consecutive_worker_failures_tolerance=-1, worker_health_probe_timeout_s=-1, worker_restore_timeout_s=-1, synchronize_filter=-1, enable_connectors=-1) \u2192 AlgorithmConfig[source]\nSets the rollout worker configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring LearnerGroup and Learner actors#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring LearnerGroup and Learner actors#\n\n\nAlgorithmConfig.learners(*, num_learners: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_learner: str | float | int | None = <ray.rllib.utils.from_config._NotProvided object>, num_gpus_per_learner: float | int | None = <ray.rllib.utils.from_config._NotProvided object>, num_aggregator_actors_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_aggregator_actor: float | None = <ray.rllib.utils.from_config._NotProvided object>, local_gpu_idx: int | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>)[source]\nSets LearnerGroup and Learner worker related configurations.\n\nParameters:\n\nnum_learners \u2013 Number of Learner workers used for updating the RLModule.\nA value of 0 means training takes place on a local Learner on main\nprocess CPUs or 1 GPU (determined by num_gpus_per_learner).\nFor multi-gpu training, you have to set num_learners to > 1 and set\nnum_gpus_per_learner accordingly (e.g., 4 GPUs total and model fits on\n1 GPU: num_learners=4; num_gpus_per_learner=1 OR 4 GPUs total and\nmodel requires 2 GPUs: num_learners=2; num_gpus_per_learner=2).\nnum_cpus_per_learner \u2013 Number of CPUs allocated per Learner worker.\nIf \u201cauto\u201d (default), use 1 if num_gpus_per_learner=0, otherwise 0.\nOnly necessary for custom processing pipeline inside each Learner\nrequiring multiple CPU cores.\nIf num_learners=0, RLlib creates only one local Learner instance and\nthe number of CPUs on the main process is\nmax(num_cpus_per_learner, num_cpus_for_main_process).\nnum_gpus_per_learner \u2013 Number of GPUs allocated per Learner worker. If\nnum_learners=0, any value greater than 0 runs the\ntraining on a single GPU on the main process, while a value of 0 runs\nthe training on main process CPUs.\nnum_aggregator_actors_per_learner \u2013 The number of aggregator actors per\nLearner (if num_learners=0, one local learner is created). Must be at\nleast 1. Aggregator actors perform the task of a) converting episodes\ninto a train batch and b) move that train batch to the same GPU that\nthe corresponding learner is located on. Good values are 1 or 2, but\nthis strongly depends on your setup and EnvRunner throughput.\nmax_requests_in_flight_per_aggregator_actor \u2013 How many in-flight requests\nare allowed per aggregator actor before new requests are dropped?\nlocal_gpu_idx \u2013 If num_gpus_per_learner > 0, and\nnum_learners < 2, then RLlib uses this GPU index for training. This is\nan index into the available\nCUDA devices. For example if os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nand local_gpu_idx=0, RLlib uses the GPU with ID=1 on the node.\nmax_requests_in_flight_per_learner \u2013 Max number of in-flight requests\nto each Learner (actor). You normally do not have to tune this setting\n(default is 3), however, for asynchronous algorithms, this determines\nthe \u201cqueue\u201d size for incoming batches (or lists of episodes) into each\nLearner worker, thus also determining, how much off-policy\u2019ness would be\nacceptable. The off-policy\u2019ness is the difference between the numbers of\nupdates a policy has undergone on the Learner vs the EnvRunners.\nSee the ray.rllib.utils.actor_manager.FaultTolerantActorManager class\nfor more details.\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring custom callbacks#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Returns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring offline RL specific settings#\n\n\nAlgorithmConfig.offline_data(*, input_: str | ~typing.Callable[[~ray.rllib.offline.io_context.IOContext], ~ray.rllib.offline.input_reader.InputReader] | None = <ray.rllib.utils.from_config._NotProvided object>, offline_data_class: ~typing.Type | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_method: str | ~typing.Callable | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_method_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_schema: ~typing.Dict[str, str] | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_episodes: bool | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_sample_batches: bool | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_batch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, input_filesystem: str | None = <ray.rllib.utils.from_config._NotProvided object>, input_filesystem_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, input_compress_columns: ~typing.List[str] | None = <ray.rllib.utils.from_config._NotProvided object>, materialize_data: bool | None = <ray.rllib.utils.from_config._NotProvided object>, materialize_mapped_data: bool | None = <ray.rllib.utils.from_config._NotProvided object>, map_batches_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, iter_batches_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, ignore_final_observation: bool | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_class: ~typing.Type | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_buffer_class: ~typing.Type | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_buffer_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_module_synch_period: int | None = <ray.rllib.utils.from_config._NotProvided object>, dataset_num_iters_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, input_config: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, actions_in_input_normalized: bool | None = <ray.rllib.utils.from_config._NotProvided object>, postprocess_inputs: bool | None = <ray.rllib.utils.from_config._NotProvided object>, shuffle_buffer_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, output: str | None = <ray.rllib.utils.from_config._NotProvided object>, output_config: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, output_compress_columns: ~typing.List[str] | None = <ray.rllib.utils.from_config._NotProvided object>, output_max_file_size: float | None = <ray.rllib.utils.from_config._NotProvided object>, output_max_rows_per_file: int | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_remaining_data: bool | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_method: str | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_method_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, output_filesystem: str | None = <ray.rllib.utils.from_config._NotProvided object>, output_filesystem_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_episodes: bool | None = <ray.rllib.utils.from_config._NotProvided object>, offline_sampling: str | None = <ray.rllib.utils.from_config._NotProvided object>) \u2192 AlgorithmConfig[source]\nSets the config\u2019s offline data settings.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring evaluation settings#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring evaluation settings#\n\n\nAlgorithmConfig.evaluation(*, evaluation_interval: int | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_duration: int | str | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_duration_unit: str | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_auto_duration_min_env_steps_per_sample: int | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_auto_duration_max_env_steps_per_sample: int | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_sample_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_parallel_to_training: bool | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_force_reset_envs_before_iteration: bool | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_config: ~ray.rllib.algorithms.algorithm_config.AlgorithmConfig | dict | None = <ray.rllib.utils.from_config._NotProvided object>, off_policy_estimation_methods: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, ope_split_batch_by_episode: bool | None = <ray.rllib.utils.from_config._NotProvided object>, evaluation_num_env_runners: int | None = <ray.rllib.utils.from_config._NotProvided object>, custom_evaluation_function: ~typing.Callable | None = <ray.rllib.utils.from_config._NotProvided object>, offline_evaluation_interval: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_offline_eval_runners: int | None = <ray.rllib.utils.from_config._NotProvided object>, offline_loss_for_module_fn: ~typing.Callable | None = <ray.rllib.utils.from_config._NotProvided object>, offline_eval_batch_size_per_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, dataset_num_iters_per_offline_eval_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, offline_eval_rl_module_inference_only: bool | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_offline_eval_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, custom_resources_per_offline_eval_runner: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, offline_evaluation_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_offline_eval_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, broadcast_offline_eval_runner_states: bool | None = <ray.rllib.utils.from_config._NotProvided object>, validate_offline_eval_runners_after_construction: bool | None = <ray.rllib.utils.from_config._NotProvided object>, restart_failed_offline_eval_runners: bool | None = <ray.rllib.utils.from_config._NotProvided object>, ignore_offline_eval_runner_failures: bool | None = <ray.rllib.utils.from_config._NotProvided object>, max_num_offline_eval_runner_restarts: int | None = <ray.rllib.utils.from_config._NotProvided object>, offline_eval_runner_health_probe_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, offline_eval_runner_restore_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, always_attach_evaluation_results=-1, evaluation_num_workers=-1) \u2192 AlgorithmConfig[source]\nSets the config\u2019s evaluation settings.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring deep learning framework settings#': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\nConfiguring deep learning framework settings#\n\n\nAlgorithmConfig.framework(framework: str | None = <ray.rllib.utils.from_config._NotProvided object>, *, eager_tracing: bool | None = <ray.rllib.utils.from_config._NotProvided object>, eager_max_retraces: int | None = <ray.rllib.utils.from_config._NotProvided object>, tf_session_args: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, local_tf_session_args: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, torch_compile_learner: bool | None = <ray.rllib.utils.from_config._NotProvided object>, torch_compile_learner_what_to_compile: str | None = <ray.rllib.utils.from_config._NotProvided object>, torch_compile_learner_dynamo_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, torch_compile_learner_dynamo_backend: str | None = <ray.rllib.utils.from_config._NotProvided object>, torch_compile_worker: bool | None = <ray.rllib.utils.from_config._NotProvided object>, torch_compile_worker_dynamo_backend: str | None = <ray.rllib.utils.from_config._NotProvided object>, torch_compile_worker_dynamo_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, torch_ddp_kwargs: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, torch_skip_nan_gradients: bool | None = <ray.rllib.utils.from_config._NotProvided object>) \u2192 AlgorithmConfig[source]\nSets the config\u2019s DL framework settings.\n\nParameters:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Environment variables used by Ray Tune#\nSome of Ray Tune\u2019s behavior can be configured using environment variables.\nThese are the environment variables Ray Tune currently considers:\n\nThere are some environment variables that are mostly relevant for integrated libraries:\n\nWANDB_API_KEY: Weights and Biases API key. You can also use wandb login\ninstead.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data#\n\n\nAlgorithmConfig.offline_data(*, input_: str | ~typing.Callable[[~ray.rllib.offline.io_context.IOContext], ~ray.rllib.offline.input_reader.InputReader] | None = <ray.rllib.utils.from_config._NotProvided object>, offline_data_class: ~typing.Type | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_method: str | ~typing.Callable | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_method_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_schema: ~typing.Dict[str, str] | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_episodes: bool | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_sample_batches: bool | None = <ray.rllib.utils.from_config._NotProvided object>, input_read_batch_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, input_filesystem: str | None = <ray.rllib.utils.from_config._NotProvided object>, input_filesystem_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, input_compress_columns: ~typing.List[str] | None = <ray.rllib.utils.from_config._NotProvided object>, materialize_data: bool | None = <ray.rllib.utils.from_config._NotProvided object>, materialize_mapped_data: bool | None = <ray.rllib.utils.from_config._NotProvided object>, map_batches_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, iter_batches_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, ignore_final_observation: bool | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_class: ~typing.Type | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_buffer_class: ~typing.Type | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_buffer_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, prelearner_module_synch_period: int | None = <ray.rllib.utils.from_config._NotProvided object>, dataset_num_iters_per_learner: int | None = <ray.rllib.utils.from_config._NotProvided object>, input_config: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, actions_in_input_normalized: bool | None = <ray.rllib.utils.from_config._NotProvided object>, postprocess_inputs: bool | None = <ray.rllib.utils.from_config._NotProvided object>, shuffle_buffer_size: int | None = <ray.rllib.utils.from_config._NotProvided object>, output: str | None = <ray.rllib.utils.from_config._NotProvided object>, output_config: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, output_compress_columns: ~typing.List[str] | None = <ray.rllib.utils.from_config._NotProvided object>, output_max_file_size: float | None = <ray.rllib.utils.from_config._NotProvided object>, output_max_rows_per_file: int | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_remaining_data: bool | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_method: str | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_method_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, output_filesystem: str | None = <ray.rllib.utils.from_config._NotProvided object>, output_filesystem_kwargs: ~typing.Dict | None = <ray.rllib.utils.from_config._NotProvided object>, output_write_episodes: bool | None = <ray.rllib.utils.from_config._NotProvided object>, offline_sampling: str | None = <ray.rllib.utils.from_config._NotProvided object>) \u2192 AlgorithmConfig[source]#\nSets the config\u2019s offline data settings.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'import os\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\nds.write_csv(\"/tmp/few_files/\", min_rows_per_file=75)\n\nprint(os.listdir(\"/tmp/few_files/\"))\n\n\n['0_000001_000000.csv', '0_000000_000000.csv', '0_000002_000000.csv']\n\n\n\n\n\nConverting Datasets to other Python libraries#\n\nConverting Datasets to pandas#\nTo convert a Dataset to a pandas DataFrame, call\nDataset.to_pandas(). Your data must fit in memory\non the head node.\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\ndf = ds.to_pandas()\nprint(df)\n\n\n     sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n0                  5.1               3.5  ...               0.2       0\n1                  4.9               3.0  ...               0.2       0\n2                  4.7               3.2  ...               0.2       0\n3                  4.6               3.1  ...               0.2       0\n4                  5.0               3.6  ...               0.2       0\n..                 ...               ...  ...               ...     ...\n145                6.7               3.0  ...               2.3       2\n146                6.3               2.5  ...               1.9       2\n147                6.5               3.0  ...               2.0       2\n148                6.2               3.4  ...               2.3       2\n149                5.9               3.0  ...               1.8       2\n<BLANKLINE>\n[150 rows x 5 columns]\n\n\n\n\nConverting Datasets to distributed DataFrames#\nRay Data interoperates with distributed data processing frameworks like Daft,\nDask, Spark, Modin, and\nMars.\n\n\n\nDaft\nTo convert a Dataset to a Daft Dataframe, call\nDataset.to_daft().\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\ndf = ds.to_daft()\n\n\n\n\n\nDask\nTo convert a Dataset to a\nDask DataFrame, call\nDataset.to_dask().\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\ndf = ds.to_dask()\n\ndf\n\n(Showing first 8 of 150 rows)\n\n\n\n\n\nSpark\nTo convert a Dataset to a Spark DataFrame,\ncall Dataset.to_spark().\nimport ray\nimport raydp\n\nspark = raydp.init_spark(\n    app_name = \"example\",\n    num_executors = 1,\n    executor_cores = 4,\n    executor_memory = \"512M\"\n)\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\ndf = ds.to_spark(spark)\n\n\n\n\n\nModin\nTo convert a Dataset to a Modin DataFrame, call\nDataset.to_modin().\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\nmdf = ds.to_modin()\n\n\n\n\n\nMars\nTo convert a Dataset from a Mars DataFrame, call\nDataset.to_mars().\nimport ray\n\nds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n\nmdf = ds.to_mars()': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'See also\nCheck out the KubeRay guide on GCS fault tolerance to learn more about how Serve leverages the external Redis cluster to provide head node fault tolerance.\n\n\n\n\nSpreading replicas across nodes#\nOne way to improve the availability of your Serve application is to spread deployment replicas across multiple nodes so that you still have enough running\nreplicas to serve traffic even after a certain number of node failures.\nBy default, Serve soft spreads all deployment replicas but it has a few limitations:\n\nThe spread is soft and best-effort with no guarantee that the it\u2019s perfectly even.\nServe tries to spread replicas among the existing nodes if possible instead of launching new nodes.\nFor example, if you have a big enough single node cluster, Serve schedules all replicas on that single node assuming\nit has enough resources. However, that node becomes the single point of failure.\n\nYou can change the spread behavior of your deployment with the max_replicas_per_node\ndeployment option, which hard limits the number of replicas of a given deployment that can run on a single node.\nIf you set it to 1 then you\u2019re effectively strict spreading the deployment replicas. If you don\u2019t set it then there\u2019s no hard spread constraint and Serve uses the default soft spread mentioned in the preceding paragraph. max_replicas_per_node option is per deployment and only affects the spread of replicas within a deployment. There\u2019s no spread between replicas of different deployments.\nThe following code example shows how to set max_replicas_per_node deployment option:\nimport ray\nfrom ray import serve\n\n@serve.deployment(max_replicas_per_node=1)\nclass Deployment1:\n  def __call__(self, request):\n    return \"hello\"\n\n@serve.deployment(max_replicas_per_node=2)\nclass Deployment2:\n  def __call__(self, request):\n    return \"world\"\n\n\nThis example has two Serve deployments with different max_replicas_per_node: Deployment1 can have at most one replica on each node and Deployment2 can have at most two replicas on each node. If you schedule two replicas of Deployment1 and two replicas of Deployment2, Serve runs a cluster with at least two nodes, each running one replica of Deployment1. The two replicas of Deployment2 may run on either a single node or across two nodes because either satisfies the max_replicas_per_node constraint.\n\n\n\nServe\u2019s recovery procedures#\nThis section explains how Serve recovers from system failures. It uses the following Serve application and config as a working example.\n\n\n\nPython Code\n# File name: sleepy_pid.py\n\nfrom ray import serve\n\n\n@serve.deployment\nclass SleepyPid:\n    def __init__(self):\n        import time\n\n        time.sleep(10)\n\n    def __call__(self) -> int:\n        import os\n\n        return os.getpid()\n\n\napp = SleepyPid.bind()\n\n\n\n\n\nKubernetes Config\n# File name: config.yaml\n\n\n\n\nFollow the KubeRay quickstart guide to:\n\nInstall kubectl and Helm\nPrepare a Kubernetes cluster\nDeploy a KubeRay operator\n\nThen, deploy the Serve application above:\n$ kubectl apply -f config.yaml\n\n\n\nWorker node failure#\nYou can simulate a worker node failure in the working example. First, take a look at the nodes and pods running in your Kubernetes cluster:\n$ kubectl get nodes\n\nNAME                                        STATUS   ROLES    AGE     VERSION\ngke-serve-demo-default-pool-ed597cce-nvm2   Ready    <none>   3d22h   v1.22.12-gke.1200\ngke-serve-demo-default-pool-ed597cce-m888   Ready    <none>   3d22h   v1.22.12-gke.1200\ngke-serve-demo-default-pool-ed597cce-pu2q   Ready    <none>   3d22h   v1.22.12-gke.1200\n\n$ kubectl get pods -o wide': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Resuming from checkpoint#\nIn the event of a failed job, you can use the latest checkpoint to resume training of the model. This example configures TorchTrainer to automatically resume\nfrom the latest checkpoint:\nexperiment_path = os.path.expanduser(\"/mnt/cluster_storage/finetune-resnet\")\nif TorchTrainer.can_restore(experiment_path):\n    trainer = TorchTrainer.restore(experiment_path,\n        train_loop_per_worker=train_loop_per_worker,\n        train_loop_config=train_loop_config,\n        scaling_config=scaling_config,\n        run_config=run_config,\n    )\nelse:\n    trainer = TorchTrainer(\n        train_loop_per_worker=train_loop_per_worker,\n        train_loop_config=train_loop_config,\n        scaling_config=scaling_config,\n        run_config=run_config,\n    )\n\n\nYou can verify automatic checkpoint recovery by redeploying the same RayJob:\nkubectl create -f ray-job.pytorch-image-classifier.yaml\n\n\nIf the previous job succeeded, the training job should restore the checkpoint state from the checkpoint_000009 directory\nand then immediately complete training with 0 iterations:\n2024-04-29 15:51:32,528 INFO experiment_state.py:366 -- Trying to find and download experiment checkpoint at /mnt/cluster_storage/finetune-resnet\n2024-04-29 15:51:32,651 INFO experiment_state.py:396 -- A remote experiment checkpoint was found and will be used to restore the previous experiment state.\n2024-04-29 15:51:32,652 INFO tune_controller.py:404 -- Using the newest experiment state file found within the experiment directory: experiment_state-2024-04-29_15-43-40.json\n\nView detailed results here: /mnt/cluster_storage/finetune-resnet\nTo visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/finetune-resnet`\n\nResult(\n  metrics={'loss': 0.070047477101968, 'acc': 0.23529411764705882},\n  path='/mnt/cluster_storage/finetune-resnet/TorchTrainer_ecc04_00000_0_2024-04-29_15-43-40',\n  filesystem='local',\n  checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/finetune-resnet/TorchTrainer_ecc04_00000_0_2024-04-29_15-43-40/checkpoint_000009)\n)\n\nTraining finished iteration 8 at 2024-04-29 17:27:29. Total running time: 54s\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Training result                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 checkpoint_dir_name   checkpoint_000007 \u2502\n\u2502 time_this_iter_s               40.46113 \u2502\n\u2502 time_total_s                   95.00043 \u2502\n\u2502 training_iteration                    8 \u2502\n\u2502 acc                             0.23529 \u2502\n\u2502 loss                            0.08811 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nTraining saved a checkpoint for iteration 8 at: (local)/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000007\n(RayTrainWorker pid=671, ip=10.108.2.65) Epoch 8-train Loss: 0.0893 Acc: 0.2459\n(RayTrainWorker pid=671, ip=10.108.2.65) Epoch 8-val Loss: 0.0859 Acc: 0.2353\n(RayTrainWorker pid=589, ip=10.108.1.83) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/finetune-resnet/TorchTrainer_96923_00000_0_2024-04-29_17-21-29/checkpoint_000008) [repeated 4x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial Progress\n\n\nTrial name                date               done  episodes_total  experiment_id                   experiment_tag    hostname                iterations_since_restore   lossnode_ip    pid  time_since_restore  time_this_iter_s  time_total_s  timestamp  timesteps_since_restoretimesteps_total    training_iterationtrial_id     warmup_time\n\n\n\n\n2023-02-07 00:04:19,366\tINFO tune.py:798 -- Total run time: 7.38 seconds (6.85 seconds for the tuning loop).\n\n\n<ray.tune.result_grid.ResultGrid at 0x137de07c0>\n\n\n\n\nWhen the script executes, a grid-search is carried out and the results are saved to the Aim repo,\nstored at the default location \u2013 the experiment log directory (in this case, it\u2019s at /tmp/ray_results/aim_example).\n\nMore Configuration Options for Aim#\nIn the example above, we used the default configuration for the AimLoggerCallback.\nThere are a few options that can be configured as arguments to the callback. For example,\nsetting AimLoggerCallback(repo=\"/path/to/repo\") will log results to the Aim repo at that\nfilepath, which could be useful if you have a central location where the results of multiple\nTune experiments are stored. Relative paths to the working directory where Tune script is\nlaunched can be used as well. By default, the repo will be set to the experiment log\ndirectory. See the API reference for more configurations.\n\n\n\nLaunching the Aim UI#\nNow that we have logged our results to the Aim repository, we can view it in Aim\u2019s web UI.\nTo do this, we first find the directory where the Aim repository lives, then we use\nthe Aim CLI to launch the web interface.\n\n\n# Uncomment the following line to launch the Aim UI!\n#!aim up --repo=/tmp/ray_results/aim_example\n\n\n\n\n--------------------------------------------------------------------------\n                Aim UI collects anonymous usage analytics.                \n                        Read how to opt-out here:                         \n    https://aimstack.readthedocs.io/en/latest/community/telemetry.html    \n--------------------------------------------------------------------------\nRunning Aim UI on repo `<Repo#-5734997863388805469 path=/tmp/ray_results/aim_example/.aim read_only=None>`\nOpen http://127.0.0.1:43800\nPress Ctrl+C to exit\n^C\n\n\n\n\nAfter launching the Aim UI, we can open the web interface at localhost:43800.\n\nThe next sections contain more in-depth information on the API of the Tune-Aim integration.\n\n\nTune Aim Logger API#\n\n\nclass ray.tune.logger.aim.AimLoggerCallback(repo: str | None = None, experiment_name: str | None = None, metrics: List[str] | None = None, **aim_run_kwargs)[source]\nAim Logger: logs metrics in Aim format.\nAim is an open-source, self-hosted ML experiment tracking tool.\nIt\u2019s good at tracking lots (thousands) of training runs, and it allows you to\ncompare them with a performant and well-designed UI.\nSource: aimhubio/aim\n\nParameters:\n\nrepo \u2013 Aim repository directory or a Repo object that the Run object will\nlog results to. If not provided, a default repo will be set up in the\nexperiment directory (one level above trial directories).\nexperiment \u2013 Sets the experiment property of each Run object, which is the\nexperiment name associated with it. Can be used later to query\nruns/sequences.\nIf not provided, the default will be the Tune experiment name set\nby RunConfig(name=...).\nmetrics \u2013 List of metric names (out of the metrics reported by Tune) to\ntrack in Aim. If no metric are specified, log everything that\nis reported.\naim_run_kwargs \u2013 Additional arguments that will be passed when creating the\nindividual Run objects for each trial. For the full list of arguments,\nplease see the Aim documentation:\nhttps://aimstack.readthedocs.io/en/latest/refs/sdk.html': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config = (\n    DQNConfig()\n    .environment(\"CartPole-v1\")\n    .training(replay_buffer_config={\n        \"type\": \"PrioritizedEpisodeReplayBuffer\",\n        \"capacity\": 60000,\n        \"alpha\": 0.5,\n        \"beta\": 0.5,\n    })\n    .env_runners(num_env_runners=1)\n)\nalgo = config.build()\nalgo.train()\nalgo.stop()\n\n\nfrom ray.rllib.algorithms.dqn.dqn import DQNConfig\nfrom ray import tune\n\nconfig = (\n    DQNConfig()\n    .environment(\"CartPole-v1\")\n    .training(\n        num_atoms=tune.grid_search([1,])\n    )\n)\ntune.Tuner(\n    \"DQN\",\n    run_config=tune.RunConfig(stop={\"training_iteration\":1}),\n    param_space=config,\n).fit()\n\n\n\n\ntraining(*, target_network_update_freq: int | None = <ray.rllib.utils.from_config._NotProvided object>, replay_buffer_config: dict | None = <ray.rllib.utils.from_config._NotProvided object>, store_buffer_in_checkpoints: bool | None = <ray.rllib.utils.from_config._NotProvided object>, lr_schedule: ~typing.List[~typing.List[int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, epsilon: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, adam_epsilon: float | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_steps_sampled_before_learning_starts: int | None = <ray.rllib.utils.from_config._NotProvided object>, tau: float | None = <ray.rllib.utils.from_config._NotProvided object>, num_atoms: int | None = <ray.rllib.utils.from_config._NotProvided object>, v_min: float | None = <ray.rllib.utils.from_config._NotProvided object>, v_max: float | None = <ray.rllib.utils.from_config._NotProvided object>, noisy: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sigma0: float | None = <ray.rllib.utils.from_config._NotProvided object>, dueling: bool | None = <ray.rllib.utils.from_config._NotProvided object>, hiddens: int | None = <ray.rllib.utils.from_config._NotProvided object>, double_q: bool | None = <ray.rllib.utils.from_config._NotProvided object>, n_step: int | ~typing.Tuple[int, int] | None = <ray.rllib.utils.from_config._NotProvided object>, before_learn_on_batch: ~typing.Callable[[~typing.Type[~ray.rllib.policy.sample_batch.MultiAgentBatch], ~typing.List[~typing.Type[~ray.rllib.policy.policy.Policy]], ~typing.Type[int]], ~typing.Type[~ray.rllib.policy.sample_batch.MultiAgentBatch]] = <ray.rllib.utils.from_config._NotProvided object>, training_intensity: float | None = <ray.rllib.utils.from_config._NotProvided object>, td_error_loss_fn: str | None = <ray.rllib.utils.from_config._NotProvided object>, categorical_distribution_temperature: float | None = <ray.rllib.utils.from_config._NotProvided object>, burn_in_len: int | None = <ray.rllib.utils.from_config._NotProvided object>, **kwargs) \u2192 DQNConfig[source]#\nSets the training related configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\n\nSoft Actor Critic (SAC)#\n[original paper],\n[follow up paper],\n[implementation].\n\n\n\nSAC architecture: SAC uses a replay buffer to temporarily store episode samples that RLlib collects from the environment.\nThroughout different training iterations, these episodes and episode fragments are re-sampled from the buffer and re-used\nfor updating the model, before eventually being discarded when the buffer has reached capacity and new samples keep coming in (FIFO).\nThis reuse of training data makes DQN very sample-efficient and off-policy.\nSAC scales out on both axes, supporting multiple EnvRunners for sample collection and multiple GPU- or CPU-based Learners\nfor updating the model.#\n\n\nTuned examples:\nPendulum-v1,\nHalfCheetah-v3,\nSAC-specific configs (see also generic algorithm settings):': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\n\nSoft Actor Critic (SAC)#\n[original paper],\n[follow up paper],\n[implementation].\n\n\n\nSAC architecture: SAC uses a replay buffer to temporarily store episode samples that RLlib collects from the environment.\nThroughout different training iterations, these episodes and episode fragments are re-sampled from the buffer and re-used\nfor updating the model, before eventually being discarded when the buffer has reached capacity and new samples keep coming in (FIFO).\nThis reuse of training data makes DQN very sample-efficient and off-policy.\nSAC scales out on both axes, supporting multiple EnvRunners for sample collection and multiple GPU- or CPU-based Learners\nfor updating the model.#\n\n\nTuned examples:\nPendulum-v1,\nHalfCheetah-v3,\nSAC-specific configs (see also generic algorithm settings):\n\n\nclass ray.rllib.algorithms.sac.sac.SACConfig(algo_class=None)[source]#\nDefines a configuration class from which an SAC Algorithm can be built.\nconfig = (\n    SACConfig()\n    .environment(\"Pendulum-v1\")\n    .env_runners(num_env_runners=1)\n    .training(\n        gamma=0.9,\n        actor_lr=0.001,\n        critic_lr=0.002,\n        train_batch_size_per_learner=32,\n    )\n)\n# Build the SAC algo object from the config and run 1 training iteration.\nalgo = config.build()\nalgo.train()\n\n\n\n\ntraining(*, twin_q: bool | None = <ray.rllib.utils.from_config._NotProvided object>, q_model_config: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, policy_model_config: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, tau: float | None = <ray.rllib.utils.from_config._NotProvided object>, initial_alpha: float | None = <ray.rllib.utils.from_config._NotProvided object>, target_entropy: str | float | None = <ray.rllib.utils.from_config._NotProvided object>, n_step: int | ~typing.Tuple[int, int] | None = <ray.rllib.utils.from_config._NotProvided object>, store_buffer_in_checkpoints: bool | None = <ray.rllib.utils.from_config._NotProvided object>, replay_buffer_config: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, training_intensity: float | None = <ray.rllib.utils.from_config._NotProvided object>, clip_actions: bool | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip: float | None = <ray.rllib.utils.from_config._NotProvided object>, optimization_config: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, actor_lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, critic_lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, alpha_lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, target_network_update_freq: int | None = <ray.rllib.utils.from_config._NotProvided object>, _deterministic_loss: bool | None = <ray.rllib.utils.from_config._NotProvided object>, _use_beta_distribution: bool | None = <ray.rllib.utils.from_config._NotProvided object>, num_steps_sampled_before_learning_starts: int | None = <ray.rllib.utils.from_config._NotProvided object>, **kwargs) \u2192 SACConfig[source]#\nSets the training related configuration.\n\nParameters:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config = (\n    DQNConfig()\n    .environment(\"CartPole-v1\")\n    .training(replay_buffer_config={\n        \"type\": \"PrioritizedEpisodeReplayBuffer\",\n        \"capacity\": 60000,\n        \"alpha\": 0.5,\n        \"beta\": 0.5,\n    })\n    .env_runners(num_env_runners=1)\n)\nalgo = config.build()\nalgo.train()\nalgo.stop()\n\n\nfrom ray.rllib.algorithms.dqn.dqn import DQNConfig\nfrom ray import tune\n\nconfig = (\n    DQNConfig()\n    .environment(\"CartPole-v1\")\n    .training(\n        num_atoms=tune.grid_search([1,])\n    )\n)\ntune.Tuner(\n    \"DQN\",\n    run_config=tune.RunConfig(stop={\"training_iteration\":1}),\n    param_space=config,\n).fit()\n\n\n\n\ntraining(*, target_network_update_freq: int | None = <ray.rllib.utils.from_config._NotProvided object>, replay_buffer_config: dict | None = <ray.rllib.utils.from_config._NotProvided object>, store_buffer_in_checkpoints: bool | None = <ray.rllib.utils.from_config._NotProvided object>, lr_schedule: ~typing.List[~typing.List[int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, epsilon: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, adam_epsilon: float | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_steps_sampled_before_learning_starts: int | None = <ray.rllib.utils.from_config._NotProvided object>, tau: float | None = <ray.rllib.utils.from_config._NotProvided object>, num_atoms: int | None = <ray.rllib.utils.from_config._NotProvided object>, v_min: float | None = <ray.rllib.utils.from_config._NotProvided object>, v_max: float | None = <ray.rllib.utils.from_config._NotProvided object>, noisy: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sigma0: float | None = <ray.rllib.utils.from_config._NotProvided object>, dueling: bool | None = <ray.rllib.utils.from_config._NotProvided object>, hiddens: int | None = <ray.rllib.utils.from_config._NotProvided object>, double_q: bool | None = <ray.rllib.utils.from_config._NotProvided object>, n_step: int | ~typing.Tuple[int, int] | None = <ray.rllib.utils.from_config._NotProvided object>, before_learn_on_batch: ~typing.Callable[[~typing.Type[~ray.rllib.policy.sample_batch.MultiAgentBatch], ~typing.List[~typing.Type[~ray.rllib.policy.policy.Policy]], ~typing.Type[int]], ~typing.Type[~ray.rllib.policy.sample_batch.MultiAgentBatch]] = <ray.rllib.utils.from_config._NotProvided object>, training_intensity: float | None = <ray.rllib.utils.from_config._NotProvided object>, td_error_loss_fn: str | None = <ray.rllib.utils.from_config._NotProvided object>, categorical_distribution_temperature: float | None = <ray.rllib.utils.from_config._NotProvided object>, burn_in_len: int | None = <ray.rllib.utils.from_config._NotProvided object>, **kwargs) \u2192 DQNConfig[source]#\nSets the training related configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\n\nSoft Actor Critic (SAC)#\n[original paper],\n[follow up paper],\n[implementation].\n\n\n\nSAC architecture: SAC uses a replay buffer to temporarily store episode samples that RLlib collects from the environment.\nThroughout different training iterations, these episodes and episode fragments are re-sampled from the buffer and re-used\nfor updating the model, before eventually being discarded when the buffer has reached capacity and new samples keep coming in (FIFO).\nThis reuse of training data makes DQN very sample-efficient and off-policy.\nSAC scales out on both axes, supporting multiple EnvRunners for sample collection and multiple GPU- or CPU-based Learners\nfor updating the model.#\n\n\nTuned examples:\nPendulum-v1,\nHalfCheetah-v3,\nSAC-specific configs (see also generic algorithm settings):': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\n\nSoft Actor Critic (SAC)#\n[original paper],\n[follow up paper],\n[implementation].\n\n\n\nSAC architecture: SAC uses a replay buffer to temporarily store episode samples that RLlib collects from the environment.\nThroughout different training iterations, these episodes and episode fragments are re-sampled from the buffer and re-used\nfor updating the model, before eventually being discarded when the buffer has reached capacity and new samples keep coming in (FIFO).\nThis reuse of training data makes DQN very sample-efficient and off-policy.\nSAC scales out on both axes, supporting multiple EnvRunners for sample collection and multiple GPU- or CPU-based Learners\nfor updating the model.#\n\n\nTuned examples:\nPendulum-v1,\nHalfCheetah-v3,\nSAC-specific configs (see also generic algorithm settings):\n\n\nclass ray.rllib.algorithms.sac.sac.SACConfig(algo_class=None)[source]#\nDefines a configuration class from which an SAC Algorithm can be built.\nconfig = (\n    SACConfig()\n    .environment(\"Pendulum-v1\")\n    .env_runners(num_env_runners=1)\n    .training(\n        gamma=0.9,\n        actor_lr=0.001,\n        critic_lr=0.002,\n        train_batch_size_per_learner=32,\n    )\n)\n# Build the SAC algo object from the config and run 1 training iteration.\nalgo = config.build()\nalgo.train()\n\n\n\n\ntraining(*, twin_q: bool | None = <ray.rllib.utils.from_config._NotProvided object>, q_model_config: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, policy_model_config: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, tau: float | None = <ray.rllib.utils.from_config._NotProvided object>, initial_alpha: float | None = <ray.rllib.utils.from_config._NotProvided object>, target_entropy: str | float | None = <ray.rllib.utils.from_config._NotProvided object>, n_step: int | ~typing.Tuple[int, int] | None = <ray.rllib.utils.from_config._NotProvided object>, store_buffer_in_checkpoints: bool | None = <ray.rllib.utils.from_config._NotProvided object>, replay_buffer_config: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, training_intensity: float | None = <ray.rllib.utils.from_config._NotProvided object>, clip_actions: bool | None = <ray.rllib.utils.from_config._NotProvided object>, grad_clip: float | None = <ray.rllib.utils.from_config._NotProvided object>, optimization_config: ~typing.Dict[str, ~typing.Any] | None = <ray.rllib.utils.from_config._NotProvided object>, actor_lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, critic_lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, alpha_lr: float | ~typing.List[~typing.List[int | float]] | ~typing.List[~typing.Tuple[int, int | float]] | None = <ray.rllib.utils.from_config._NotProvided object>, target_network_update_freq: int | None = <ray.rllib.utils.from_config._NotProvided object>, _deterministic_loss: bool | None = <ray.rllib.utils.from_config._NotProvided object>, _use_beta_distribution: bool | None = <ray.rllib.utils.from_config._NotProvided object>, num_steps_sampled_before_learning_starts: int | None = <ray.rllib.utils.from_config._NotProvided object>, **kwargs) \u2192 SACConfig[source]#\nSets the training related configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\n\n\nHigh-Throughput On- and Off-Policy#\n\nAsynchronous Proximal Policy Optimization (APPO)#\n\nTip\nAPPO was originally published under the name \u201cIMPACT\u201d. RLlib\u2019s APPO exactly matches the algorithm described in the paper.\n\n[paper]\n[implementation]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Parameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.\n\n\n\n\n\n\n\nHigh-Throughput On- and Off-Policy#\n\nAsynchronous Proximal Policy Optimization (APPO)#\n\nTip\nAPPO was originally published under the name \u201cIMPACT\u201d. RLlib\u2019s APPO exactly matches the algorithm described in the paper.\n\n[paper]\n[implementation]\n\n\n\nAPPO architecture: APPO is an asynchronous variant of Proximal Policy Optimization (PPO) based on the IMPALA architecture,\nbut using a surrogate policy loss with clipping, allowing for multiple SGD passes per collected train batch.\nIn a training iteration, APPO requests samples from all EnvRunners asynchronously and the collected episode\nsamples are returned to the main algorithm process as Ray references rather than actual objects available on the local process.\nAPPO then passes these episode references to the Learners for asynchronous updates of the model.\nRLlib doesn\u2019t always synch back the weights to the EnvRunners right after a new model version is available.\nTo account for the EnvRunners being off-policy, APPO uses a procedure called v-trace,\ndescribed in the IMPALA paper.\nAPPO scales out on both axes, supporting multiple EnvRunners for sample collection and multiple GPU- or CPU-based Learners\nfor updating the model.#\n\n\nTuned examples:\nPong-v5\nHalfCheetah-v4\nAPPO-specific configs (see also generic algorithm settings):\n\n\nclass ray.rllib.algorithms.appo.appo.APPOConfig(algo_class=None)[source]#\nDefines a configuration class from which an APPO Algorithm can be built.\nfrom ray.rllib.algorithms.appo import APPOConfig\nconfig = (\n    APPOConfig()\n    .training(lr=0.01, grad_clip=30.0, train_batch_size_per_learner=50)\n)\nconfig = config.learners(num_learners=1)\nconfig = config.env_runners(num_env_runners=1)\nconfig = config.environment(\"CartPole-v1\")\n\n# Build an Algorithm object from the config and run 1 training iteration.\nalgo = config.build()\nalgo.train()\ndel algo\n\n\nfrom ray.rllib.algorithms.appo import APPOConfig\nfrom ray import tune\n\nconfig = APPOConfig()\n# Update the config object.\nconfig = config.training(lr=tune.grid_search([0.001,]))\n# Set the config object's env.\nconfig = config.environment(env=\"CartPole-v1\")\n# Use to_dict() to get the old-style python config dict when running with tune.\ntune.Tuner(\n    \"APPO\",\n    run_config=tune.RunConfig(\n        stop={\"training_iteration\": 1},\n        verbose=0,\n    ),\n    param_space=config.to_dict(),\n\n).fit()\n\n\n\n\ntraining(*, vtrace: bool | None = <ray.rllib.utils.from_config._NotProvided object>, use_gae: bool | None = <ray.rllib.utils.from_config._NotProvided object>, lambda_: float | None = <ray.rllib.utils.from_config._NotProvided object>, clip_param: float | None = <ray.rllib.utils.from_config._NotProvided object>, use_kl_loss: bool | None = <ray.rllib.utils.from_config._NotProvided object>, kl_coeff: float | None = <ray.rllib.utils.from_config._NotProvided object>, kl_target: float | None = <ray.rllib.utils.from_config._NotProvided object>, target_network_update_freq: int | None = <ray.rllib.utils.from_config._NotProvided object>, tau: float | None = <ray.rllib.utils.from_config._NotProvided object>, target_worker_clipping: float | None = <ray.rllib.utils.from_config._NotProvided object>, circular_buffer_num_batches: int | None = <ray.rllib.utils.from_config._NotProvided object>, circular_buffer_iterations_per_batch: int | None = <ray.rllib.utils.from_config._NotProvided object>, target_update_frequency=-1, use_critic=-1, **kwargs) \u2192 APPOConfig[source]#\nSets the training related configuration.\n\nParameters:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '@ray.remote\ndef f():\n    pass\n\n@ray.remote\nclass A:\n    def f(self):\n        pass\n\nstart = time.time()\nbad_env = {\"conda\": {\"dependencies\": [\"this_doesnt_exist\"]}}\n\n# [Tasks] will raise `RuntimeEnvSetupError`.\ntry:\n  ray.get(f.options(runtime_env=bad_env).remote())\nexcept ray.exceptions.RuntimeEnvSetupError:\n  print(\"Task fails with RuntimeEnvSetupError\")\n\n# [Actors] will raise `RuntimeEnvSetupError`.\na = A.options(runtime_env=bad_env).remote()\ntry:\n  ray.get(a.f.remote())\nexcept ray.exceptions.RuntimeEnvSetupError:\n  print(\"Actor fails with RuntimeEnvSetupError\")\n\n\nTask fails with RuntimeEnvSetupError\nActor fails with RuntimeEnvSetupError\n\n\nFull logs can always be found in the file runtime_env_setup-[job_id].log for per-actor, per-task and per-job environments, or in\nruntime_env_setup-ray_client_server_[port].log for per-job environments when using Ray Client.\nYou can also enable runtime_env debugging log streaming by setting an environment variable RAY_RUNTIME_ENV_LOG_TO_DRIVER_ENABLED=1 on each node before starting Ray, for example using setup_commands in the Ray Cluster configuration file (reference).\nThis will print the full runtime_env setup log messages to the driver (the script that calls ray.init()).\nExample log output:\nray.init(runtime_env={\"pip\": [\"requests\"]})\n\n\nSee Logging Directory Structure for more details.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'search_space = {\n    \"steps\": 100,\n    \"width\": tune.uniform(0, 20),\n    \"height\": tune.uniform(-100, 100),\n}\n\n\n\n\nFinally, we run the experiment to \"min\"imize the \u201cmean_loss\u201d of the objective by searching search_config via algo, num_samples times. This previous sentence is fully characterizes the search problem we aim to solve. With this in mind, notice how efficient it is to execute tuner.fit().\n\n\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n        metric=\"mean_loss\",\n        mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space=search_space,\n)\nresults = tuner.fit()\n\n\n\n\n\n\n\n== Status ==Current time: 2022-07-22 15:30:53 (running for 00:00:43.91)Memory usage on this node: 10.4/16.0 GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.47 GiB heap, 0.0/2.0 GiB objectsCurrent best trial: d42ac71c with mean_loss=-9.536507956046009 and parameters={'steps': 100, 'width': 19.398197043239886, 'height': -95.88310114083951}Result logdir: /Users/kai/ray_results/objective_2022-07-22_15-30-08Number of trials: 10/10 (10 TERMINATED)\n\nTrial name        status    loc              height   width    loss  iter  total time (s)  iterations  neg_mean_loss\n\n\nobjective_c9daa5d4TERMINATED127.0.0.1:46960-25.092 19.0143 -2.45636   100         10.9865          99        2.45636\nobjective_cb9bc830TERMINATED127.0.0.1:46968 46.398811.9732  4.72354   100         11.5661          99       -4.72354\nobjective_cb9d338cTERMINATED127.0.0.1:46969-68.7963 3.11989-6.56602   100         11.648           99        6.56602\nobjective_cb9e97e0TERMINATED127.0.0.1:46970-88.383317.3235 -8.78036   100         11.6948          99        8.78036\nobjective_d229961eTERMINATED127.0.0.1:47009 20.223 14.1615  2.09312   100         10.8549          99       -2.09312\nobjective_d42ac71cTERMINATED127.0.0.1:47036-95.883119.3982 -9.53651   100         10.7931          99        9.53651\nobjective_d43ca61cTERMINATED127.0.0.1:47039 66.4885 4.24678 6.88118   100         10.7606          99       -6.88118\nobjective_d43fb190TERMINATED127.0.0.1:47040-63.635  3.66809-6.09551   100         10.7997          99        6.09551\nobjective_da1ff46cTERMINATED127.0.0.1:47057-39.151610.4951 -3.81983   100         10.7762          99        3.81983\nobjective_dc25c796TERMINATED127.0.0.1:47062-13.611  5.82458-1.19064   100         10.7213          99        1.19064\n\n\n\n\nHere are the hyperparamters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 19.398197043239886, 'height': -95.88310114083951}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Global configuration#\n\n\nclass ray.data.DataContext(target_max_block_size: int = 134217728, target_shuffle_max_block_size: int = 1073741824, target_min_block_size: int = 1048576, streaming_read_buffer_size: int = 33554432, enable_pandas_block: bool = True, actor_prefetcher_enabled: bool = False, use_push_based_shuffle: bool = False, _shuffle_strategy: ~ray.data.context.ShuffleStrategy = ShuffleStrategy.SORT_SHUFFLE_PULL_BASED, pipeline_push_based_shuffle_reduce_tasks: bool = True, max_hash_shuffle_aggregators: int | None = 64, max_hash_shuffle_finalization_batch_size: int | None = None, join_operator_actor_num_cpus_per_partition_override: float = None, hash_shuffle_operator_actor_num_cpus_per_partition_override: float = None, hash_aggregate_operator_actor_num_cpus_per_partition_override: float = None, scheduling_strategy: None | str | ~ray.util.scheduling_strategies.PlacementGroupSchedulingStrategy | ~ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy | ~ray.util.scheduling_strategies.NodeLabelSchedulingStrategy = 'SPREAD', scheduling_strategy_large_args: None | str | ~ray.util.scheduling_strategies.PlacementGroupSchedulingStrategy | ~ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy | ~ray.util.scheduling_strategies.NodeLabelSchedulingStrategy = 'DEFAULT', large_args_threshold: int = 52428800, use_polars: bool = False, eager_free: bool = True, decoding_size_estimation: bool = True, min_parallelism: int = 200, read_op_min_num_blocks: int = 200, enable_tensor_extension_casting: bool = True, use_arrow_tensor_v2: bool = True, enable_fallback_to_arrow_object_ext_type: bool | None = None, enable_auto_log_stats: bool = False, verbose_stats_logs: bool = False, trace_allocations: bool = False, execution_options: ExecutionOptions = <factory>, use_ray_tqdm: bool = True, enable_progress_bars: bool = True, enable_operator_progress_bars: bool = True, enable_progress_bar_name_truncation: bool = True, enable_get_object_locations_for_metrics: bool = False, write_file_retry_on_errors: ~typing.List[str] = ('AWS Error INTERNAL_FAILURE', 'AWS Error NETWORK_CONNECTION', 'AWS Error SLOW_DOWN', 'AWS Error UNKNOWN (HTTP status 503)'), warn_on_driver_memory_usage_bytes: int = 2147483648, actor_task_retry_on_errors: bool | ~typing.List[BaseException] = False, op_resource_reservation_enabled: bool = True, op_resource_reservation_ratio: float = 0.5, max_errored_blocks: int = 0, log_internal_stack_trace_to_stdout: bool = False, raise_original_map_exception: bool = False, print_on_execution_start: bool = True, s3_try_create_dir: bool = False, wait_for_min_actors_s: int = 600, retried_io_errors: ~typing.List[str] = <factory>, enable_per_node_metrics: bool = False, override_object_store_memory_limit_fraction: float = None, memory_usage_poll_interval_s: float | None = 1, dataset_logger_id: str | None = None)[source]#\nGlobal settings for Ray Data.\nConfigure this class to enable advanced features and tune performance.\n\nWarning\nApply changes before creating a Dataset. Changes made after\nwon\u2019t take effect.\n\n\nNote\nThis object is automatically propagated to workers. Access it from the driver\nand remote workers with DataContext.get_current().\n\nExamples\n>>> from ray.data import DataContext\n>>> DataContext.get_current().enable_progress_bars = False\n\n\n\nParameters:\n\n\n\nDeveloperAPI: This API may change across minor Ray releases.\n\n\n\nDataContext.get_current\nGet or create the current DataContext.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'INFO:ray._private.runtime_env.plugin_schema_manager:Loading the default runtime env schemas: ['/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/working_dir_schema.json', '/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/pip_schema.json'].\n\n\nINFO:ray.tune.tune:Total run time: 44.70 seconds (43.68 seconds for the tuning loop).\n\n\n\n\nHere are the hyperparamters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 2.1174116156230918, 'height': -90.50653873694615, 'activation': 'relu, tanh'}\n\n\n\n\n\nOptional: passing the (hyper)parameter space into the search algorithm#\nWe can also pass the search space into NevergradSearch using their designed format.\n\n\nspace = ng.p.Dict(\n    width=ng.p.Scalar(lower=0, upper=20),\n    height=ng.p.Scalar(lower=-100, upper=100),\n    activation=ng.p.Choice(choices=[\"relu\", \"tanh\"])\n)\n\n\n\n\n\n\nalgo = NevergradSearch(\n    optimizer=ng.optimizers.OnePlusOne,\n    space=space,\n    metric=\"mean_loss\",\n    mode=\"min\"\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\n\n\n\n\nAgain we run the experiment, this time with a less passed via the config and instead passed through search_alg.\n\n\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n#         metric=\"mean_loss\",\n#         mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space={\"steps\": 100},\n)\nresults = tuner.fit()\n\n\n\n\n== Status ==Current time: 2022-07-22 15:25:27 (running for 00:00:43.22)Memory usage on this node: 10.8/16.0 GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.61 GiB heap, 0.0/2.0 GiB objectsResult logdir: /Users/kai/ray_results/objective_2022-07-22_15-24-44Number of trials: 10/10 (10 TERMINATED)\n\nTrial name        status    loc            activation     height   width     loss  iter  total time (s)  iterations  neg_mean_loss': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'INFO:ray._private.runtime_env.plugin_schema_manager:Loading the default runtime env schemas: ['/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/working_dir_schema.json', '/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/pip_schema.json'].\n\n\nINFO:ray.tune.tune:Total run time: 44.70 seconds (43.68 seconds for the tuning loop).\n\n\n\n\nHere are the hyperparamters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 2.1174116156230918, 'height': -90.50653873694615, 'activation': 'relu, tanh'}\n\n\n\n\n\nOptional: passing the (hyper)parameter space into the search algorithm#\nWe can also pass the search space into NevergradSearch using their designed format.\n\n\nspace = ng.p.Dict(\n    width=ng.p.Scalar(lower=0, upper=20),\n    height=ng.p.Scalar(lower=-100, upper=100),\n    activation=ng.p.Choice(choices=[\"relu\", \"tanh\"])\n)\n\n\n\n\n\n\nalgo = NevergradSearch(\n    optimizer=ng.optimizers.OnePlusOne,\n    space=space,\n    metric=\"mean_loss\",\n    mode=\"min\"\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\n\n\n\n\nAgain we run the experiment, this time with a less passed via the config and instead passed through search_alg.\n\n\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n#         metric=\"mean_loss\",\n#         mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space={\"steps\": 100},\n)\nresults = tuner.fit()\n\n\n\n\n== Status ==Current time: 2022-07-22 15:25:27 (running for 00:00:43.22)Memory usage on this node: 10.8/16.0 GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.61 GiB heap, 0.0/2.0 GiB objectsResult logdir: /Users/kai/ray_results/objective_2022-07-22_15-24-44Number of trials: 10/10 (10 TERMINATED)\n\nTrial name        status    loc            activation     height   width     loss  iter  total time (s)  iterations  neg_mean_loss': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'INFO:ray._private.runtime_env.plugin_schema_manager:Loading the default runtime env schemas: ['/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/working_dir_schema.json', '/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/pip_schema.json'].\n\n\nINFO:ray.tune.tune:Total run time: 44.70 seconds (43.68 seconds for the tuning loop).\n\n\n\n\nHere are the hyperparamters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 2.1174116156230918, 'height': -90.50653873694615, 'activation': 'relu, tanh'}\n\n\n\n\n\nOptional: passing the (hyper)parameter space into the search algorithm#\nWe can also pass the search space into NevergradSearch using their designed format.\n\n\nspace = ng.p.Dict(\n    width=ng.p.Scalar(lower=0, upper=20),\n    height=ng.p.Scalar(lower=-100, upper=100),\n    activation=ng.p.Choice(choices=[\"relu\", \"tanh\"])\n)\n\n\n\n\n\n\nalgo = NevergradSearch(\n    optimizer=ng.optimizers.OnePlusOne,\n    space=space,\n    metric=\"mean_loss\",\n    mode=\"min\"\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\n\n\n\n\nAgain we run the experiment, this time with a less passed via the config and instead passed through search_alg.\n\n\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n#         metric=\"mean_loss\",\n#         mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space={\"steps\": 100},\n)\nresults = tuner.fit()\n\n\n\n\n== Status ==Current time: 2022-07-22 15:25:27 (running for 00:00:43.22)Memory usage on this node: 10.8/16.0 GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.61 GiB heap, 0.0/2.0 GiB objectsResult logdir: /Users/kai/ray_results/objective_2022-07-22_15-24-44Number of trials: 10/10 (10 TERMINATED)\n\nTrial name        status    loc            activation     height   width     loss  iter  total time (s)  iterations  neg_mean_loss': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'INFO:ray._private.runtime_env.plugin_schema_manager:Loading the default runtime env schemas: ['/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/working_dir_schema.json', '/Users/kai/coding/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/pip_schema.json'].\n\n\nINFO:ray.tune.tune:Total run time: 44.70 seconds (43.68 seconds for the tuning loop).\n\n\n\n\nHere are the hyperparamters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 2.1174116156230918, 'height': -90.50653873694615, 'activation': 'relu, tanh'}\n\n\n\n\n\nOptional: passing the (hyper)parameter space into the search algorithm#\nWe can also pass the search space into NevergradSearch using their designed format.\n\n\nspace = ng.p.Dict(\n    width=ng.p.Scalar(lower=0, upper=20),\n    height=ng.p.Scalar(lower=-100, upper=100),\n    activation=ng.p.Choice(choices=[\"relu\", \"tanh\"])\n)\n\n\n\n\n\n\nalgo = NevergradSearch(\n    optimizer=ng.optimizers.OnePlusOne,\n    space=space,\n    metric=\"mean_loss\",\n    mode=\"min\"\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\n\n\n\n\nAgain we run the experiment, this time with a less passed via the config and instead passed through search_alg.\n\n\ntuner = tune.Tuner(\n    objective,\n    tune_config=tune.TuneConfig(\n#         metric=\"mean_loss\",\n#         mode=\"min\",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space={\"steps\": 100},\n)\nresults = tuner.fit()\n\n\n\n\n== Status ==Current time: 2022-07-22 15:25:27 (running for 00:00:43.22)Memory usage on this node: 10.8/16.0 GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.61 GiB heap, 0.0/2.0 GiB objectsResult logdir: /Users/kai/ray_results/objective_2022-07-22_15-24-44Number of trials: 10/10 (10 TERMINATED)\n\nTrial name        status    loc            activation     height   width     loss  iter  total time (s)  iterations  neg_mean_loss': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '== Status ==Current time: 2022-07-22 15:25:27 (running for 00:00:43.22)Memory usage on this node: 10.8/16.0 GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.61 GiB heap, 0.0/2.0 GiB objectsResult logdir: /Users/kai/ray_results/objective_2022-07-22_15-24-44Number of trials: 10/10 (10 TERMINATED)\n\nTrial name        status    loc            activation     height   width     loss  iter  total time (s)  iterations  neg_mean_loss\n\n\nobjective_085274beTERMINATED127.0.0.1:46516tanh          0      10       1.1        100         10.7324          99      -1.1     \nobjective_09dee4f2TERMINATED127.0.0.1:46524tanh        -44.4216 12.9653 -3.36485    100         11.3476          99       3.36485 \nobjective_09e0846aTERMINATED127.0.0.1:46525relu        -38.0638 11.1574  6.28334    100         11.3103          99      -6.28334 \nobjective_09e21122TERMINATED127.0.0.1:46526relu         41.2509  9.7558514.2276     100         11.3512          99     -14.2276  \nobjective_1045958eTERMINATED127.0.0.1:46544relu        -73.2818  5.78832 2.84334    100         10.7372          99      -2.84334 \nobjective_12309db2TERMINATED127.0.0.1:46549relu        -94.9666 16.9764  0.562486   100         10.7329          99      -0.562486\nobjective_12342770TERMINATED127.0.0.1:46550tanh        -98.0775 17.2252 -8.74945    100         10.7455          99       8.74945 \nobjective_12374d7eTERMINATED127.0.0.1:46551relu         -1.6075918.0841  9.89479    100         10.7348          99      -9.89479 \nobjective_18344524TERMINATED127.0.0.1:46569tanh        -41.1284 12.2952 -3.03135    100         12.622           99       3.03135 \nobjective_1a1e29b8TERMINATED127.0.0.1:46576tanh         64.0289 10.0482  7.50242    100         10.8237          99      -7.50242 \n\n\nINFO:ray.tune.tune:Total run time: 43.33 seconds (43.21 seconds for the tuning loop).\n\n\n\n\nHere are the hyperparamters found to minimize the mean loss of the defined objective. Note that we have to pass the metric and mode here because we don\u2019t set it in the TuneConfig.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result(\"mean_loss\", \"min\").config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 17.225166732233465, 'height': -98.07750812064515, 'activation': 'tanh'}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'ray.rllib.algorithms.algorithm_config.AlgorithmConfig.env_runners#\n\n\nAlgorithmConfig.env_runners(*, env_runner_cls: type | None = <ray.rllib.utils.from_config._NotProvided object>, num_env_runners: int | None = <ray.rllib.utils.from_config._NotProvided object>, create_local_env_runner: bool | None = <ray.rllib.utils.from_config._NotProvided object>, create_env_on_local_worker: bool | None = <ray.rllib.utils.from_config._NotProvided object>, num_envs_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, gym_env_vectorize_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, num_cpus_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, num_gpus_per_env_runner: float | int | None = <ray.rllib.utils.from_config._NotProvided object>, custom_resources_per_env_runner: dict | None = <ray.rllib.utils.from_config._NotProvided object>, validate_env_runners_after_construction: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sample_timeout_s: float | None = <ray.rllib.utils.from_config._NotProvided object>, max_requests_in_flight_per_env_runner: int | None = <ray.rllib.utils.from_config._NotProvided object>, env_to_module_connector: ~typing.Callable[[~typing.Any | gymnasium.Env], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, module_to_env_connector: ~typing.Callable[[~typing.Any | gymnasium.Env, RLModule], ConnectorV2 | ~typing.List[ConnectorV2]] | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_env_to_module_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, add_default_connectors_to_module_to_env_pipeline: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episode_lookback_horizon: int | None = <ray.rllib.utils.from_config._NotProvided object>, merge_env_runner_states: str | bool | None = <ray.rllib.utils.from_config._NotProvided object>, broadcast_env_runner_states: bool | None = <ray.rllib.utils.from_config._NotProvided object>, compress_observations: bool | None = <ray.rllib.utils.from_config._NotProvided object>, rollout_fragment_length: int | str | None = <ray.rllib.utils.from_config._NotProvided object>, batch_mode: str | None = <ray.rllib.utils.from_config._NotProvided object>, explore: bool | None = <ray.rllib.utils.from_config._NotProvided object>, episodes_to_numpy: bool | None = <ray.rllib.utils.from_config._NotProvided object>, use_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, update_worker_filter_stats: bool | None = <ray.rllib.utils.from_config._NotProvided object>, exploration_config: dict | None = <ray.rllib.utils.from_config._NotProvided object>, sample_collector: ~typing.Type[~ray.rllib.evaluation.collectors.sample_collector.SampleCollector] | None = <ray.rllib.utils.from_config._NotProvided object>, remote_worker_envs: bool | None = <ray.rllib.utils.from_config._NotProvided object>, remote_env_batch_wait_ms: float | None = <ray.rllib.utils.from_config._NotProvided object>, preprocessor_pref: str | None = <ray.rllib.utils.from_config._NotProvided object>, observation_filter: str | None = <ray.rllib.utils.from_config._NotProvided object>, enable_tf1_exec_eagerly: bool | None = <ray.rllib.utils.from_config._NotProvided object>, sampler_perf_stats_ema_coef: float | None = <ray.rllib.utils.from_config._NotProvided object>, num_rollout_workers=-1, num_envs_per_worker=-1, validate_workers_after_construction=-1, ignore_worker_failures=-1, recreate_failed_workers=-1, restart_failed_sub_environments=-1, num_consecutive_worker_failures_tolerance=-1, worker_health_probe_timeout_s=-1, worker_restore_timeout_s=-1, synchronize_filter=-1, enable_connectors=-1) \u2192 AlgorithmConfig[source]#\nSets the rollout worker configuration.\n\nParameters:\n\n\nReturns:\nThis updated AlgorithmConfig object.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '-n, --name <name>#\nName of an application. If set, this will display only the status of the specified application.\n\n\n\n\n\nServe REST API#\nThe Serve REST API is exposed at the same port as the Ray Dashboard. The Dashboard port is 8265 by default. This port can be changed using the --dashboard-port argument when running ray start. All example requests in this section use the default port.\n\nPUT \"/api/serve/applications/\"#\nDeclaratively deploys a list of Serve applications. If Serve is already running on the Ray cluster, removes all applications not listed in the new config. If Serve is not running on the Ray cluster, starts Serve. See multi-app config schema for the request\u2019s JSON schema.\nExample Request:\nPUT /api/serve/applications/ HTTP/1.1\nHost: http://localhost:8265/\nAccept: application/json\nContent-Type: application/json\n\n{\n    \"applications\": [\n        {\n            \"name\": \"text_app\",\n            \"route_prefix\": \"/\",\n            \"import_path\": \"text_ml:app\",\n            \"runtime_env\": {\n                \"working_dir\": \"https://github.com/ray-project/serve_config_examples/archive/HEAD.zip\"\n            },\n            \"deployments\": [\n                {\"name\": \"Translator\", \"user_config\": {\"language\": \"french\"}},\n                {\"name\": \"Summarizer\"},\n            ]\n        },\n    ]\n}\n\n\nExample Response\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n\n\n\nGET \"/api/serve/applications/\"#\nGets cluster-level info and comprehensive details on all Serve applications deployed on the Ray cluster. See metadata schema for the response\u2019s JSON schema.\nGET /api/serve/applications/ HTTP/1.1\nHost: http://localhost:8265/\nAccept: application/json\n\n\nExample Response (abridged JSON):\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n\n\n\nDELETE \"/api/serve/applications/\"#\nShuts down Serve and all applications running on the Ray cluster. Has no effect if Serve is not running on the Ray cluster.\nExample Request:\nDELETE /api/serve/applications/ HTTP/1.1\nHost: http://localhost:8265/\nAccept: application/json\n\n\nExample Response\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n\n\n\n\nConfig Schemas#\n\n\nschema.ServeDeploySchema\nMulti-application config for deploying a list of Serve applications to the Ray cluster.\n\nschema.gRPCOptionsSchema\nOptions to start the gRPC Proxy with.\n\nschema.HTTPOptionsSchema\nOptions to start the HTTP Proxy with.\n\nschema.ServeApplicationSchema\nDescribes one Serve application, and currently can also be used as a standalone config to deploy a single application to a Ray cluster.\n\nschema.DeploymentSchema\nSpecifies options for one deployment within a Serve application.\n\nschema.RayActorOptionsSchema\nOptions with which to start a replica actor.\n\n\n\n\n\nResponse Schemas#\n\n\nschema.ServeInstanceDetails\nServe metadata with system-level info and details on all applications deployed to the Ray cluster.\n\nschema.APIType\nTracks the type of API that an application originates from.\n\nschema.ApplicationStatus\nThe current status of the application.\n\nschema.ApplicationDetails\nDetailed info about a Serve application.\n\nschema.DeploymentDetails\nDetailed info about a deployment within a Serve application.\n\nschema.ReplicaDetails\nDetailed info about a single deployment replica.\n\nschema.ProxyStatus\nThe current status of the proxy.\n\nschema.TargetGroup\nPublicAPI (alpha): This API is in alpha and may change before becoming stable.\n\nschema.Target\nPublicAPI (alpha): This API is in alpha and may change before becoming stable.\n\n\n\n\n\nObservability#\n\n\nmetrics.Counter\nA serve cumulative metric that is monotonically increasing.\n\nmetrics.Histogram\nTracks the size and number of events in buckets.\n\nmetrics.Gauge\nGauges keep the last recorded value and drop everything before.\n\nschema.LoggingConfig\nLogging config schema for configuring serve components logs.\n\n\n\n\n\nLLM API#\n\nBuilders#\n\n\nserve.llm.build_llm_deployment\nHelper to build a single vllm deployment from the given llm config.\n\nserve.llm.build_openai_app\nHelper to build an OpenAI compatible app with the llm deployment setup from the given llm serving args.\n\n\n\n\n\nConfigs#\n\n\nserve.llm.LLMConfig\nThe configuration for starting an LLM deployment.\n\nserve.llm.LLMServingArgs\nThe configuration for starting an LLM deployment application.\n\nserve.llm.ModelLoadingConfig\nThe configuration for loading an LLM model.\n\nserve.llm.CloudMirrorConfig\nThe configuration for mirroring an LLM model from cloud storage.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Launch Ray in Docker#\nStart out by launching the deployment container.\ndocker run --shm-size=<shm-size> -t -i rayproject/ray\n\n\nReplace <shm-size> with a limit appropriate for your system, for example\n512M or 2G. A good estimate for this is to use roughly 30% of your available memory (this is\nwhat Ray uses internally for its Object Store). The -t and -i options here are required to support\ninteractive use of the container.\nIf you use a GPU version Docker image, remember to add --gpus all option. Replace <ray-version> with your target ray version in the following command:\ndocker run --shm-size=<shm-size> -t -i --gpus all rayproject/ray:<ray-version>-gpu\n\n\nNote: Ray requires a large amount of shared memory because each object\nstore keeps all of its objects in shared memory, so the amount of shared memory\nwill limit the size of the object store.\nYou should now see a prompt that looks something like:\nroot@ebc78f68d100:/ray#\n\n\n\n\nTest if the installation succeeded#\nTo test if the installation was successful, try running some tests. This assumes\nthat you\u2019ve cloned the git repository.\npython -m pytest -v python/ray/tests/test_mini.py\n\n\n\n\nInstalled Python dependencies#\nOur docker images are shipped with pre-installed Python dependencies\nrequired for Ray and its libraries.\nWe publish the dependencies that are installed in our ray Docker images for Python 3.9.\n\n\n\n\n\n\n\n\n\nInstall Ray Java with Maven#\n\nNote\nAll Ray Java APIs are experimental and only supported by the community.\n\nBefore installing Ray Java with Maven, you should install Ray Python with pip install -U ray . Note that the versions of Ray Java and Ray Python must match.\nNote that nightly Ray python wheels are also required if you want to install Ray Java snapshot version.\nFind the latest Ray Java release in the central repository. To use the latest Ray Java release in your application, add the following entries in your pom.xml:\n<dependency>\n  <groupId>io.ray</groupId>\n  <artifactId>ray-api</artifactId>\n  <version>${ray.version}</version>\n</dependency>\n<dependency>\n  <groupId>io.ray</groupId>\n  <artifactId>ray-runtime</artifactId>\n  <version>${ray.version}</version>\n</dependency>\n\n\nThe latest Ray Java snapshot can be found in sonatype repository. To use the latest Ray Java snapshot in your application, add the following entries in your pom.xml:\n<!-- only needed for snapshot version of ray -->\n<repositories>\n  <repository>\n    <id>sonatype</id>\n    <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\n    <releases>\n      <enabled>false</enabled>\n    </releases>\n    <snapshots>\n      <enabled>true</enabled>\n    </snapshots>\n  </repository>\n</repositories>\n\n<dependencies>\n  <dependency>\n    <groupId>io.ray</groupId>\n    <artifactId>ray-api</artifactId>\n    <version>${ray.version}</version>\n  </dependency>\n  <dependency>\n    <groupId>io.ray</groupId>\n    <artifactId>ray-runtime</artifactId>\n    <version>${ray.version}</version>\n  </dependency>\n</dependencies>\n\n\n\nNote\nWhen you run pip install to install Ray, Java jars are installed as well. The above dependencies are only used to build your Java code and to run your code in local mode.\nIf you want to run your Java code in a multi-node Ray cluster, it\u2019s better to exclude Ray jars when packaging your code to avoid jar conflicts if the versions (installed Ray with pip install and maven dependencies) don\u2019t match.\n\n\n\nInstall Ray C++#\n\nNote\nAll Ray C++ APIs are experimental and only supported by the community.\n\nYou can install and use Ray C++ API as follows.\npip install -U ray[cpp]\n\n# Create a Ray C++ project template to start with.\nray cpp --generate-bazel-project-template-to ray-template\n\n\n\nNote\nIf you build Ray from source, remove the build option build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" from the file cpp/example/.bazelrc before running your application. The related issue is this.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '[INFO 07-22 15:04:27] ax.service.ax_client: Completed trial 3 with data: {'landscape': (-0.011984, None), 'l2norm': (1.530347, None)}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Generated new trial 5 with parameters {'x1': 0.126064, 'x2': 0.703408, 'x3': 0.344681, 'x4': 0.337363, 'x5': 0.401396, 'x6': 0.679202, 'iterations': 100}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Completed trial 1 with data: {'landscape': (-0.11286, None), 'l2norm': (1.163407, None)}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Generated new trial 6 with parameters {'x1': 0.091094, 'x2': 0.304138, 'x3': 0.869848, 'x4': 0.405435, 'x5': 0.567922, 'x6': 0.228608, 'iterations': 100}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Completed trial 2 with data: {'landscape': (-0.11348, None), 'l2norm': (1.359954, None)}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Generated new trial 7 with parameters {'x1': 0.603178, 'x2': 0.409057, 'x3': 0.729056, 'x4': 0.082598, 'x5': 0.572948, 'x6': 0.508304, 'iterations': 100}.\n\n\n[INFO 07-22 15:04:30] ax.service.ax_client: Completed trial 4 with data: {'landscape': (-0.00678, None), 'l2norm': (1.80573, None)}.\n[INFO 07-22 15:04:30] ax.service.ax_client: Generated new trial 8 with parameters {'x1': 0.454189, 'x2': 0.271772, 'x3': 0.530871, 'x4': 0.991841, 'x5': 0.691843, 'x6': 0.472366, 'iterations': 100}.\n[INFO 07-22 15:04:30] ax.service.ax_client: Completed trial 5 with data: {'landscape': (-0.904622, None), 'l2norm': (1.168644, None)}.\n[INFO 07-22 15:04:30] ax.service.ax_client: Generated new trial 9 with parameters {'x1': 0.265264, 'x2': 0.924884, 'x3': 0.151716, 'x4': 0.436026, 'x5': 0.85731, 'x6': 0.08981, 'iterations': 100}.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '[INFO 07-22 15:04:27] ax.service.ax_client: Completed trial 3 with data: {'landscape': (-0.011984, None), 'l2norm': (1.530347, None)}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Generated new trial 5 with parameters {'x1': 0.126064, 'x2': 0.703408, 'x3': 0.344681, 'x4': 0.337363, 'x5': 0.401396, 'x6': 0.679202, 'iterations': 100}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Completed trial 1 with data: {'landscape': (-0.11286, None), 'l2norm': (1.163407, None)}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Generated new trial 6 with parameters {'x1': 0.091094, 'x2': 0.304138, 'x3': 0.869848, 'x4': 0.405435, 'x5': 0.567922, 'x6': 0.228608, 'iterations': 100}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Completed trial 2 with data: {'landscape': (-0.11348, None), 'l2norm': (1.359954, None)}.\n[INFO 07-22 15:04:27] ax.service.ax_client: Generated new trial 7 with parameters {'x1': 0.603178, 'x2': 0.409057, 'x3': 0.729056, 'x4': 0.082598, 'x5': 0.572948, 'x6': 0.508304, 'iterations': 100}.\n\n\n[INFO 07-22 15:04:30] ax.service.ax_client: Completed trial 4 with data: {'landscape': (-0.00678, None), 'l2norm': (1.80573, None)}.\n[INFO 07-22 15:04:30] ax.service.ax_client: Generated new trial 8 with parameters {'x1': 0.454189, 'x2': 0.271772, 'x3': 0.530871, 'x4': 0.991841, 'x5': 0.691843, 'x6': 0.472366, 'iterations': 100}.\n[INFO 07-22 15:04:30] ax.service.ax_client: Completed trial 5 with data: {'landscape': (-0.904622, None), 'l2norm': (1.168644, None)}.\n[INFO 07-22 15:04:30] ax.service.ax_client: Generated new trial 9 with parameters {'x1': 0.265264, 'x2': 0.924884, 'x3': 0.151716, 'x4': 0.436026, 'x5': 0.85731, 'x6': 0.08981, 'iterations': 100}.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Result for objective_313d3d3a:\n  date: 2022-07-22_15-04-30\n  done: true\n  experiment_id: fa7afd557e154fbebe4f54d8eedb3573\n  experiment_tag: 5_iterations=100,x1=0.0419,x2=0.9928,x3=0.9060,x4=0.5944,x5=0.8254,x6=0.6464\n  hostname: Kais-MacBook-Pro.local\n  iterations_since_restore: 100\n  l2norm: 1.805729990121368\n  landscape: -0.006779757704679272\n  node_ip: 127.0.0.1\n  pid: 44747\n  time_since_restore: 3.1623308658599854\n  time_this_iter_s: 0.02911996841430664\n  time_total_s: 3.1623308658599854\n  timestamp: 1658498670\n  timesteps_since_restore: 0\n  timesteps_total: 99\n  training_iteration: 100\n  trial_id: 313d3d3a\n  warmup_time: 0.0029790401458740234\n  \nResult for objective_32c9acd8:\n  date: 2022-07-22_15-04-30\n  done: true\n  experiment_id: c555bfed13ac43e5b8c8e9f6d4b9b2f7\n  experiment_tag: 6_iterations=100,x1=0.1261,x2=0.7034,x3=0.3447,x4=0.3374,x5=0.4014,x6=0.6792\n  hostname: Kais-MacBook-Pro.local\n  iterations_since_restore: 100\n  l2norm: 1.1686440476629836\n  landscape: -0.9046216637367911\n  node_ip: 127.0.0.1\n  pid: 44726\n  time_since_restore: 3.1211891174316406\n  time_this_iter_s: 0.02954697608947754\n  time_total_s: 3.1211891174316406\n  timestamp: 1658498670\n  timesteps_since_restore: 0\n  timesteps_total: 99\n  training_iteration: 100\n  trial_id: 32c9acd8\n  warmup_time: 0.0026290416717529297\n  \n\n\n[INFO 07-22 15:04:32] ax.service.ax_client: Completed trial 7 with data: {'landscape': (-0.247223, None), 'l2norm': (1.286911, None)}.\n[INFO 07-22 15:04:32] ax.service.ax_client: Completed trial 6 with data: {'landscape': (-0.146532, None), 'l2norm': (1.181781, None)}.\n\n\n[INFO 07-22 15:04:35] ax.service.ax_client: Completed trial 8 with data: {'landscape': (-0.013292, None), 'l2norm': (1.499166, None)}.\n[INFO 07-22 15:04:35] ax.service.ax_client: Completed trial 9 with data: {'landscape': (-1.662444, None), 'l2norm': (1.371845, None)}.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '[INFO 07-22 15:04:32] ax.service.ax_client: Completed trial 7 with data: {'landscape': (-0.247223, None), 'l2norm': (1.286911, None)}.\n[INFO 07-22 15:04:32] ax.service.ax_client: Completed trial 6 with data: {'landscape': (-0.146532, None), 'l2norm': (1.181781, None)}.\n\n\n[INFO 07-22 15:04:35] ax.service.ax_client: Completed trial 8 with data: {'landscape': (-0.013292, None), 'l2norm': (1.499166, None)}.\n[INFO 07-22 15:04:35] ax.service.ax_client: Completed trial 9 with data: {'landscape': (-1.662444, None), 'l2norm': (1.371845, None)}.\n\n\nResult for objective_34adf04a:\n  date: 2022-07-22_15-04-35\n  done: true\n  experiment_id: 4f65c5b68f5c49d98fda388e37c83deb\n  experiment_tag: 9_iterations=100,x1=0.4542,x2=0.2718,x3=0.5309,x4=0.9918,x5=0.6918,x6=0.4724\n  hostname: Kais-MacBook-Pro.local\n  iterations_since_restore: 100\n  l2norm: 1.4991655675380078\n  landscape: -0.01329150870283869\n  node_ip: 127.0.0.1\n  pid: 44768\n  time_since_restore: 2.7032668590545654\n  time_this_iter_s: 0.029300928115844727\n  time_total_s: 2.7032668590545654\n  timestamp: 1658498675\n  timesteps_since_restore: 0\n  timesteps_total: 99\n  training_iteration: 100\n  trial_id: 34adf04a\n  warmup_time: 0.0027239322662353516\n  \nResult for objective_34b7abda:\n  date: 2022-07-22_15-04-35\n  done: true\n  experiment_id: f135a2c40f5644ba9d2ae096a9dd10e0\n  experiment_tag: 10_iterations=100,x1=0.2653,x2=0.9249,x3=0.1517,x4=0.4360,x5=0.8573,x6=0.0898\n  hostname: Kais-MacBook-Pro.local\n  iterations_since_restore: 100\n  l2norm: 1.3718451333547932\n  landscape: -1.6624439263544026\n  node_ip: 127.0.0.1\n  pid: 44771\n  time_since_restore: 2.6852078437805176\n  time_this_iter_s: 0.029579877853393555\n  time_total_s: 2.6852078437805176\n  timestamp: 1658498675\n  timesteps_since_restore: 0\n  timesteps_total: 99\n  training_iteration: 100\n  trial_id: 34b7abda\n  warmup_time: 0.002721071243286133\n  \n\n\n\n\nAnd now we have the hyperparameters found to minimize the mean loss.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'iterations': 100, 'x1': 0.26526361983269453, 'x2': 0.9248840995132923, 'x3': 0.15171580761671066, 'x4': 0.43602637108415365, 'x5': 0.8573104059323668, 'x6': 0.08981018699705601}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpdj6sv23q/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73647) Extracting /tmp/tmpjm0jv6rr/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpjm0jv6rr/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=73647)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpdj6sv23q/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73647) Extracting /tmp/tmpjm0jv6rr/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpjm0jv6rr/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=73647)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpd1qkzrfz/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80951) Extracting /tmp/tmpyrcbok27/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyrcbok27/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=80951)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpdj6sv23q/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73647) Extracting /tmp/tmpjm0jv6rr/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpjm0jv6rr/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=73647)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpd1qkzrfz/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80951) Extracting /tmp/tmpyrcbok27/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyrcbok27/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=80951)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpdj6sv23q/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73647) Extracting /tmp/tmpjm0jv6rr/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpjm0jv6rr/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=73647)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpd1qkzrfz/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80951) Extracting /tmp/tmpyrcbok27/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyrcbok27/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=80951)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=101545) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95492) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=95492) Extracting /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=95492)  [repeated 12x across cluster]\n\n\n  0%|          | 0/9912422 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 104607984.65it/s]\n\n\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpdj6sv23q/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73647) Extracting /tmp/tmpjm0jv6rr/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpjm0jv6rr/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=73647)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpd1qkzrfz/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80951) Extracting /tmp/tmpyrcbok27/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyrcbok27/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=80951)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=101545) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95492) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=95492) Extracting /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=95492)  [repeated 12x across cluster]\n\n\n  0%|          | 0/9912422 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 104607984.65it/s]\n\n\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n\n\n(RayTrainWorker pid=108863) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Extracting /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw [repeated 8x across cluster]\n(RayTrainWorker pid=101546)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpdj6sv23q/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73647) Extracting /tmp/tmpjm0jv6rr/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpjm0jv6rr/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=73647)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpd1qkzrfz/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80951) Extracting /tmp/tmpyrcbok27/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyrcbok27/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=80951)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=101545) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95492) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=95492) Extracting /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=95492)  [repeated 12x across cluster]\n\n\n  0%|          | 0/9912422 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 104607984.65it/s]\n\n\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n\n\n(RayTrainWorker pid=108863) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Extracting /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw [repeated 8x across cluster]\n(RayTrainWorker pid=101546)  [repeated 12x across cluster]\n\n\n(autoscaler +11m23s) [workspace snapshot] New snapshot created successfully (Size: 327.01 KB)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpdj6sv23q/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73647) Extracting /tmp/tmpjm0jv6rr/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpjm0jv6rr/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=73647)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpd1qkzrfz/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80951) Extracting /tmp/tmpyrcbok27/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyrcbok27/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=80951)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=101545) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95492) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=95492) Extracting /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=95492)  [repeated 12x across cluster]\n\n\n  0%|          | 0/9912422 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 104607984.65it/s]\n\n\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n\n\n(RayTrainWorker pid=108863) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Extracting /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw [repeated 8x across cluster]\n(RayTrainWorker pid=101546)  [repeated 12x across cluster]\n\n\n(autoscaler +11m23s) [workspace snapshot] New snapshot created successfully (Size: 327.01 KB)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(autoscaler +11m23s) [workspace snapshot] New snapshot created successfully (Size: 327.01 KB)\n\n\n(RayTrainWorker pid=111131) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=111131) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpddnnc0iv/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=108863) Extracting /tmp/tmpxcg0v86z/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpxcg0v86z/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=108863)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpdj6sv23q/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73647) Extracting /tmp/tmpjm0jv6rr/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpjm0jv6rr/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=73647)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpd1qkzrfz/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80951) Extracting /tmp/tmpyrcbok27/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyrcbok27/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=80951)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=101545) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95492) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=95492) Extracting /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=95492)  [repeated 12x across cluster]\n\n\n  0%|          | 0/9912422 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 104607984.65it/s]\n\n\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n\n\n(RayTrainWorker pid=108863) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Extracting /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw [repeated 8x across cluster]\n(RayTrainWorker pid=101546)  [repeated 12x across cluster]\n\n\n(autoscaler +11m23s) [workspace snapshot] New snapshot created successfully (Size: 327.01 KB)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(autoscaler +11m23s) [workspace snapshot] New snapshot created successfully (Size: 327.01 KB)\n\n\n(RayTrainWorker pid=111131) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=111131) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpddnnc0iv/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=108863) Extracting /tmp/tmpxcg0v86z/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpxcg0v86z/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=108863)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=118364) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=118364) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmp0sbwiedt/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=111130) Extracting /tmp/tmpfmuq9_qh/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpfmuq9_qh/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=111130)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'System Info\n      Using AsyncHyperBand: num_stopped=10Bracket: Iter 4.000: 0.9709362387657166 | Iter 2.000: 0.9617255330085754 | Iter 1.000: 0.9477165043354034Logical resource usage: 4.0/48 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:None)\n    \n\n\n\nTrial Status\n\n\nTrial name              status    loc                train_loop_config/ba\ntch_size    train_loop_config/la\nyer_1_size    train_loop_config/la\nyer_2_size  train_loop_config/lr  iter  total time (s)  ptl/train_loss  ptl/train_accuracy  ptl/val_loss\n\n\nTorchTrainer_5144b_00000TERMINATED10.0.0.84:63990 32 64256           0.0316233       5         29.3336      0.973613              0.766667     0.580943 \nTorchTrainer_5144b_00001TERMINATED10.0.0.84:71294 64128 64           0.0839278       1         12.2275      2.19514               0.266667     1.56644  \nTorchTrainer_5144b_00002TERMINATED10.0.0.84:73540 32 64256           0.000233034     5         29.1314      0.146903              0.933333     0.114229 \nTorchTrainer_5144b_00003TERMINATED10.0.0.84:80840 64128 64           0.00109259      5         21.6534      0.0474913             0.966667     0.0714878\nTorchTrainer_5144b_00004TERMINATED10.0.0.84:88077 32 32128           0.00114083      5         29.6367      0.0990443             0.966667     0.0891999\nTorchTrainer_5144b_00005TERMINATED10.0.0.84:95388 32 64 64           0.00924264      4         25.7089      0.0349707             1            0.153937 \nTorchTrainer_5144b_00006TERMINATED10.0.0.84:10143432128256           0.00325671      5         29.5763      0.0708755             0.966667     0.0820903\nTorchTrainer_5144b_00007TERMINATED10.0.0.84:10875032 32 64           0.000123766     1         13.9326      0.27464               0.966667     0.401102 \nTorchTrainer_5144b_00008TERMINATED10.0.0.84:11101964128256           0.00371762      5         21.8337      0.00108961            1            0.0579874\nTorchTrainer_5144b_00009TERMINATED10.0.0.84:11825532128128           0.00397956      5         29.8334      0.00940019            1            0.0685028\n\n\n\n\n\n\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n(RayTrainWorker pid=64102) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpydcy4598/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 120812916.07it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 101305832.98it/s]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=71408) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=64101) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=64101) Extracting /tmp/tmpt8k8jglf/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt8k8jglf/MNIST/raw [repeated 11x across cluster]\n(RayTrainWorker pid=64101)  [repeated 11x across cluster]\n\n\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73648) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpcy67mfe_/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=71409) Extracting /tmp/tmpmxchio03/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpmxchio03/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=71409)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80950) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpdj6sv23q/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=73647) Extracting /tmp/tmpjm0jv6rr/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpjm0jv6rr/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=73647)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88186) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpd1qkzrfz/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=80951) Extracting /tmp/tmpyrcbok27/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyrcbok27/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=80951)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95494) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpkvf1rrst/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=88184) Extracting /tmp/tmppk4zrz1w/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmppk4zrz1w/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=88184)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=101545) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=95492) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 11x across cluster]\n(RayTrainWorker pid=95492) Extracting /tmp/tmpyy7a6r11/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpyy7a6r11/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=95492)  [repeated 12x across cluster]\n\n\n  0%|          | 0/9912422 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 104607984.65it/s]\n\n\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n(RayTrainWorker pid=101545) Extracting /tmp/tmpxobpdr_p/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpxobpdr_p/MNIST/raw\n\n\n(RayTrainWorker pid=108863) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=101546) Extracting /tmp/tmpt_if2tuu/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpt_if2tuu/MNIST/raw [repeated 8x across cluster]\n(RayTrainWorker pid=101546)  [repeated 12x across cluster]\n\n\n(autoscaler +11m23s) [workspace snapshot] New snapshot created successfully (Size: 327.01 KB)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(autoscaler +11m23s) [workspace snapshot] New snapshot created successfully (Size: 327.01 KB)\n\n\n(RayTrainWorker pid=111131) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=111131) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpddnnc0iv/MNIST/raw/train-images-idx3-ubyte.gz [repeated 13x across cluster]\n(RayTrainWorker pid=108863) Extracting /tmp/tmpxcg0v86z/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpxcg0v86z/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=108863)  [repeated 12x across cluster]\n\n\n(RayTrainWorker pid=118364) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=118364) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmp0sbwiedt/MNIST/raw/train-images-idx3-ubyte.gz [repeated 12x across cluster]\n(RayTrainWorker pid=111130) Extracting /tmp/tmpfmuq9_qh/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpfmuq9_qh/MNIST/raw [repeated 12x across cluster]\n(RayTrainWorker pid=111130)  [repeated 12x across cluster]\n\n\n\n\n\n\n\nresults.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")\n\n\n\n\nResult(\n  metrics={'ptl/train_loss': 0.00108961365185678, 'ptl/train_accuracy': 1.0, 'ptl/val_loss': 0.05798737704753876, 'ptl/val_accuracy': 0.9820601940155029, 'epoch': 4, 'step': 1435},\n  path='/home/ray/ray_results/TorchTrainer_2023-09-07_13-58-38/TorchTrainer_5144b_00008_8_batch_size=64,layer_1_size=128,layer_2_size=256,lr=0.0037_2023-09-07_13-58-38',\n  filesystem='local',\n  checkpoint=Checkpoint(filesystem=local, path=/home/ray/ray_results/TorchTrainer_2023-09-07_13-58-38/TorchTrainer_5144b_00008_8_batch_size=64,layer_1_size=128,layer_2_size=256,lr=0.0037_2023-09-07_13-58-38/checkpoint_000004)\n)\n\n\n\n\nIn the example above, Tune runs 10 trials with different hyperparameter configurations.\nAs you can see in the training_iteration column, trials with a high loss (and low accuracy) have been terminated early. The best performing trial used\nbatch_size=64, layer_1_size=128, layer_2_size=256, and lr=0.0037.\n\n\n\nMore PyTorch Lightning Examples#\n\n[Basic] Train a PyTorch Lightning Image Classifier with Ray Train.\n[Intermediate] Fine-tune a BERT Text Classifier with PyTorch Lightning and Ray Train\n[Advanced] Fine-tune dolly-v2-7b with PyTorch Lightning and FSDP\nMLflow PyTorch Lightning Example: Example for using MLflow\nand Pytorch Lightning with Ray Tune.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '# Construct your `OfflineData` class instance.\noffline_data = ImageOfflineData(config)\n\n# Check, how the data is transformed. Note, the\n# example dataset has only 3 such images.\nbatch = offline_data.data.take_batch(3)\n\n# Construct your `OfflinePreLearner`.\noffline_prelearner = ImageOfflinePreLearner(\n    config=config,\n    learner=None,\n    spaces=(\n        config.observation_space,\n        config.action_space,\n    ),\n    module_spec=module_spec,\n)\n\n# Transform the raw data to `MultiAgentBatch` data.\nbatch = offline_prelearner(batch)\n\n# Show the transformed batch.\nprint(f\"Batch: {batch}\")\n\n\n\nTip\nConsider this approach carefully: in many cases, fully transforming your data into a suitable format before engaging RLlib\u2019s offline RL API can be more efficient. For instance, in the example above, you could preprocess the entire image dataset into numpy arrays beforehand and utilize RLlib\u2019s default OfflineData class for subsequent steps.\n\n\n\n\n\nMonitoring#\nTo effectively monitor your offline data pipeline, leverage Ray Data\u2019s built-in monitoring capacities. Focus on ensuring that all stages of your offline data streaming pipeline are actively processing data. Additionally, keep an eye on the Learner instance, particularly the learner_update_timer, which should maintain low values - around 0.02 for small models - to indicate efficient data processing and model updates.\n\nNote\nRLlib doesn\u2019t include Ray Data  metrics in its results or display them in Tensorboard through Ray Tune\u2019s TBXLoggerCallback. It\u2019s strongly recommended to enable the Ray Dashboard, accessible at 127.0.0.1:8265, for comprehensive monitoring and insights.\n\n\n\n\n\n\nOutput API#\nYou can configure experience output for an agent using the following options:\ndef offline_data(\n    # Specify where experiences should be saved:\n    # - None: don't save any experiences\n    # - a path/URI to save to a custom output directory (for example, \"s3://bckt/\")\n    output: Optional[str],\n    # What sample batch columns to LZ4 compress in the output data.\n    # Note that providing `rllib.core.columns.Columns.OBS` also\n    # compresses `rllib.core.columns.Columns.NEXT_OBS`.\n    output_compress_columns: Optional[List[str]],\n    # Max output file size (in bytes) before rolling over to a new\n    # file.\n    output_max_file_size: Optional[float],\n    # Max output row numbers before rolling over to a new file.\n    output_max_rows_per_file: Optional[int],\n    # Write method for the `ray.data.Dataset` to write the\n    # offline data to `output`. The default is `read_parquet` for Parquet\n    # files. See https://docs.ray.io/en/latest/data/api/input_output.html for\n    # more info about available read methods in `ray.data`.\n    output_write_method: Optional[str],\n    # Keyword arguments for the `output_write_method`. These are\n    # passed into the write method without checking.\n    output_write_method_kwargs: Optional[Dict],\n    # A cloud filesystem to handle access to cloud storage when\n    # writing experiences. Can be \"gcs\" for Google Cloud Storage, \"s3\" for AWS\n    # S3 buckets, \"abs\" for Azure Blob Storage, or any filesystem supported\n    # by PyArrow. In general the file path is sufficient for accessing data\n    # from public or local storage systems. See\n    # https://arrow.apache.org/docs/python/filesystems.html for details.\n    output_filesystem: Optional[str],\n    # A dictionary holding the keyword arguments for the filesystem\n    # given by `output_filesystem`. See `gcsfs.GCSFilesystem` for GCS,\n    # `pyarrow.fs.S3FileSystem`, for S3, and `ablfs.AzureBlobFilesystem` for\n    # ABS filesystem arguments.\n    output_filesystem_kwargs: Optional[Dict],\n    # If data should be recorded in RLlib's `EpisodeType`\n    # format (i.e. `SingleAgentEpisode` objects). Use this format, if you\n    # need data to be ordered in time and directly grouped by episodes for\n    # example to train stateful modules or if you plan to use recordings\n    # exclusively in RLlib. Otherwise data is recorded in tabular (columnar)\n    # format. Default is `True`.\n    output_write_episodes: Optional[bool],': cannot unpack non-iterable NoneType object\n"
        }
      ]
    },
    {
      "id": "Kclp",
      "code_hash": "9d7c758a7d366f99946efc436fbdb9a9",
      "outputs": [
        {
          "type": "error",
          "ename": "syntax",
          "evalue": "line 12\n    except Exception as e:\n                          ^\nIndentationError: expected an indented block after 'except' statement on line 12\n",
          "traceback": []
        }
      ],
      "console": []
    },
    {
      "id": "setup",
      "code_hash": "1bbb850824d09fcbff9763d96ddf3dbd",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    }
  ]
}