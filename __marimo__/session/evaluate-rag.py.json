{
  "version": "1",
  "metadata": {
    "marimo_version": "0.13.15"
  },
  "cells": [
    {
      "id": "Hbol",
      "code_hash": "b03ae3d1a8c7e35b57a5d71425064bb8",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "MJUe",
      "code_hash": "3354c6b61507ca5280e99f5402a451a4",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"imports\">Imports</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "vblA",
      "code_hash": "3d8d5f5f3a01e81db2661040ede9c323",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "bkHC",
      "code_hash": "fd8fa912b94bb9a38de1a27f4bdb1071",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"setup-code\">Setup code</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "lEQa",
      "code_hash": "46fb1ae645e4ecefb51ca602f3ecfa9c",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Using CUDA ? YES\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "GPU: NVIDIA GeForce GTX 1660 Ti\n"
        }
      ]
    },
    {
      "id": "PKri",
      "code_hash": "fb851e7d70c8ebd9057f38b946bc7771",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "Xref",
      "code_hash": "0bb3ad46cbe3cdd55a2d6bed7c715d82",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "SFPL",
      "code_hash": "3f585e48250067f18535656fd7788467",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "BYtC",
      "code_hash": "6f88351e37e23686b1d26753777f1fd3",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"custom-sentencechunker\">Custom SentenceChunker</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "RGSE",
      "code_hash": "83c05119a9f82e758a5da33f11fbf01e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "Kclp",
      "code_hash": "3eb55a2d4f6bd053f02d13bc3700bc12",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "emfo",
      "code_hash": "22a6a08eb3f43eaa942a7da22953c011",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Recall: 0.7902 \u00b1 0.3878\nPrecision: 0.0331 \u00b1 0.0301\nPrecision \u03a9: 0.1672 \u00b1 0.1086\nIoU: 0.0322 \u00b1 0.0279\n"
        }
      ]
    },
    {
      "id": "Hstk",
      "code_hash": "02d78ea922d7b008bff79c040828bb1f",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"recursivetokenchunker\">RecursiveTokenChunker</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "nWHF",
      "code_hash": "fef48e8ff34e1f08915d0b19ee74a1b1",
      "outputs": [],
      "console": []
    },
    {
      "id": "iLit",
      "code_hash": "26baa78e587b31589a9809d2e86985d6",
      "outputs": [],
      "console": []
    },
    {
      "id": "ZHCJ",
      "code_hash": "c0ee2bd4a0f7bceade7e372d3113f88b",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"synthetic-data-evaluation\">Synthetic Data Evaluation</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "ROlb",
      "code_hash": "639c789fb0ad1b3d7fa23ff4cca8f1a0",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "qnkX",
      "code_hash": "5cfd2adc7a1e0d7540b6c93db2b93883",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "TqIu",
      "code_hash": "c0813abf2564034ed6378b1c4e4d6929",
      "outputs": [],
      "console": []
    },
    {
      "id": "Vxnm",
      "code_hash": "a3311f98b8863136cfc6b9ef67520286",
      "outputs": [],
      "console": []
    },
    {
      "id": "DnEU",
      "code_hash": "5bed804e105b97cb28b551a49e57b547",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'ray.rllib.core.rl_module.multi_rl_module.MultiRLModule#\n\n\n__init__\nInitializes a MultiRLModule instance.\n\nadd_module\nAdds a module at run time to the multi-agent module.\n\nas_multi_rl_module\nReturns self in order to match RLModule.as_multi_rl_module() behavior.\n\nforeach_module\nCalls the given function with each (module_id, module).\n\nforward_exploration\nDO NOT OVERRIDE! Forward-pass during exploration, called from the sampler.\n\nforward_inference\nDO NOT OVERRIDE! Forward-pass during evaluation, called from the sampler.\n\nforward_train\nDO NOT OVERRIDE! Forward-pass during training called from the learner.\n\nfrom_checkpoint\nCreates a new Checkpointable instance from the given location and returns it.\n\nget\nReturns the module with the given module ID or default if not found in self.\n\nget_exploration_action_dist_cls\nReturns the action distribution class for this RLModule used for exploration.\n\nget_inference_action_dist_cls\nReturns the action distribution class for this RLModule used for inference.\n\nget_metadata\nReturns JSON writable metadata further describing the implementing class.\n\nget_train_action_dist_cls\nReturns the action distribution class for this RLModule used for training.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-11 16:13:34\nRunning for: 00:00:01.87        \nMemory:      22.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name                     status    loc                    eta  max_depth  min_child_weight  subsample     acc  iter  total time (s)\n\n\n\n\n\n2025-02-11 16:13:34,649\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-31' in 0.0057s.\n2025-02-11 16:13:34,652\tINFO tune.py:1041 -- Total run time: 1.88 seconds (1.86 seconds for the tuning loop).': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-11 16:13:34\nRunning for: 00:00:01.87        \nMemory:      22.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name                     status    loc                    eta  max_depth  min_child_weight  subsample     acc  iter  total time (s)\n\n\n\n\n\n2025-02-11 16:13:34,649\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-31' in 0.0057s.\n2025-02-11 16:13:34,652\tINFO tune.py:1041 -- Total run time: 1.88 seconds (1.86 seconds for the tuning loop).\n\n\n\n\n\nAs you can see, the changes in the actual training function are minimal. Instead of\nreturning the accuracy value, we report it back to Tune using session.report().\nOur config dictionary only changed slightly. Instead of passing hard-coded\nparameters, we tell Tune to choose values from a range of valid options. There are\na number of options we have here, all of which are explained in\nthe Tune docs.\nFor a brief explanation, this is what they do:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-11 16:13:34\nRunning for: 00:00:01.87        \nMemory:      22.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name                     status    loc                    eta  max_depth  min_child_weight  subsample     acc  iter  total time (s)\n\n\n\n\n\n2025-02-11 16:13:34,649\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-31' in 0.0057s.\n2025-02-11 16:13:34,652\tINFO tune.py:1041 -- Total run time: 1.88 seconds (1.86 seconds for the tuning loop).\n\n\n\n\n\nAs you can see, the changes in the actual training function are minimal. Instead of\nreturning the accuracy value, we report it back to Tune using session.report().\nOur config dictionary only changed slightly. Instead of passing hard-coded\nparameters, we tell Tune to choose values from a range of valid options. There are\na number of options we have here, all of which are explained in\nthe Tune docs.\nFor a brief explanation, this is what they do:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'tune.randint(min, max) chooses a random integer value between min and max.\nNote that max is exclusive, so it will not be sampled.\ntune.choice([a, b, c]) chooses one of the items of the list at random. Each item\nhas the same chance to be sampled.\ntune.uniform(min, max) samples a floating point number between min and max.\nNote that max is exclusive here, too.\ntune.loguniform(min, max, base=10) samples a floating point number between min and max,\nbut applies a logarithmic transformation to these boundaries first. Thus, this makes\nit easy to sample values from different orders of magnitude.\n\n\nThe best configuration we found used eta=0.000950272, max_depth=3,\nmin_child_weight=2, subsample=0.835504 and reached an accuracy of\n0.965035.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-11 16:13:34\nRunning for: 00:00:01.87        \nMemory:      22.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name                     status    loc                    eta  max_depth  min_child_weight  subsample     acc  iter  total time (s)\n\n\n\n\n\n2025-02-11 16:13:34,649\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-31' in 0.0057s.\n2025-02-11 16:13:34,652\tINFO tune.py:1041 -- Total run time: 1.88 seconds (1.86 seconds for the tuning loop).\n\n\n\n\n\nAs you can see, the changes in the actual training function are minimal. Instead of\nreturning the accuracy value, we report it back to Tune using session.report().\nOur config dictionary only changed slightly. Instead of passing hard-coded\nparameters, we tell Tune to choose values from a range of valid options. There are\na number of options we have here, all of which are explained in\nthe Tune docs.\nFor a brief explanation, this is what they do:': cannot unpack non-iterable NoneType object\nError in finding 'tune.randint(min, max) chooses a random integer value between min and max.\nNote that max is exclusive, so it will not be sampled.\ntune.choice([a, b, c]) chooses one of the items of the list at random. Each item\nhas the same chance to be sampled.\ntune.uniform(min, max) samples a floating point number between min and max.\nNote that max is exclusive here, too.\ntune.loguniform(min, max, base=10) samples a floating point number between min and max,\nbut applies a logarithmic transformation to these boundaries first. Thus, this makes\nit easy to sample values from different orders of magnitude.\n\n\nThe best configuration we found used eta=0.000950272, max_depth=3,\nmin_child_weight=2, subsample=0.835504 and reached an accuracy of\n0.965035.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'train_breast_cancer_32eb5_00000TERMINATED127.0.0.1:897630.000830475          5                 1   0.675899    10       0.0169384      0.640195    0.342657\n\n\n\n\n\n2025-02-11 16:13:35,717\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-34' in 0.0018s.\n2025-02-11 16:13:35,719\tINFO tune.py:1041 -- Total run time: 1.05 seconds (1.04 seconds for the tuning loop).\n\n\nBest model parameters: {'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.675899175238225, 'eta': 0.0008304750981897656}\nBest model total accuracy: 0.6573\n\n\n\n\n\n\n Best model parameters: {'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.5015513240240503, 'eta': 0.024272050872920895}\n Best model total accuracy: 0.9301': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-24 16:22:18\nRunning for: 00:00:01.24        \nMemory:      21.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name            status    loc              h0   lr  iter  total time (s)       Q     theta0   theta1\n\n\ntrain_func_91d06_00000TERMINATED127.0.0.1:23610   00.015   100       0.068691 0.5906680.9        0.0427973\ntrain_func_91d06_00001TERMINATED127.0.0.1:23609   10.045   100       0.06599690.3899990.0008300930.9      \n\n\n\n\n\n\n\n\n\nAs we can see, neither trial makes it to the optimum, since the search configs are stuck with their original values. This illustrates a key advantage of PBT: while traditional hyperparameter search methods (like grid search) keep fixed search values throughout training, PBT can adapt the search dynamically, allowing it to find better solutions with the same computational budget.\n\n\nfig, axs = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw=dict(width_ratios=[1.5, 1]))\n\ncolors = [\"red\", \"black\"]\nlabels = [\"h = [1, 0]\", \"h = [0, 1]\"]': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'In ray-cluster.external-redis.yaml, the gcsFaultToleranceOptions.externalStorageNamespace option isn\u2019t set for the RayCluster.\nTherefore, KubeRay automatically injects the environment variable RAY_external_storage_namespace to all Ray Pods managed by the RayCluster with the RayCluster\u2019s UID as the external storage namespace by default.\nSee this section to learn more about the option.\n\n\nStep 7: Kill the GCS process in the head Pod#\n\n\n# Step 7.1: Check the `RAY_gcs_rpc_server_reconnect_timeout_s` environment variable in both the head Pod and worker Pod.\nkubectl get pods $HEAD_POD -o=jsonpath='{.spec.containers[0].env}' | jq\n\n\n\n\n\n\n\n\nkubectl get pods $YOUR_WORKER_POD -o=jsonpath='{.spec.containers[0].env}' | jq': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'In ray-cluster.external-redis.yaml, the gcsFaultToleranceOptions.externalStorageNamespace option isn\u2019t set for the RayCluster.\nTherefore, KubeRay automatically injects the environment variable RAY_external_storage_namespace to all Ray Pods managed by the RayCluster with the RayCluster\u2019s UID as the external storage namespace by default.\nSee this section to learn more about the option.\n\n\nStep 7: Kill the GCS process in the head Pod#\n\n\n# Step 7.1: Check the `RAY_gcs_rpc_server_reconnect_timeout_s` environment variable in both the head Pod and worker Pod.\nkubectl get pods $HEAD_POD -o=jsonpath='{.spec.containers[0].env}' | jq\n\n\n\n\n\n\n\n\nkubectl get pods $YOUR_WORKER_POD -o=jsonpath='{.spec.containers[0].env}' | jq\n\n\n\n\n\n\n\n\n# Step 7.2: Kill the GCS process in the head Pod.\nkubectl exec -i $HEAD_POD -- pkill gcs_server\n\n\n\n\n\n\n# Step 7.3: The head Pod fails and restarts after `RAY_gcs_rpc_server_reconnect_timeout_s` (60) seconds.\n# In addition, the worker Pod isn't terminated by the new head after reconnecting because GCS fault\n# tolerance is enabled.\nkubectl get pods -l=ray.io/is-ray-node=yes': cannot unpack non-iterable NoneType object\nError in finding 'For more information about configuring network access to a Ray cluster on Kubernetes, see the networking notes.\n\n\n\n\n\nWarning\nThe Ray Dashboard provides read and write access to the Ray Cluster. The reverse proxy must provide authentication or network ingress controls to prevent unauthorized access to the Cluster.\n\n\n\nDisabling the Dashboard#\nDashboard is included if you use ray[default] or other installation commands and automatically started.\nTo disable Dashboard, use the following arguments --include-dashboard.\n\n\n\nSingle-node local cluster\nStart the cluster explicitly with CLI \nray start --include-dashboard=False\n\n\nStart the cluster implicitly with ray.init \nimport ray\nray.init(include_dashboard=False)\n\n\n\n\n\nVM Cluster Launcher\nInclude the ray start --head --include-dashboard=False argument\nin the head_start_ray_commands section of the Cluster Launcher\u2019s YAML file.\n\n\n\nKubeRay\n\nWarning\nIt\u2019s not recommended to disable Dashboard because several KubeRay features like RayJob and RayService depend on it.\n\nSet spec.headGroupSpec.rayStartParams.include-dashboard to False. Check out this example YAML file.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Tip\nsession.report can\u2019t be used within a Trainable class.\n\n\n\nLearn more about the details of Trainables here\nand have a look at our examples.\nNext, let\u2019s have a closer look at what the config dictionary is that you pass into your trainables.\n\n\n\n\nTune Trials#\nYou use Tuner.fit to execute and manage hyperparameter tuning and generate your trials.\nAt a minimum, your Tuner call takes in a trainable as first argument, and a param_space dictionary\nto define the search space.\nThe Tuner.fit() function also provides many features such as logging,\ncheckpointing, and early stopping.\nIn the example, minimizing a (x ** 2) + b, a simple Tune run with a simplistic search space for a and b\nlooks like this:\n# Pass in a Trainable class or function, along with a search space \"config\".\ntuner = tune.Tuner(trainable, param_space={\"a\": 2, \"b\": 4})\ntuner.fit()': cannot unpack non-iterable NoneType object\nError in finding 'Tip\nsession.report can\u2019t be used within a Trainable class.\n\n\n\nLearn more about the details of Trainables here\nand have a look at our examples.\nNext, let\u2019s have a closer look at what the config dictionary is that you pass into your trainables.\n\n\n\n\nTune Trials#\nYou use Tuner.fit to execute and manage hyperparameter tuning and generate your trials.\nAt a minimum, your Tuner call takes in a trainable as first argument, and a param_space dictionary\nto define the search space.\nThe Tuner.fit() function also provides many features such as logging,\ncheckpointing, and early stopping.\nIn the example, minimizing a (x ** 2) + b, a simple Tune run with a simplistic search space for a and b\nlooks like this:\n# Pass in a Trainable class or function, along with a search space \"config\".\ntuner = tune.Tuner(trainable, param_space={\"a\": 2, \"b\": 4})\ntuner.fit()': cannot unpack non-iterable NoneType object\nError in finding 'Tip\nsession.report can\u2019t be used within a Trainable class.\n\n\n\nLearn more about the details of Trainables here\nand have a look at our examples.\nNext, let\u2019s have a closer look at what the config dictionary is that you pass into your trainables.\n\n\n\n\nTune Trials#\nYou use Tuner.fit to execute and manage hyperparameter tuning and generate your trials.\nAt a minimum, your Tuner call takes in a trainable as first argument, and a param_space dictionary\nto define the search space.\nThe Tuner.fit() function also provides many features such as logging,\ncheckpointing, and early stopping.\nIn the example, minimizing a (x ** 2) + b, a simple Tune run with a simplistic search space for a and b\nlooks like this:\n# Pass in a Trainable class or function, along with a search space \"config\".\ntuner = tune.Tuner(trainable, param_space={\"a\": 2, \"b\": 4})\ntuner.fit()': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'In case you want to implement your own search algorithm, the interface is easy to implement,\nyou can read the instructions here.\nTune also provides helpful utilities to use with Search Algorithms:\n\n\nRepeated Evaluations (tune.search.Repeater): Support for running each sampled hyperparameter with multiple random seeds.\nConcurrencyLimiter (tune.search.ConcurrencyLimiter): Limits the amount of concurrent trials when running optimization.\nShim Instantiation (tune.create_searcher): Allows creation of the search algorithm object given a string.\n\n\nNote that in the example above we  tell Tune to stop after 20 training iterations.\nThis way of stopping trials with explicit rules is useful, but in many cases we can do even better with\nschedulers.\n\n# Create HyperBand scheduler and minimize the score\nhyperband = HyperBandScheduler(metric=\"score\", mode=\"max\")\n\nconfig = {\"a\": tune.uniform(0, 1), \"b\": tune.uniform(0, 1)}\n\ntuner = tune.Tuner(\n    trainable,\n    tune_config=tune.TuneConfig(\n        num_samples=20,\n        scheduler=hyperband,\n    ),\n    param_space=config,\n)\ntuner.fit()': cannot unpack non-iterable NoneType object\nError in finding 'DeprecationWarning,\n\n\n== Status ==Current time: 2022-07-22 15:41:31 (running for 00:00:06.73)Memory usage on this node: 9.9/16.0 GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.5 GiB heap, 0.0/2.0 GiB objectsCurrent best trial: 5bf98_00000 with loss=1.0234101880766688 and parameters={'mean': 1, 'sd': 0.40575843135279466}Result logdir: /Users/kai/ray_results/train_function_2022-07-22_15-41-18Number of trials: 3/3 (3 TERMINATED)\n\nTrial name                status    loc              mean      sd  iter  total time (s)   loss\n\n\ntrain_function_5bf98_00000TERMINATED127.0.0.1:48140     10.405758    30       2.11758  1.02341\ntrain_function_5bf98_00001TERMINATED127.0.0.1:48147     20.647335    30       0.07707311.53993\ntrain_function_5bf98_00002TERMINATED127.0.0.1:48151     30.256568    30       0.07284313.0393 \n\n\n2022-07-22 15:41:31,290\tINFO tune.py:738 -- Total run time: 7.36 seconds (6.72 seconds for the tuning loop).\n\n\n{'mean': 1, 'sd': 0.40575843135279466}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\nError in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\nError in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45401) 2022-07-22 15:11:27,287\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_2397442c_1_activation=tanh,height=32.8422,steps=100,width=12.1847_2022-07-22_15-11-11/checkpoint_tmp2e0d09\n(objective pid=45401) 2022-07-22 15:11:27,287\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.535153865814209, '_episodes_total': 0}\n\n\n\n\nHere again are the hyperparameters found to minimize the mean loss of the\ndefined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'activation': 'tanh', 'height': -48.451797714080236, 'steps': 100, 'width': 10.119125894538891}': cannot unpack non-iterable NoneType object\nError in finding 'Step 3: Optionally, configure the resources allocated per trial. Tune uses this resources allocation to control the parallelism. For example, if each trial was configured to use 4 CPUs, and the cluster had only 32 CPUs, then Tune will limit the number of concurrent trials to 8 to avoid overloading the cluster. For more information, see A Guide To Parallelism and Resources for Ray Tune.\n# Can customize resources per trial, here we set 1 CPU each.\ntrain_model = tune.with_resources(train_model, {\"cpu\": 1})\n\n\nStep 4: Run the trial with Tune. Tune will report on experiment status, and after the experiment finishes, you can inspect the results. Tune can retry failed trials automatically, as well as entire experiments; see How to Define Stopping Criteria for a Ray Tune Experiment.\n# Start a Tune run and print the best result.\ntuner = tune.Tuner(train_model, param_space=trial_space)\nresults = tuner.fit()\n\n# Access individual results.\nprint(results[0])\nprint(results[1])\nprint(results[2])\n\n2022-09-21 10:19:35,159     INFO tune.py:762 -- Total run time: 5.06 seconds (4.46 seconds for the tuning loop).': cannot unpack non-iterable NoneType object\nError in finding 'Handling Failures and Node Preemption#\n\nImportant\nThis user guide shows how to configure fault tolerance for the revamped Ray Train V2\navailable starting from Ray 2.43 by enabling the environment variable RAY_TRAIN_V2_ENABLED=1.\nThis user guide assumes that the environment variable has been enabled.\nPlease see here for information about the deprecation and migration.\n\nRay Train provides fault tolerance at three levels:\n\nWorker process fault tolerance handles errors that happen to one or more Train worker processes while they are executing the user defined training function.\nWorker node fault tolerance handles node failures that may occur during training.\nJob driver fault tolerance handles the case where Ray Train driver process crashes, and training needs to be kicked off again, possibly from a new cluster.\n\nThis user guide covers how to configure and use these fault tolerance mechanisms.\n\n# Tries to recover a run up to this many times.\nfailure_config = ray.train.FailureConfig(max_failures=2)\n\n# No limit on the number of retries.\nfailure_config = ray.train.FailureConfig(max_failures=-1)': cannot unpack non-iterable NoneType object\nError in finding 'Trial Progress\n\n\nTrial name                date               done  episodes_total  experiment_id                   experiment_tag    hostname              iterations_since_restore    lossnode_ip    pid  time_since_restore  time_this_iter_s  time_total_s  timestamp  timesteps_since_restoretimesteps_total    training_iterationtrial_id     warmup_time\n\n\n\n\n2022-11-02 16:03:13,913\tINFO tune.py:788 -- Total run time: 28.53 seconds (27.28 seconds for the tuning loop).\n\n\n\n\n\nTune Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 GiB       \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.44 GiB heap, 0.0/1.72 GiB objects\n    \n\n\n\nTrial Status\n\n\nTrial name                      status    loc              mean      sd  iter  total time (s)    loss': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial Progress\n\n\nTrial name                date               done  episodes_total  experiment_id                   experiment_tag    hostname              iterations_since_restore    lossnode_ip    pid  time_since_restore  time_this_iter_s  time_total_s  timestamp  timesteps_since_restoretimesteps_total    training_iterationtrial_id     warmup_time\n\n\n\n\n2022-11-02 16:03:13,913\tINFO tune.py:788 -- Total run time: 28.53 seconds (27.28 seconds for the tuning loop).\n\n\n\n\n\nTune Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 GiB       \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.44 GiB heap, 0.0/1.72 GiB objects\n    \n\n\n\nTrial Status\n\n\nTrial name                      status    loc              mean      sd  iter  total time (s)    loss': cannot unpack non-iterable NoneType object\nError in finding 'Trial Progress\n\n\nTrial name                date               done  episodes_total  experiment_id                   experiment_tag    hostname              iterations_since_restore    lossnode_ip    pid  time_since_restore  time_this_iter_s  time_total_s  timestamp  timesteps_since_restoretimesteps_total    training_iterationtrial_id     warmup_time\n\n\n\n\n2022-11-02 16:03:13,913\tINFO tune.py:788 -- Total run time: 28.53 seconds (27.28 seconds for the tuning loop).\n\n\n\n\n\nTune Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 GiB       \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.44 GiB heap, 0.0/1.72 GiB objects\n    \n\n\n\nTrial Status\n\n\nTrial name                      status    loc              mean      sd  iter  total time (s)    loss': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial Progress\n\n\nTrial name                date               done  episodes_total  experiment_id                   experiment_tag    hostname              iterations_since_restore    lossnode_ip    pid  time_since_restore  time_this_iter_s  time_total_s  timestamp  timesteps_since_restoretimesteps_total    training_iterationtrial_id     warmup_time\n\n\n\n\n2022-11-02 16:03:13,913\tINFO tune.py:788 -- Total run time: 28.53 seconds (27.28 seconds for the tuning loop).\n\n\n\n\n\nTune Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 GiB       \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.44 GiB heap, 0.0/1.72 GiB objects\n    \n\n\n\nTrial Status\n\n\nTrial name                      status    loc              mean      sd  iter  total time (s)    loss': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Wandb\u2019s group, run_id and run_name are automatically selected\nby Tune, but can be overwritten by filling out the respective configuration\nvalues.\nPlease see here for all other valid configuration settings:\nhttps://docs.wandb.ai/library/init\nPublicAPI (alpha): This API is in alpha and may change before becoming stable.\n\n\n\nsetup_wandb#\n\nParameters:\n\nconfig \u2013 Configuration dict to be logged to Weights and Biases. Can contain\narguments for wandb.init() as well as authentication information.\napi_key \u2013 API key to use for authentication with Weights and Biases.\napi_key_file \u2013 File pointing to API key for with Weights and Biases.\nrank_zero_only \u2013 If True, will return an initialized session only for the\nrank 0 worker in distributed training. If False, will initialize a\nsession for all workers.\nkwargs \u2013 Passed to wandb.init().\n\n\n\nExample\nfrom ray.air.integrations.wandb import setup_wandb\n\ndef training_loop(config):\n    wandb = setup_wandb(config)\n    # ...\n    wandb.log({\"loss\": 0.123})\n\n\nPublicAPI (alpha): This API is in alpha and may change before becoming stable.': cannot unpack non-iterable NoneType object\n"
        }
      ]
    },
    {
      "id": "ulZA",
      "code_hash": "80563fd01cb66c1bfd343a44a2e9cc99",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Recall: 0.7259 \u00b1 0.3883\nPrecision: 0.0612 \u00b1 0.0636\nPrecision \u03a9: 0.3142 \u00b1 0.1939\nIoU: 0.0629 \u00b1 0.0739\n"
        }
      ]
    },
    {
      "id": "ecfG",
      "code_hash": "43a3b3aaea4d0f170e9d2cb92de0a0a7",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Recall: 0.7482 \u00b1 0.3807\nPrecision: 0.0465 \u00b1 0.0423\nPrecision \u03a9: 0.2648 \u00b1 0.2034\nIoU: 0.0469 \u00b1 0.0452\n"
        }
      ]
    },
    {
      "id": "Pvdt",
      "code_hash": "fdbbe6857cab3de40a20387cf18934a7",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"evaluate-fine-tuned-embedding-model\">Evaluate fine-tuned embedding model</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "ZBYS",
      "code_hash": "da9d005579e1c9e37e204b9acc5a9469",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "aLJB",
      "code_hash": "6a9e1ba8c4fb2e9273cb5eafca0c494f",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"with-recursivetokenchunking\">With RecursiveTokenChunking</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "nHfw",
      "code_hash": "593171de8c099029a9305fd7637c96c9",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'ray.rllib.core.rl_module.multi_rl_module.MultiRLModule#\n\n\n__init__\nInitializes a MultiRLModule instance.\n\nadd_module\nAdds a module at run time to the multi-agent module.\n\nas_multi_rl_module\nReturns self in order to match RLModule.as_multi_rl_module() behavior.\n\nforeach_module\nCalls the given function with each (module_id, module).\n\nforward_exploration\nDO NOT OVERRIDE! Forward-pass during exploration, called from the sampler.\n\nforward_inference\nDO NOT OVERRIDE! Forward-pass during evaluation, called from the sampler.\n\nforward_train\nDO NOT OVERRIDE! Forward-pass during training called from the learner.\n\nfrom_checkpoint\nCreates a new Checkpointable instance from the given location and returns it.\n\nget\nReturns the module with the given module ID or default if not found in self.\n\nget_exploration_action_dist_cls\nReturns the action distribution class for this RLModule used for exploration.\n\nget_inference_action_dist_cls\nReturns the action distribution class for this RLModule used for inference.\n\nget_metadata\nReturns JSON writable metadata further describing the implementing class.\n\nget_train_action_dist_cls\nReturns the action distribution class for this RLModule used for training.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-11 16:13:34\nRunning for: 00:00:01.87        \nMemory:      22.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name                     status    loc                    eta  max_depth  min_child_weight  subsample     acc  iter  total time (s)\n\n\n\n\n\n2025-02-11 16:13:34,649\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-31' in 0.0057s.\n2025-02-11 16:13:34,652\tINFO tune.py:1041 -- Total run time: 1.88 seconds (1.86 seconds for the tuning loop).': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-11 16:13:34\nRunning for: 00:00:01.87        \nMemory:      22.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name                     status    loc                    eta  max_depth  min_child_weight  subsample     acc  iter  total time (s)\n\n\n\n\n\n2025-02-11 16:13:34,649\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-31' in 0.0057s.\n2025-02-11 16:13:34,652\tINFO tune.py:1041 -- Total run time: 1.88 seconds (1.86 seconds for the tuning loop).\n\n\n\n\n\nAs you can see, the changes in the actual training function are minimal. Instead of\nreturning the accuracy value, we report it back to Tune using session.report().\nOur config dictionary only changed slightly. Instead of passing hard-coded\nparameters, we tell Tune to choose values from a range of valid options. There are\na number of options we have here, all of which are explained in\nthe Tune docs.\nFor a brief explanation, this is what they do:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-11 16:13:34\nRunning for: 00:00:01.87        \nMemory:      22.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name                     status    loc                    eta  max_depth  min_child_weight  subsample     acc  iter  total time (s)\n\n\n\n\n\n2025-02-11 16:13:34,649\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-31' in 0.0057s.\n2025-02-11 16:13:34,652\tINFO tune.py:1041 -- Total run time: 1.88 seconds (1.86 seconds for the tuning loop).\n\n\n\n\n\nAs you can see, the changes in the actual training function are minimal. Instead of\nreturning the accuracy value, we report it back to Tune using session.report().\nOur config dictionary only changed slightly. Instead of passing hard-coded\nparameters, we tell Tune to choose values from a range of valid options. There are\na number of options we have here, all of which are explained in\nthe Tune docs.\nFor a brief explanation, this is what they do:': cannot unpack non-iterable NoneType object\nError in finding 'tune.randint(min, max) chooses a random integer value between min and max.\nNote that max is exclusive, so it will not be sampled.\ntune.choice([a, b, c]) chooses one of the items of the list at random. Each item\nhas the same chance to be sampled.\ntune.uniform(min, max) samples a floating point number between min and max.\nNote that max is exclusive here, too.\ntune.loguniform(min, max, base=10) samples a floating point number between min and max,\nbut applies a logarithmic transformation to these boundaries first. Thus, this makes\nit easy to sample values from different orders of magnitude.\n\n\nThe best configuration we found used eta=0.000950272, max_depth=3,\nmin_child_weight=2, subsample=0.835504 and reached an accuracy of\n0.965035.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-11 16:13:34\nRunning for: 00:00:01.87        \nMemory:      22.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name                     status    loc                    eta  max_depth  min_child_weight  subsample     acc  iter  total time (s)\n\n\n\n\n\n2025-02-11 16:13:34,649\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-31' in 0.0057s.\n2025-02-11 16:13:34,652\tINFO tune.py:1041 -- Total run time: 1.88 seconds (1.86 seconds for the tuning loop).\n\n\n\n\n\nAs you can see, the changes in the actual training function are minimal. Instead of\nreturning the accuracy value, we report it back to Tune using session.report().\nOur config dictionary only changed slightly. Instead of passing hard-coded\nparameters, we tell Tune to choose values from a range of valid options. There are\na number of options we have here, all of which are explained in\nthe Tune docs.\nFor a brief explanation, this is what they do:': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'tune.randint(min, max) chooses a random integer value between min and max.\nNote that max is exclusive, so it will not be sampled.\ntune.choice([a, b, c]) chooses one of the items of the list at random. Each item\nhas the same chance to be sampled.\ntune.uniform(min, max) samples a floating point number between min and max.\nNote that max is exclusive here, too.\ntune.loguniform(min, max, base=10) samples a floating point number between min and max,\nbut applies a logarithmic transformation to these boundaries first. Thus, this makes\nit easy to sample values from different orders of magnitude.\n\n\nThe best configuration we found used eta=0.000950272, max_depth=3,\nmin_child_weight=2, subsample=0.835504 and reached an accuracy of\n0.965035.': cannot unpack non-iterable NoneType object\nError in finding 'train_breast_cancer_32eb5_00000TERMINATED127.0.0.1:897630.000830475          5                 1   0.675899    10       0.0169384      0.640195    0.342657\n\n\n\n\n\n2025-02-11 16:13:35,717\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rdecal/ray_results/train_breast_cancer_2025-02-11_16-13-34' in 0.0018s.\n2025-02-11 16:13:35,719\tINFO tune.py:1041 -- Total run time: 1.05 seconds (1.04 seconds for the tuning loop).\n\n\nBest model parameters: {'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.675899175238225, 'eta': 0.0008304750981897656}\nBest model total accuracy: 0.6573\n\n\n\n\n\n\n Best model parameters: {'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.5015513240240503, 'eta': 0.024272050872920895}\n Best model total accuracy: 0.9301': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Show code cell output\nHide code cell output\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-24 16:22:18\nRunning for: 00:00:01.24        \nMemory:      21.5/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n    \n\n\n\nTrial Status\n\n\nTrial name            status    loc              h0   lr  iter  total time (s)       Q     theta0   theta1\n\n\ntrain_func_91d06_00000TERMINATED127.0.0.1:23610   00.015   100       0.068691 0.5906680.9        0.0427973\ntrain_func_91d06_00001TERMINATED127.0.0.1:23609   10.045   100       0.06599690.3899990.0008300930.9      \n\n\n\n\n\n\n\n\n\nAs we can see, neither trial makes it to the optimum, since the search configs are stuck with their original values. This illustrates a key advantage of PBT: while traditional hyperparameter search methods (like grid search) keep fixed search values throughout training, PBT can adapt the search dynamically, allowing it to find better solutions with the same computational budget.\n\n\nfig, axs = plt.subplots(1, 2, figsize=(13, 6), gridspec_kw=dict(width_ratios=[1.5, 1]))\n\ncolors = [\"red\", \"black\"]\nlabels = [\"h = [1, 0]\", \"h = [0, 1]\"]': cannot unpack non-iterable NoneType object\nError in finding 'In ray-cluster.external-redis.yaml, the gcsFaultToleranceOptions.externalStorageNamespace option isn\u2019t set for the RayCluster.\nTherefore, KubeRay automatically injects the environment variable RAY_external_storage_namespace to all Ray Pods managed by the RayCluster with the RayCluster\u2019s UID as the external storage namespace by default.\nSee this section to learn more about the option.\n\n\nStep 7: Kill the GCS process in the head Pod#\n\n\n# Step 7.1: Check the `RAY_gcs_rpc_server_reconnect_timeout_s` environment variable in both the head Pod and worker Pod.\nkubectl get pods $HEAD_POD -o=jsonpath='{.spec.containers[0].env}' | jq\n\n\n\n\n\n\n\n\nkubectl get pods $YOUR_WORKER_POD -o=jsonpath='{.spec.containers[0].env}' | jq': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'In ray-cluster.external-redis.yaml, the gcsFaultToleranceOptions.externalStorageNamespace option isn\u2019t set for the RayCluster.\nTherefore, KubeRay automatically injects the environment variable RAY_external_storage_namespace to all Ray Pods managed by the RayCluster with the RayCluster\u2019s UID as the external storage namespace by default.\nSee this section to learn more about the option.\n\n\nStep 7: Kill the GCS process in the head Pod#\n\n\n# Step 7.1: Check the `RAY_gcs_rpc_server_reconnect_timeout_s` environment variable in both the head Pod and worker Pod.\nkubectl get pods $HEAD_POD -o=jsonpath='{.spec.containers[0].env}' | jq\n\n\n\n\n\n\n\n\nkubectl get pods $YOUR_WORKER_POD -o=jsonpath='{.spec.containers[0].env}' | jq\n\n\n\n\n\n\n\n\n# Step 7.2: Kill the GCS process in the head Pod.\nkubectl exec -i $HEAD_POD -- pkill gcs_server\n\n\n\n\n\n\n# Step 7.3: The head Pod fails and restarts after `RAY_gcs_rpc_server_reconnect_timeout_s` (60) seconds.\n# In addition, the worker Pod isn't terminated by the new head after reconnecting because GCS fault\n# tolerance is enabled.\nkubectl get pods -l=ray.io/is-ray-node=yes': cannot unpack non-iterable NoneType object\nError in finding 'For more information about configuring network access to a Ray cluster on Kubernetes, see the networking notes.\n\n\n\n\n\nWarning\nThe Ray Dashboard provides read and write access to the Ray Cluster. The reverse proxy must provide authentication or network ingress controls to prevent unauthorized access to the Cluster.\n\n\n\nDisabling the Dashboard#\nDashboard is included if you use ray[default] or other installation commands and automatically started.\nTo disable Dashboard, use the following arguments --include-dashboard.\n\n\n\nSingle-node local cluster\nStart the cluster explicitly with CLI \nray start --include-dashboard=False\n\n\nStart the cluster implicitly with ray.init \nimport ray\nray.init(include_dashboard=False)\n\n\n\n\n\nVM Cluster Launcher\nInclude the ray start --head --include-dashboard=False argument\nin the head_start_ray_commands section of the Cluster Launcher\u2019s YAML file.\n\n\n\nKubeRay\n\nWarning\nIt\u2019s not recommended to disable Dashboard because several KubeRay features like RayJob and RayService depend on it.\n\nSet spec.headGroupSpec.rayStartParams.include-dashboard to False. Check out this example YAML file.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Tip\nsession.report can\u2019t be used within a Trainable class.\n\n\n\nLearn more about the details of Trainables here\nand have a look at our examples.\nNext, let\u2019s have a closer look at what the config dictionary is that you pass into your trainables.\n\n\n\n\nTune Trials#\nYou use Tuner.fit to execute and manage hyperparameter tuning and generate your trials.\nAt a minimum, your Tuner call takes in a trainable as first argument, and a param_space dictionary\nto define the search space.\nThe Tuner.fit() function also provides many features such as logging,\ncheckpointing, and early stopping.\nIn the example, minimizing a (x ** 2) + b, a simple Tune run with a simplistic search space for a and b\nlooks like this:\n# Pass in a Trainable class or function, along with a search space \"config\".\ntuner = tune.Tuner(trainable, param_space={\"a\": 2, \"b\": 4})\ntuner.fit()': cannot unpack non-iterable NoneType object\nError in finding 'Tip\nsession.report can\u2019t be used within a Trainable class.\n\n\n\nLearn more about the details of Trainables here\nand have a look at our examples.\nNext, let\u2019s have a closer look at what the config dictionary is that you pass into your trainables.\n\n\n\n\nTune Trials#\nYou use Tuner.fit to execute and manage hyperparameter tuning and generate your trials.\nAt a minimum, your Tuner call takes in a trainable as first argument, and a param_space dictionary\nto define the search space.\nThe Tuner.fit() function also provides many features such as logging,\ncheckpointing, and early stopping.\nIn the example, minimizing a (x ** 2) + b, a simple Tune run with a simplistic search space for a and b\nlooks like this:\n# Pass in a Trainable class or function, along with a search space \"config\".\ntuner = tune.Tuner(trainable, param_space={\"a\": 2, \"b\": 4})\ntuner.fit()': cannot unpack non-iterable NoneType object\nError in finding 'Tip\nsession.report can\u2019t be used within a Trainable class.\n\n\n\nLearn more about the details of Trainables here\nand have a look at our examples.\nNext, let\u2019s have a closer look at what the config dictionary is that you pass into your trainables.\n\n\n\n\nTune Trials#\nYou use Tuner.fit to execute and manage hyperparameter tuning and generate your trials.\nAt a minimum, your Tuner call takes in a trainable as first argument, and a param_space dictionary\nto define the search space.\nThe Tuner.fit() function also provides many features such as logging,\ncheckpointing, and early stopping.\nIn the example, minimizing a (x ** 2) + b, a simple Tune run with a simplistic search space for a and b\nlooks like this:\n# Pass in a Trainable class or function, along with a search space \"config\".\ntuner = tune.Tuner(trainable, param_space={\"a\": 2, \"b\": 4})\ntuner.fit()': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'In case you want to implement your own search algorithm, the interface is easy to implement,\nyou can read the instructions here.\nTune also provides helpful utilities to use with Search Algorithms:\n\n\nRepeated Evaluations (tune.search.Repeater): Support for running each sampled hyperparameter with multiple random seeds.\nConcurrencyLimiter (tune.search.ConcurrencyLimiter): Limits the amount of concurrent trials when running optimization.\nShim Instantiation (tune.create_searcher): Allows creation of the search algorithm object given a string.\n\n\nNote that in the example above we  tell Tune to stop after 20 training iterations.\nThis way of stopping trials with explicit rules is useful, but in many cases we can do even better with\nschedulers.\n\n# Create HyperBand scheduler and minimize the score\nhyperband = HyperBandScheduler(metric=\"score\", mode=\"max\")\n\nconfig = {\"a\": tune.uniform(0, 1), \"b\": tune.uniform(0, 1)}\n\ntuner = tune.Tuner(\n    trainable,\n    tune_config=tune.TuneConfig(\n        num_samples=20,\n        scheduler=hyperband,\n    ),\n    param_space=config,\n)\ntuner.fit()': cannot unpack non-iterable NoneType object\nError in finding 'DeprecationWarning,\n\n\n== Status ==Current time: 2022-07-22 15:41:31 (running for 00:00:06.73)Memory usage on this node: 9.9/16.0 GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.5 GiB heap, 0.0/2.0 GiB objectsCurrent best trial: 5bf98_00000 with loss=1.0234101880766688 and parameters={'mean': 1, 'sd': 0.40575843135279466}Result logdir: /Users/kai/ray_results/train_function_2022-07-22_15-41-18Number of trials: 3/3 (3 TERMINATED)\n\nTrial name                status    loc              mean      sd  iter  total time (s)   loss\n\n\ntrain_function_5bf98_00000TERMINATED127.0.0.1:48140     10.405758    30       2.11758  1.02341\ntrain_function_5bf98_00001TERMINATED127.0.0.1:48147     20.647335    30       0.07707311.53993\ntrain_function_5bf98_00002TERMINATED127.0.0.1:48151     30.256568    30       0.07284313.0393 \n\n\n2022-07-22 15:41:31,290\tINFO tune.py:738 -- Total run time: 7.36 seconds (6.72 seconds for the tuning loop).\n\n\n{'mean': 1, 'sd': 0.40575843135279466}': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp/objective_a0c11456_7_activation=tanh,height=-57.9777,steps=100,width=3.7250_2022-07-22_15-07-31/checkpoint_tmp3f09eb\n(objective pid=45117) 2022-07-22 15:07:43,765\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 2.273958206176758, '_episodes_total': 0}\n\n\n\n\nHere are the hyperparameters found to minimize the mean loss of the defined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the search space via the TuneBOHB algorithm#\nWe can define the hyperparameter search space using ConfigSpace,\nwhich is the format accepted by BOHB.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'config_space = CS.ConfigurationSpace()\nconfig_space.add_hyperparameter(\n    CS.Constant(\"steps\", 100)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20)\n)\nconfig_space.add_hyperparameter(\n    CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100)\n)\nconfig_space.add_hyperparameter(\n    CS.CategoricalHyperparameter(\n        \"activation\", choices=[\"relu\", \"tanh\"]\n    )\n)\n\n\n\n\nactivation, Type: Categorical, Choices: {relu, tanh}, Default: relu\n\n\n\n\n\n\n# As we are passing config space directly to the searcher,\n# we need to define metric and mode in it as well, in addition\n# to Tuner()\nalgo = TuneBOHB(\n    space=config_space,\n    metric=\"mean_loss\",\n    mode=\"max\",\n)\nalgo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\nscheduler = HyperBandForBOHB(\n    time_attr=\"training_iteration\",\n    max_t=100,\n    reduction_factor=4,\n    stop_last_trials=False,\n)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding '(objective pid=45401) 2022-07-22 15:11:27,287\tINFO trainable.py:655 -- Restored on 127.0.0.1 from checkpoint: /Users/kai/ray_results/bohb_exp_2/objective_2397442c_1_activation=tanh,height=32.8422,steps=100,width=12.1847_2022-07-22_15-11-11/checkpoint_tmp2e0d09\n(objective pid=45401) 2022-07-22 15:11:27,287\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 0.535153865814209, '_episodes_total': 0}\n\n\n\n\nHere again are the hyperparameters found to minimize the mean loss of the\ndefined objective.\n\n\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'activation': 'tanh', 'height': -48.451797714080236, 'steps': 100, 'width': 10.119125894538891}': cannot unpack non-iterable NoneType object\nError in finding 'Step 3: Optionally, configure the resources allocated per trial. Tune uses this resources allocation to control the parallelism. For example, if each trial was configured to use 4 CPUs, and the cluster had only 32 CPUs, then Tune will limit the number of concurrent trials to 8 to avoid overloading the cluster. For more information, see A Guide To Parallelism and Resources for Ray Tune.\n# Can customize resources per trial, here we set 1 CPU each.\ntrain_model = tune.with_resources(train_model, {\"cpu\": 1})\n\n\nStep 4: Run the trial with Tune. Tune will report on experiment status, and after the experiment finishes, you can inspect the results. Tune can retry failed trials automatically, as well as entire experiments; see How to Define Stopping Criteria for a Ray Tune Experiment.\n# Start a Tune run and print the best result.\ntuner = tune.Tuner(train_model, param_space=trial_space)\nresults = tuner.fit()\n\n# Access individual results.\nprint(results[0])\nprint(results[1])\nprint(results[2])\n\n2022-09-21 10:19:35,159     INFO tune.py:762 -- Total run time: 5.06 seconds (4.46 seconds for the tuning loop).': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Handling Failures and Node Preemption#\n\nImportant\nThis user guide shows how to configure fault tolerance for the revamped Ray Train V2\navailable starting from Ray 2.43 by enabling the environment variable RAY_TRAIN_V2_ENABLED=1.\nThis user guide assumes that the environment variable has been enabled.\nPlease see here for information about the deprecation and migration.\n\nRay Train provides fault tolerance at three levels:\n\nWorker process fault tolerance handles errors that happen to one or more Train worker processes while they are executing the user defined training function.\nWorker node fault tolerance handles node failures that may occur during training.\nJob driver fault tolerance handles the case where Ray Train driver process crashes, and training needs to be kicked off again, possibly from a new cluster.\n\nThis user guide covers how to configure and use these fault tolerance mechanisms.\n\n# Tries to recover a run up to this many times.\nfailure_config = ray.train.FailureConfig(max_failures=2)\n\n# No limit on the number of retries.\nfailure_config = ray.train.FailureConfig(max_failures=-1)': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial Progress\n\n\nTrial name                date               done  episodes_total  experiment_id                   experiment_tag    hostname              iterations_since_restore    lossnode_ip    pid  time_since_restore  time_this_iter_s  time_total_s  timestamp  timesteps_since_restoretimesteps_total    training_iterationtrial_id     warmup_time\n\n\n\n\n2022-11-02 16:03:13,913\tINFO tune.py:788 -- Total run time: 28.53 seconds (27.28 seconds for the tuning loop).\n\n\n\n\n\nTune Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 GiB       \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.44 GiB heap, 0.0/1.72 GiB objects\n    \n\n\n\nTrial Status\n\n\nTrial name                      status    loc              mean      sd  iter  total time (s)    loss': cannot unpack non-iterable NoneType object\nError in finding 'Trial Progress\n\n\nTrial name                date               done  episodes_total  experiment_id                   experiment_tag    hostname              iterations_since_restore    lossnode_ip    pid  time_since_restore  time_this_iter_s  time_total_s  timestamp  timesteps_since_restoretimesteps_total    training_iterationtrial_id     warmup_time\n\n\n\n\n2022-11-02 16:03:13,913\tINFO tune.py:788 -- Total run time: 28.53 seconds (27.28 seconds for the tuning loop).\n\n\n\n\n\nTune Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 GiB       \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.44 GiB heap, 0.0/1.72 GiB objects\n    \n\n\n\nTrial Status\n\n\nTrial name                      status    loc              mean      sd  iter  total time (s)    loss': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Trial Progress\n\n\nTrial name                date               done  episodes_total  experiment_id                   experiment_tag    hostname              iterations_since_restore    lossnode_ip    pid  time_since_restore  time_this_iter_s  time_total_s  timestamp  timesteps_since_restoretimesteps_total    training_iterationtrial_id     warmup_time\n\n\n\n\n2022-11-02 16:03:13,913\tINFO tune.py:788 -- Total run time: 28.53 seconds (27.28 seconds for the tuning loop).\n\n\n\n\n\nTune Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 GiB       \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.44 GiB heap, 0.0/1.72 GiB objects\n    \n\n\n\nTrial Status\n\n\nTrial name                      status    loc              mean      sd  iter  total time (s)    loss': cannot unpack non-iterable NoneType object\nError in finding 'Trial Progress\n\n\nTrial name                date               done  episodes_total  experiment_id                   experiment_tag    hostname              iterations_since_restore    lossnode_ip    pid  time_since_restore  time_this_iter_s  time_total_s  timestamp  timesteps_since_restoretimesteps_total    training_iterationtrial_id     warmup_time\n\n\n\n\n2022-11-02 16:03:13,913\tINFO tune.py:788 -- Total run time: 28.53 seconds (27.28 seconds for the tuning loop).\n\n\n\n\n\nTune Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 GiB       \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/3.44 GiB heap, 0.0/1.72 GiB objects\n    \n\n\n\nTrial Status\n\n\nTrial name                      status    loc              mean      sd  iter  total time (s)    loss': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Error in finding 'Wandb\u2019s group, run_id and run_name are automatically selected\nby Tune, but can be overwritten by filling out the respective configuration\nvalues.\nPlease see here for all other valid configuration settings:\nhttps://docs.wandb.ai/library/init\nPublicAPI (alpha): This API is in alpha and may change before becoming stable.\n\n\n\nsetup_wandb#\n\nParameters:\n\nconfig \u2013 Configuration dict to be logged to Weights and Biases. Can contain\narguments for wandb.init() as well as authentication information.\napi_key \u2013 API key to use for authentication with Weights and Biases.\napi_key_file \u2013 File pointing to API key for with Weights and Biases.\nrank_zero_only \u2013 If True, will return an initialized session only for the\nrank 0 worker in distributed training. If False, will initialize a\nsession for all workers.\nkwargs \u2013 Passed to wandb.init().\n\n\n\nExample\nfrom ray.air.integrations.wandb import setup_wandb\n\ndef training_loop(config):\n    wandb = setup_wandb(config)\n    # ...\n    wandb.log({\"loss\": 0.123})\n\n\nPublicAPI (alpha): This API is in alpha and may change before becoming stable.': cannot unpack non-iterable NoneType object\n"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Recall: 0.8274 \u00b1 0.3014\nPrecision: 0.0675 \u00b1 0.0567\nPrecision \u03a9: 0.3142 \u00b1 0.1939\nIoU: 0.0722 \u00b1 0.0812\n"
        }
      ]
    },
    {
      "id": "xXTn",
      "code_hash": "727c72a088cc5ff320b8b78b6972408d",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"with-sentencechunker\">With SentenceChunker</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "AjVT",
      "code_hash": "0b6df35e400a18c3992ce6113f3b4b46",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Recall: 0.8382 \u00b1 0.3019\nPrecision: 0.0475 \u00b1 0.0399\nPrecision \u03a9: 0.2648 \u00b1 0.2034\nIoU: 0.0483 \u00b1 0.0432\n"
        }
      ]
    }
  ]
}