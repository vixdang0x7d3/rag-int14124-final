question,references,corpus_id
What modifications can be made using the evaluation_metrics parameter in RLlibCallback.on_evaluate_end?,"[{""content"": ""recent evaluation round.\nevaluation_metrics \u2013 Results dict to be returned from algorithm.evaluate().\nYou can mutate this object to add additional metrics.\nkwargs \u2013 Forward "", ""start_index"": 468, ""end_index"": 640}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end.txt
What functionality does the ray.util.ActorPool utility provide?,"[{""content"": ""\n\nUtility#\n\n\nray.util.ActorPool\nUtility class to operate on a fixed pool of actors.\n\nray.util.queue.Queue\nA first-in, first-out queue implementation on "", ""start_index"": 0, ""end_index"": 152}]",datasets/raydocs_full/ray-core_api_utility.txt
What does the MultiRLModule class assume about the communication between its underlying RLModules?,"[{""content"": ""corresponding RLModule object with the associated batch within the\ninput.\nIt also assumes that the underlying RLModules do not share any parameters or\ncommunication with one another. The behavior of "", ""start_index"": 1315, ""end_index"": 1514}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.txt
What is the role of MultiRLModule in handling different RLModules?,"[{""content"": ""RLModule that contains n sub-RLModules.\nThis class holds a mapping from ModuleID to underlying RLModules. It provides\na convenient way of accessing each individual module, as well as accessing all of\nthem with only one API call. Whether a given module is trainable is\ndetermined by the caller "", ""start_index"": 443, ""end_index"": 736}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.txt
What are the required functions for implementing the InferenceOnlyAPI in an RLModule?,"[{""content"": ""ray.rllib.core.rl_module.apis.inference_only_api.InferenceOnlyAPI[source]#\nAn API to be implemented by RLModules that have an inference-only mode.\nOnly the get_non_inference_attributes method needs to get implemented for\nan RLModule to have the following functionality:\n- On EnvRunners (or when "", ""start_index"": 5535, ""end_index"": 5830}, {""content"": ""stribution for\neither Q-values or advantages (in case of a dueling architecture),\n(\u201catoms\u201d), the logits per action and atom and the probabilities\nof the discrete distribution (per action and atom of the support).\n\n\n\n\n\n\nSelfSupervisedLossAPI#\n\n\nclass ray.rllib.core.rl_module.apis.self_supervised_l"", ""start_index"": 9280, ""end_index"": 9577}]",datasets/raydocs_full/rllib_package_ref_rl_modules.txt
What happens if the learner class in AlgorithmConfig is not set by the user?,"[{""content"": ""User leaves learner class unset (None) and the AlgorithmConfig itself\nfigures out the actual learner class by calling its own\n.get_default_learner_class() method.\n\n\n"", ""start_index"": 273, ""end_index"": 466}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.txt
What are the alternative methods to set a learner class for the AlgorithmConfig?,"[{""content"": ""Algorithm.\nEither\na) User sets a specific learner class via calling .training(learner_class=...)\nb) User leaves learner class unset (None) and the AlgorithmConfig itself\nfigures out the actual learner class by calling its own\n.get_default_learner_class() method.\n\n\n"", ""start_index"": 173, ""end_index"": 466}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.txt
Why should secret credentials not be included in URIs for runtime_env in Ray?,"[{""content"": ""secret credential that authenticates this URI. While Ray can successfully access your dependencies using authenticated URIs, you should not include secret credentials in your URIs for two reasons:\n\nRay may log the URIs used in your runtime_env, which means the Ray logs could contain your credentials.\nRay stores your remote dependency package in a local directory, and it uses a parsed version of the remote URI\u2013including your credential\u2013as the directory\u2019s name.\n\nIn short, your "", ""start_index"": 924, ""end_index"": 1404}]",datasets/raydocs_full/ray-core_runtime_env_auth.txt
What is the preferred method for authenticating remote URIs in a Ray runtime environment?,"[{""content"": ""parsed version of the remote URI\u2013including your credential\u2013as the directory\u2019s name.\n\nIn short, your remote URI is not treated as a secret, so it should not contain secret info. Instead, use a netrc file.\n\n\nRunning on VMs: the netrc File#\nThe netrc file contains credentials that Ray uses to "", ""start_index"": 1304, ""end_index"": 1595}]",datasets/raydocs_full/ray-core_runtime_env_auth.txt
How can KubeRay utilize the netrc file to authenticate remote URIs?,"[{""content"": ""when they don\u2019t contain credentials.\n\n\nRunning on KubeRay: Secrets with netrc#\nKubeRay can also obtain credentials from a netrc file for remote URIs. Supply your netrc file using a Kubernetes secret and a Kubernetes volume with these steps:\n1. Launch your Kubernetes cluster.\n2. Create the netrc file locally in your home directory.\n3. Store the netrc file\u2019s contents as a Kubernetes secret on your cluster:\nkubectl create secret generic netrc-secret --from-file=.netrc=\""$HOME/.netrc\""\n\n\n4. Expose the secret to your KubeRay application using a mounted volume, and update the NETRC environment variable to point to the netrc file. Include the following YAML in your KubeRay config.\nheadGroupSpec:\n    ...\n    containers:\n        - name: ...\n          image: rayproject/ray:latest\n          ...\n          volumeMounts:\n            - mountPath: \""/home/ray/netrcvolume/\""\n              name: netrc-kuberay\n              readOnly: true\n          env:\n            - name: NETRC\n              value: \""/home/ray/netrcvolume/.netrc\""\n    volumes:\n        - name: netrc-kuberay\n          secret:\n            secretName: netrc-secret\n\nworkerGroupSpecs:\n    ...\n    containers:\n        - name: ...\n          image: rayproject/ray:latest\n          ...\n          volumeMounts:\n            - mountPath: \""/home/ray/netrcvolume/\""\n              name: netrc-kuberay\n              readOnly: true\n          env:\n            - name: NETRC\n              value: \""/home/ray/netrcvolume/.netrc\""\n    volumes:\n        - name: netrc-kuberay\n          secret:\n            secretName: netrc-secret\n\n\n5. Apply your KubeRay config.\nYour KubeRay application can use the netrc file to access private remote URIs, even when they don\u2019t contain credentials.\n\n\n"", ""start_index"": 2274, ""end_index"": 4055}]",datasets/raydocs_full/ray-core_runtime_env_auth.txt
What are the examples of batch inference workloads using PyTorch mentioned in the text?,"[{""content"": ""data processing with a variety of frameworks and use cases.BeginnerFrameworkExamplePyTorchImage Classification Batch Inference with PyTorch ResNet152PyTorchObject Detection Batch Inference with "", ""start_index"": 97, ""end_index"": 291}, {""content"": "" PyTorch FasterRCNN_ResNet50TransformersImage Classification Batch Inference with Hugging Face Vision TransformervLLMBatch Inference with LoRA AdaptervLLMBatch Inference with Structural Output"", ""start_index"": 290, ""end_index"": 482}]",datasets/raydocs_full/data_examples.txt
What tasks can be accomplished with Hugging Face Vision Transformer according to the text?,"[{""content"": ""PyTorch FasterRCNN_ResNet50TransformersImage Classification Batch Inference with Hugging Face Vision TransformervLLMBatch Inference with LoRA AdaptervLLMBatch Inference with Structural "", ""start_index"": 291, ""end_index"": 476}]",datasets/raydocs_full/data_examples.txt
What does 'MultiRLModuleSpec.build' return when 'module_id' is not provided?,"[{""content"": ""provided, otherwise the built\nMultiRLModule.\n\n\n\n\n"", ""start_index"": 387, ""end_index"": 487}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.txt
What kinds of data will not be collected according to the collection policy?,"[{""content"": ""have control over your data, and we will honor requests to delete your data.\nWe will not collect any personally identifiable data or proprietary code/data\nWe will not sell data or buy data about "", ""start_index"": 667, ""end_index"": 862}]",datasets/raydocs_full/cluster_usage-stats.txt
How can a user disable usage stats collection for a Ray cluster?,"[{""content"": ""UsageStatsToReport class to see the data we collect.\n\n\nHow to disable it#\nThere are multiple ways to disable usage stats collection before starting a cluster:\n\nAdd --disable-usage-stats option to the command that starts the Ray cluster (e.g., ray start --head --disable-usage-stats command).\nRun ray disable-usage-stats to disable collection for all future clusters. This won\u2019t affect currently running clusters. Under the hood, this command writes {\""usage_stats\"": true} to the global config file ~/.ray/config.json.\nSet the environment variable RAY_USAGE_STATS_ENABLED to 0 (e.g., RAY_USAGE_STATS_ENABLED=0 ray start --head command).\nIf you\u2019re using KubeRay, you can add disable-usage-stats: 'true' to .spec.[headGroupSpec|workerGroupSpecs].rayStartParams..\n\nCurrently "", ""start_index"": 1181, ""end_index"": 1951}]",datasets/raydocs_full/cluster_usage-stats.txt
What are the specific drawbacks of online inference solutions like Bento ML and Sagemaker Batch Transform?,"[{""content"": ""it easy to write performant inference code and can abstract away infrastructure complexities. But they are designed for online inference rather than offline batch inference, which are two different problems with different sets of requirements. These solutions introduce additional complexity like HTTP, and cannot effectively handle large datasets leading inference service providers like Bento ML to integrating with Apache Spark for offline inference.\nRay Data is built for offline batch "", ""start_index"": 1087, ""end_index"": 1577}]",datasets/raydocs_full/data_comparisons.txt
Which specific solutions provide online inference capabilities?,"[{""content"": ""fer of data from storage to CPU to GPU.\n\n\n\nOnline inference solutions: Bento ML, Sagemaker Batch Transform\n\n\n\n\nSolutions like Bento ML, Sagemaker Batch Transform, or Ray Serve provide APIs to make "", ""start_index"": 890, ""end_index"": 1087}]",datasets/raydocs_full/data_comparisons.txt
What are the advantages of using Ray Data for batch processing?,"[{""content"": ""service providers like Bento ML to integrating with Apache Spark for offline inference.\nRay Data is built for offline batch jobs, without all the extra complexities of starting servers or sending HTTP requests.\nFor a more detailed performance comparison between Ray Data and Sagemaker Batch "", ""start_index"": 1453, ""end_index"": 1744}, {""content"": "" Ray Data compare to other solutions for ML training ingest?#\n\n\nPyTorch Dataset and DataLoader\n\n\n\n\n\nFramework-agnostic: Datasets is framework-agnostic and portable between different distributed"", ""start_index"": 2315, ""end_index"": 2508}]",datasets/raydocs_full/data_comparisons.txt
What are the primary functions of Ray Tune in hyperparameter tuning?,"[{""content"": ""tuning comes into play. By using tuning libraries such as\nRay Tune we can try out combinations of hyperparameters. Using sophisticated search\nstrategies, these parameters can be selected so that they are likely to lead to good\nresults (avoiding an expensive exhaustive search). Also, trials that do not perform\nwell can be preemptively stopped to reduce waste of computing resources. Lastly, Ray Tune\nalso takes care of training these runs in parallel, greatly increasing search "", ""start_index"": 11075, ""end_index"": 11554}]",datasets/raydocs_full/tune_examples_tune-xgboost.txt
What is the default value for the maximum tree depth in XGBoost?,"[{""content"": ""and 6 is often a good starting point for this parameter.\nXGBoost\u2019s default value is 3.\n\n\nMinimum "", ""start_index"": 6573, ""end_index"": 6670}]",datasets/raydocs_full/tune_examples_tune-xgboost.txt
What are some key advantages of using XGBoost for machine learning?,"[{""content"": ""xgboost\n\n\n\n\nWhat is XGBoost#\nXGBoost (eXtreme Gradient Boosting) is a powerful and efficient implementation of gradient boosted decision trees. It has become one of the most popular machine learning algorithms due to its:\n\nPerformance: Consistently strong results across many types of problems\nSpeed: Highly optimized implementation that can leverage GPU acceleration\nFlexibility: Works with many types of prediction problems (classification, regression, ranking)\n\nKey "", ""start_index"": 967, ""end_index"": 1436}]",datasets/raydocs_full/tune_examples_tune-xgboost.txt
What issues arise from forking new processes in Ray application code?,"[{""content"": ""processes for you. Ray Objects, Tasks, and\nActors manages sockets to communicate with the Raylet and the GCS. If you fork new\nprocesses in your application code, the processes could share the same sockets without\nany synchronization. This can lead to corrupted message and unexpected\nbehavior.\nThe solution is to:\n1. use \u201cspawn\u201d method to start new processes so that "", ""start_index"": 295, ""end_index"": 662}]",datasets/raydocs_full/ray-core_patterns_fork-new-processes.txt
What are the recommended methods to start new processes in Ray application code?,"[{""content"": ""application code-for example, in\ndriver, tasks or actors. Instead, use \u201cspawn\u201d method to start new processes or use Ray\ntasks and actors to parallelize your workload\nRay manages the lifecycle of "", ""start_index"": 100, ""end_index"": 295}, {""content"": "" process\u2019s\nmemory space isn\u2019t copied to the child processes or\n2. use Ray tasks and\nactors to parallelize your workload and let Ray to manage the lifecycle of the\nprocesses for you.\n\nCode example#\nimport os\n\nos.environ[\""RAY_DEDUP_LOGS\""] = \""0\""\n\nimport ray\nfrom concurrent.futures import Pr"", ""start_index"": 668, ""end_index"": 956}]",datasets/raydocs_full/ray-core_patterns_fork-new-processes.txt
What is the expected behavior when setting environment variables on the Driver in Ray?,"[{""content"": ""1\n\n\nray.get(myfunc.remote())\n# this prints: \""myenv is None\""\n\n\nExpected behavior: Users may expect that setting environment variables on the Driver sends them to all Worker processes as if running "", ""start_index"": 815, ""end_index"": 1011}]",datasets/raydocs_full/ray-observability_user-guides_debug-apps_general-debugging.txt
What exception is raised by MultiRLModule.remove_module when a specified module ID does not exist?,"[{""content"": ""not\nfound.\n\n\nRaises:\nValueError \u2013 If the module ID does not exist and raise_err_if_not_found is\n    True.\n\n\n\n\n"", ""start_index"": 353, ""end_index"": 553}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.txt
What parameter determines if an error is raised when a module ID is not found using MultiRLModule.remove_module?,"[{""content"": ""remove.\nraise_err_if_not_found \u2013 Whether to raise an error if the module ID is not\nfound.\n\n\nRaises:\nValueError \u2013 If the module ID does not exist and raise_err_if_not_found is\n    "", ""start_index"": 274, ""end_index"": 453}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.txt
What function in MultiRLModule is used to remove a module at runtime?,"[{""content"": ""\n\nray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module#\n\n\nMultiRLModule.remove_module(module_id: str, *, raise_err_if_not_found: bool = True) \u2192 None[source]#\nRemoves a module at "", ""start_index"": 0, ""end_index"": 194}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.txt
What does the function 'convert_to_torch_tensor' do with None values in the input structure?,"[{""content"": ""Tensor.to().\n\n\nReturns:\nA new structure with the same layout as x but with all leaves converted\nto torch.Tensors. Leaves that are None are left unchanged.\n\n\n\n\n"", ""start_index"": 664, ""end_index"": 863}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.torch_utils.convert_to_torch_tensor.txt
What optional parameters does the function 'convert_to_torch_tensor' accept?,"[{""content"": ""h_tensor(x, device: str | None = None, pin_memory: bool = False, use_stream: bool = False, stream: torch.cuda.Stream | torch.cuda.classes.Stream | None = None)[source]#\nConverts any (possibly nested) structure to torch.Tensors.\n\nParameters:\n\nx \u2013 The input structure whose leaves will be converted.\ndevice \u2013 The device to create the tensor on (e.g. \u201ccuda:0\u201d or \u201ccpu\u201d).\npin_memory \u2013 If True, calls pin_memory() on the created tensors.\nuse_stream \u2013 If True, uses a separate CUDA stream for Tensor.to().\nstream \u2013 An optional CUDA stream for the host-to-device copy in "", ""start_index"": 100, ""end_index"": 664}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.torch_utils.convert_to_torch_tensor.txt
What are the methods for achieving concurrency in Ray applications?,"[{""content"": ""the number of pending tasks\nPattern: Using resources to limit the number of concurrently running tasks\nPattern: Using asyncio to run actor methods concurrently\nPattern: Using an actor to "", ""start_index"": 285, ""end_index"": 472}, {""content"": ""a task harms performance and fault tolerance\nAnti-pattern: Calling ray.get in a loop harms parallelism\nAnti-pattern: Calling ray.get unnecessarily harms performance\nAnti-pattern: "", ""start_index"": 667, ""end_index"": 846}]",datasets/raydocs_full/ray-core_patterns_index.txt
How does Ray Data handle tensors of variable shapes?,"[{""content"": ""batch[\""image\""].dtype\ndtype('uint8')\n\n\n\n\nBatches of variable-shape tensors#\nIf your tensors vary in shape, Ray Data represents batches as arrays of object dtype.\n>>> import ray\n>>> ds = "", ""start_index"": 742, ""end_index"": 927}]",datasets/raydocs_full/data_working-with-tensors.txt
What action occurs in PopulationBasedTraining when cloning a lower scoring trial into a higher scoring one?,"[{""content"": ""\n\n[PopulationBasedTraining] [Exploit] Cloning trial 942f2_00000 (score = 0.090282) into trial 942f2_00001 (score = -0.168306)\n\n2025-02-24 16:22:22,343\tINFO pbt.py:905 -- "", ""start_index"": 44427, ""end_index"": 44597}, {""content"": ""03 (score = 0.239975)\n\n2025-02-24 16:22:25,541\tINFO pbt.py:905 -- \n\n[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial942f2_00003:\nlr : 0.09"", ""start_index"": 46913, ""end_index"": 47083}]",datasets/raydocs_full/tune_examples_pbt_visualization_pbt_visualization.txt
What are the significant indicators shown in the training progress animation for PBT?,"[{""content"": ""filename=\""pbt.gif\"",\n)\n\n\n\n\nWe can also animate the training progress to see what\u2019s happening to the model parameters at each step. The animation shows:\n\nHow parameters move through space during training\nWhen exploitation occurs (jumps in parameter space)\nHow gradient directions change after hyperparameter perturbation\nBoth trials eventually converging to the optimal parameter "", ""start_index"": 32541, ""end_index"": 32919}]",datasets/raydocs_full/tune_examples_pbt_visualization_pbt_visualization.txt
What types of data can the Distribution.logp method accept?,"[{""content"": ""\n\nray.rllib.models.distributions.Distribution.logp#\n\n\nabstract Distribution.logp(value: numpy.array | jnp.ndarray | tf.Tensor | torch.Tensor, **kwargs) \u2192 numpy.array | jnp.ndarray | tf.Tensor | "", ""start_index"": 0, ""end_index"": 194}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.models.distributions.Distribution.logp.txt
What is computed by the Distribution.logp method?,"[{""content"": ""torch.Tensor[source]#\nThe log-likelihood of the distribution computed at value\n\nParameters:\n\nvalue "", ""start_index"": 194, ""end_index"": 293}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.models.distributions.Distribution.logp.txt
What are the benefits of the streaming loading interface patch developed by Data-Juicer for Apache Arrow?,"[{""content"": ""18.1.0), doesn\u2019t support streaming reading of JSON files.\nTo address the lack of native support for streaming JSON data, the Data-Juicer team developed a streaming loading interface and contributed an in-house patch for Apache Arrow (PR to the repository). This patch helps alleviate Out-of-Memory issues. With this patch, Data-Juicer in Ray mode, by default, uses the streaming loading interface "", ""start_index"": 2896, ""end_index"": 3293}]",datasets/raydocs_full/ray-more-libs_data_juicer_distributed_data_processing.txt
What is the general size of datasets deduplicated by Data-Juicer's MinHash-based operator?,"[{""content"": ""streaming-read support for CSV and Parquet files is already enabled.\n\n\nDeduplication#\nData-Juicer provides an optimized MinHash-LSH-based deduplication operator in Ray mode. It\u2019s a multiprocessing Union-Find set in Ray Actors and a load-balanced distributed algorithm, BTS, to complete equivalence class merging. This operator can deduplicate terabyte-sized datasets on 1280 CPU cores "", ""start_index"": 3326, ""end_index"": 3711}]",datasets/raydocs_full/ray-more-libs_data_juicer_distributed_data_processing.txt
What features of Ray and Arrow are considered for pre-splitting datasets in Data-Juicer?,"[{""content"": ""for such cases, Data-Juicer automatically splits the original dataset into smaller files in advance, taking into consideration the features of Ray and Arrow. When you encounter such "", ""start_index"": 2068, ""end_index"": 2250}]",datasets/raydocs_full/ray-more-libs_data_juicer_distributed_data_processing.txt
How can GPU resources be allocated to a deployment using Ray Serve?,"[{""content"": ""replica uses a single GPU, you can do the\nfollowing:\n@serve.deployment(ray_actor_options={\""num_gpus\"": 1})\ndef func(*args):\n    return "", ""start_index"": 733, ""end_index"": 867}, {""content"": "" \""num_cpus\"".\n\n\nCustom resources, accelerator types, and more#\nYou can also specify custom resources in ray_actor_options, for example to ensure that a deployment is scheduled on a specific node.\nFor example, if you have a deployment that requires 2 units of the \""custom_resource\"" resource, you can specify it like this:\n@serve.deployment(ray_actor_options={\""resources\"": {\""custom_resou"", ""start_index"": 1898, ""end_index"": 2282}]",datasets/raydocs_full/serve_resource-allocation.txt
What types of resource management does Ray Serve support for its deployments?,"[{""content"": "" helps you configure Ray Serve to:\n\nScale your deployments horizontally by specifying a number of replicas\nScale up and down automatically to react to changing traffic\nAllocate hardware resources (CPUs, GPUs, other accelerators, etc) for each deployment\n\n\nResource management (CPUs, GPUs, "", ""start_index"": 33, ""end_index"": 322}, {""content"": ""(CPUs, GPUs, other accelerators, etc) for each deployment\n\n\nResource management (CPUs, GPUs, accelerators)#\nYou may want to specify a deployment\u2019s resource requirements to reserve cluster resources like GPUs or other accelerators.  To assign hardware resources per replica, you can pass "", ""start_index"": 229, ""end_index"": 516}]",datasets/raydocs_full/serve_resource-allocation.txt
How does Ray Serve manage CPU resource allocations for replicas by default?,"[{""content"": ""resource requirements to\nray_actor_options.\nBy default, each replica reserves one CPU.\nTo learn "", ""start_index"": 516, ""end_index"": 612}]",datasets/raydocs_full/serve_resource-allocation.txt
What are the specific integration examples listed for using Ray Tune with different machine learning frameworks?,"[{""content"": ""frameworks#\n\n\nRay Tune integrates with many popular machine learning frameworks. Here you find a few practical examples showing you how to tune your models. At the end of these guides you will often find links to even more examples.\n\n\nHow to use Tune with Keras and TensorFlow models\n\nHow to use Tune with PyTorch models\n\nHow to tune PyTorch Lightning models\n\nTuning RL experiments with Ray Tune and Ray Serve\n\nTuning XGBoost parameters with Tune\n\nTuning LightGBM parameters with Tune\n\nTuning Horovod parameters with Tune\n\nTuning Hugging Face Transformers with Tune\n\nEnd-to-end example for tuning a TensorFlow model\n\nEnd-to-end example for tuning a PyTorch model with PBT\n\n\n\n\n\nExperiment tracking tools#\n\n\nRay Tune integrates with some popular Experiment tracking and "", ""start_index"": 273, ""end_index"": 1041}]",datasets/raydocs_full/tune_examples_index.txt
What are the parameters of the RLlibCallback.on_episode_created method?,"[{""content"": ""\n\nray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created#\n\n\nRLlibCallback.on_episode_created(*, episode: SingleAgentEpisode | MultiAgentEpisode | EpisodeV2, worker: EnvRunner | None = None, env_runner: EnvRunner | None = None, metrics_logger: MetricsLogger | None = None, base_env: BaseEnv | None = None, env: gymnasium.Env | None = None, policies: Dict[str, Policy] | None = None, rl_module: RLModule | None = None, env_index: int, **kwargs) \u2192 None[source]#\nCallback run when a "", ""start_index"": 0, ""end_index"": 487}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created.txt
What topics are covered in the user guides for Ray applications?,"[{""content"": ""guides include:\n\nDebugging Applications\nMonitoring with the CLI or SDK\nConfiguring Logging\nAdding Application-Level Metrics\nTracing\n\n\n"", ""start_index"": 97, ""end_index"": 295}]",datasets/raydocs_full/ray-observability_user-guides_index.txt
What specific guides are provided for monitoring Ray applications?,"[{""content"": ""guides include:\n\nDebugging Applications\nMonitoring with the CLI or SDK\nConfiguring Logging\nAdding "", ""start_index"": 97, ""end_index"": 195}]",datasets/raydocs_full/ray-observability_user-guides_index.txt
What are the specific configurations for enabling model parallelism in the vLLMEngineProcessorConfig?,"[{""content"": ""ls, specify model parallelism.\nconfig = vLLMEngineProcessorConfig(\n    model_source=\""unsloth/Llama-3.1-8B-Instruct\"",\n    engine_kwargs={\n        \""max_model_len\"": 16384,\n        \""tensor_parallel_size\"": 2,\n        \""pipeline_parallel_size\"": 2,\n        \""enable_chunked_prefill\"": True,\n        \""max_num_batched_tokens\"": 2048,\n    },\n    concurrency=1,\n    batch_size=64,\n)\n\n\nThe underlying Processor object instantiates replicas of the vLLM engine and "", ""start_index"": 3324, ""end_index"": 3771}]",datasets/raydocs_full/data_working-with-llms.txt
What is the specified model source for the vLLMEngineProcessorConfig that includes AWS S3 details?,"[{""content"": ""argument.\nconfig = vLLMEngineProcessorConfig(\n    model_source=\""s3://your-bucket/your-model/\"",  # Make sure adding the trailing slash!\n    engine_kwargs={\""load_format\"": \""runai_streamer\""},\n    runtime_env={\""env_vars\"": {\n        \""AWS_ACCESS_KEY_ID\"": \""your_access_key_id\"",\n        \""AWS_SECRET_ACCESS_KEY\"": \""your_secret_access_key\"",\n        \""AWS_REGION\"": \""your_region\"",\n    }},\n    concurrency=1,\n    batch_size=64,\n)\n\n\nTo do multi-LoRA batch inference, you need to set LoRA "", ""start_index"": 4430, ""end_index"": 4901}]",datasets/raydocs_full/data_working-with-llms.txt
What steps are involved in setting up a RayJob using KubeRay?,"[{""content"": ""is step creates a local Kubernetes cluster using Kind. If you already have a Kubernetes cluster, you can skip this step.\nkind create cluster --image=kindest/node:v1.26.0\n\n\n\n\nStep 2: Install "", ""start_index"": 258, ""end_index"": 448}, {""content"": ""KubeRay operator#\nFollow this document to install the latest stable KubeRay operator from the Helm repository.\n\n\nStep 3: Create a RayJob#\nA RayJob consists of a RayCluster custom resource and a job "", ""start_index"": 448, ""end_index"": 646}, {""content"": ""bmits a job when the cluster is ready. The following is a CPU-only RayJob description YAML file for MNIST training on a PyTorch model.\n# Download `ray-job.pytorch-mnist.yaml`\ncurl -LO https://raw.gi"", ""start_index"": 737, ""end_index"": 935}, {""content"": ""US      RESTARTS   AGE\n# kuberay-operator-6dddd689fb-ksmcs                                1/1     Running     0          113m\n# rayjob-pytorch-mnist-raycluster-rkdmq-small-group-"", ""start_index"": 3801, ""end_index"": 3979}]",datasets/raydocs_full/cluster_kubernetes_examples_mnist-training-example.txt
What are the requirements for the worker and head Pods in a RayJob using KubeRay?,"[{""content"": ""Each worker Pod requires 3 CPUs, and the head Pod requires 1 CPU, as described in the template "", ""start_index"": 1315, ""end_index"": 1410}]",datasets/raydocs_full/cluster_kubernetes_examples_mnist-training-example.txt
How long did the RayJob's training process take?,"[{""content"": ""Training completed after 10 iterations at 2024-06-16 22:33:06. Total running time: 7min 10s\n\n# "", ""start_index"": 5521, ""end_index"": 5616}]",datasets/raydocs_full/cluster_kubernetes_examples_mnist-training-example.txt
What is the property mentioned in the context of ray.rllib.offline.offline_data.OfflineData?,"[{""content"": ""OfflineData.default_map_batches_kwargs#\n\n\n"", ""start_index"": 84, ""end_index"": 184}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.txt
Which property of OfflineData is described in the text?,"[{""content"": ""\n\nray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs#\n\n\nproperty OfflineData.default_map_batches_kwargs#\n\n\n"", ""start_index"": 0, ""end_index"": 184}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.txt
What action space does an RL environment define?,"[{""content"": ""which is the structure and shape of observable tensors at each timestep,\nan action space, which defines the available actions for the agents at each time step, a reward function,\nand the rules "", ""start_index"": 3797, ""end_index"": 3990}]",datasets/raydocs_full/rllib_key-concepts.txt
How can you mitigate the issue of Torch tensors on GPU devices when saving to disk?,"[{""content"": ""NumPy.\nFor more information on saving data, read\nSaving data.\n\nCaution\nTorch tensors that are on GPU devices can\u2019t be serialized and written to disk. Convert the tensors to CPU (tensor.to(\""cpu\"")) before saving the data.\n\n\n\n\nParquet\nimport torch\nimport ray\n\ntensor = torch.Tensor(1)\nds = "", ""start_index"": 7807, ""end_index"": 8094}]",datasets/raydocs_full/data_working-with-pytorch.txt
What are the key features of Ray Data's integration with Ray Train?,"[{""content"": ""Train#\nRay Data integrates with Ray Train for easy data ingest for data parallel training, with support for PyTorch, PyTorch Lightning, or Hugging Face training.\nimport torch\nfrom torch import "", ""start_index"": 1080, ""end_index"": 1273}]",datasets/raydocs_full/data_working-with-pytorch.txt
How does the operation of iterating over batches differ between PyTorch DataLoader and Ray Data?,"[{""content"": ""ds.map(extract_label).map(transform_image)\n\n\n\n\n\n\nPyTorch DataLoader#\nThe PyTorch DataLoader can be replaced by calling Dataset.iter_torch_batches() to iterate over batches of the dataset.\nThe "", ""start_index"": 11977, ""end_index"": 12169}]",datasets/raydocs_full/data_working-with-pytorch.txt
What is the return value of get_module if the specified module_id is not found?,"[{""content"": ""'default_policy') \u2192 RLModule | None[source]#\nReturns the (single-agent) RLModule with model_id (None if ID not found).\n\nParameters:\nmodule_id \u2013 ID of the (single-agent) RLModule to return from "", ""start_index"": 95, ""end_index"": 288}, {""content"": "" inside the local EnvRunner\u2019s\nMultiRLModule. None if module_id doesn\u2019t exist.\n\n\n\n\n"", ""start_index"": 383, ""end_index"": 579}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.get_module.txt
What does the get_module function return when the specified module_id is 'default_policy'?,"[{""content"": ""\n\nray.rllib.algorithms.algorithm.Algorithm.get_module#\n\n\nAlgorithm.get_module(module_id: str = 'default_policy') \u2192 RLModule | None[source]#\nReturns the (single-agent) RLModule with model_id "", ""start_index"": 0, ""end_index"": 190}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.get_module.txt
What parameter is used by the Algorithm.get_module function to identify the RLModule to return?,"[{""content"": ""(None if ID not found).\n\nParameters:\nmodule_id \u2013 ID of the (single-agent) RLModule to return from the MARLModule\nused by the local EnvRunner.\n\nReturns:\nThe RLModule found under the ModuleID key "", ""start_index"": 190, ""end_index"": 384}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.get_module.txt
How does KubeRay manage the Redis cleanup process?,"[{""content"": ""Redis cleanup.\nKubeRay only removes this finalizer after the Kubernetes Job successfully cleans up Redis.\n\nIn other words, if the Kubernetes Job fails, the RayCluster won\u2019t be deleted. In that case, "", ""start_index"": 14895, ""end_index"": 15094}, {""content"": ""a best-effort basis.\nKubeRay still removes the Kubernetes finalizer from the RayCluster if the Kubernetes Job fails, thereby unblocking the deletion of the RayCluster.\nUsers can turn off this by "", ""start_index"": 15387, ""end_index"": 15582}, {""content"": ""tion in conjunction with the YAML file.\n\nThese configurations require KubeRay 1.3.0+\nThe following section uses the new gcsFaultToleranceOptions field introduced in KubeRay 1.3.0. For the old GCS"", ""start_index"": 15979, ""end_index"": 16174}]",datasets/raydocs_full/cluster_kubernetes_user-guides_kuberay-gcs-ft.txt
What are the different timeout values set for the head Pod and the worker Pods in RayCluster?,"[{""content"": ""RayCluster.\nTherefore, KubeRay automatically injects the RAY_gcs_rpc_server_reconnect_timeout_s environment variable with the value 600 to the worker Pod and uses the default value 60 for the head Pod.\nThe timeout value for worker Pods must be longer than the timeout value for the head Pod "", ""start_index"": 12553, ""end_index"": 12844}]",datasets/raydocs_full/cluster_kubernetes_user-guides_kuberay-gcs-ft.txt
What is the dashboard URL for the connected Ray cluster?,"[{""content"": ""-- Connected to Ray cluster. View the dashboard at 10.244.0.8:8265 \n2025-04-18 02:51:29,069\tINFO "", ""start_index"": 4798, ""end_index"": 4895}]",datasets/raydocs_full/cluster_kubernetes_user-guides_kuberay-gcs-ft.txt
What methods must subclasses of ray.rllib.utils.checkpoints.Checkpointable implement?,"[{""content"": ""of RLlib that can be checkpointed to disk.\nSubclasses must implement the following APIs:\n- save_to_path()\n- restore_from_path()\n- from_checkpoint()\n- get_state()\n- set_state()\n- get_ctor_args_and_kwargs()\n- get_metadata()\n- get_checkpointable_components()\nPublicAPI (alpha): "", ""start_index"": 153, ""end_index"": 428}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.checkpoints.Checkpointable.txt
What class does ray.rllib.utils.checkpoints.Checkpointable inherit from?,"[{""content"": ""ray.rllib.utils.checkpoints.Checkpointable[source]#\nBases: ABC\nAbstract base class for a component "", ""start_index"": 54, ""end_index"": 153}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.checkpoints.Checkpointable.txt
What is the purpose of the set_state method in MultiRLModule?,"[{""content"": "" Dict[str, Any]) \u2192 None[source]#\nSets the state of the multi-agent module.\nIt is assumed that the state_dict is a mapping from module IDs to the\ncorresponding module\u2019s state. This method sets the state of each module by\ncalling their set_state method. If you want to set the state of some of "", ""start_index"": 100, ""end_index"": 392}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.txt
What does the state_dict represent in the context of MultiRLModule.set_state method?,"[{""content"": "" Dict[str, Any]) \u2192 None[source]#\nSets the state of the multi-agent module.\nIt is assumed that the state_dict is a mapping from module IDs to the\ncorresponding module\u2019s state. This method sets the "", ""start_index"": 100, ""end_index"": 296}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.txt
What is the default value of the inference_only attribute in RLModuleSpec?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only#\n\n\nRLModuleSpec.inference_only: bool = False#\n\n\n"", ""start_index"": 0, ""end_index"": 197}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only.txt
What is the eval_env_runner in the context of the Algorithm?,"[{""content"": ""Algorithm.eval_env_runner#\nThe local EnvRunner instance within the algo\u2019s evaluation EnvRunnerGroup.\n\n\n"", ""start_index"": 71, ""end_index"": 256}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner.txt
What details are provided about the MultiAgentEnv class in the ray.rllib.env module?,"[{""content"": ""ray.rllib.env.multi_agent_env.MultiAgentEnv(*args: Any, **kwargs: Any)[source]#\nAn environment that hosts multiple independent agents.\nAgents are identified by AgentIDs (string).\nPublicAPI (beta): "", ""start_index"": 349, ""end_index"": 546}]",datasets/raydocs_full/rllib_package_ref_env_multi_agent_env.txt
What are the output values provided by the step method for the MultiAgentEnv instance?,"[{""content"": ""env can vary over time.\n\nReturns:\nTuple containing 1) new observations for\neach ready agent, 2) reward values for each ready agent. If\nthe episode is just started, the value will be None.\n3) Terminated values for each ready agent. The special key\n\u201c__all__\u201d (required) is used to indicate env termination.\n4) Truncated values for each ready agent.\n5) Info values for each agent id (may be empty dicts).\n\n\nenv = ...\nobs, rewards, terminateds, truncateds, infos = env.step(action_dict={\n    "", ""start_index"": 1479, ""end_index"": 1967}]",datasets/raydocs_full/rllib_package_ref_env_multi_agent_env.txt
What is the environment variable associated with the NEURON_RT_VISIBLE_CORES for the Neuron Core in the Ray framework?,"[{""content"": ""print(\""NEURON_RT_VISIBLE_CORES: {}\"".format(os.environ[\""NEURON_RT_VISIBLE_CORES\""]))\n\n@ray.remote(resources={\""neuron_cores\"": 1})\ndef "", ""start_index"": 6690, ""end_index"": 6821}]",datasets/raydocs_full/ray-core_scheduling_accelerators.txt
How does Ray assign accelerators to tasks or actors that require them?,"[{""content"": ""and Actors#\nIf a task or actor requires accelerators, you can specify the corresponding resource requirements (e.g. @ray.remote(num_gpus=1)).\nRay then schedules the task or actor to a node that has enough free accelerator resources\nand assign accelerators to the task or actor by setting the corresponding environment variable (e.g. CUDA_VISIBLE_DEVICES) before running the task or actor "", ""start_index"": 3537, ""end_index"": 3925}]",datasets/raydocs_full/ray-core_scheduling_accelerators.txt
What does ExecutionResources specify in the context of ExecutionOptions?,"[{""content"": ""execution.\n\n\n\n\n\nResource Options#\n\n\nExecutionResources\nSpecifies resources usage or resource limits for execution.\n\n\n\n\n\n"", ""start_index"": 76, ""end_index"": 276}]",datasets/raydocs_full/data_api_execution_options.txt
What does the ExecutionOptions API include?,"[{""content"": ""\n\nExecutionOptions API#\n\nConstructor#\n\n\nExecutionOptions\nCommon options for execution.\n\n\n\n\n\nResource Options#\n\n\nExecutionResources\nSpecifies resources usage or resource limits "", ""start_index"": 0, ""end_index"": 176}, {""content"": ""execution.\n\n\n\n\n\nResource Options#\n\n\nExecutionResources\nSpecifies resources usage or resource limits for execution.\n\n\n\n\n\n"", ""start_index"": 76, ""end_index"": 276}]",datasets/raydocs_full/data_api_execution_options.txt
What methods are available for iterating over batches in the Ray Data dataset?,"[{""content"": ""batches#\nA batch contains data from multiple rows. Iterate over batches of dataset in different\nformats by calling one of the following methods:\n\nDataset.iter_batches() <ray.data.Dataset.iter_batches>\nDataset.iter_torch_batches() <ray.data.Dataset.iter_torch_batches>\nDataset.to_tf() <ray.data.Dataset.to_tf>\n\n\n\n\nNumPy\nimport "", ""start_index"": 951, ""end_index"": 1277}]",datasets/raydocs_full/data_iterating-over-data.txt
What are some techniques to optimize the shuffling of batches?,"[{""content"": ""shuffling#\nDataset.random_shuffle is slow because it\nshuffles all rows. If a full global shuffle isn\u2019t required, you can shuffle a subset of\nrows up to a provided buffer size during iteration by specifying\nlocal_shuffle_buffer_size. While this isn\u2019t a true global shuffle like\nrandom_shuffle, it\u2019s more performant because it doesn\u2019t require excessive data\nmovement. For more details about these options, see Shuffling Data.\n\nTip\nTo configure local_shuffle_buffer_size, choose the smallest value that achieves\nsufficient randomness. Higher values result in more randomness at the cost of slower\niteration. See Local shuffle when iterating over batches\non how to diagnose slowdowns.\n\n\n\n\nNumPy\nimport ray\n\nds = "", ""start_index"": 3093, ""end_index"": 3801}]",datasets/raydocs_full/data_iterating-over-data.txt
What are the consequences of calling ray.get() unnecessarily?,"[{""content"": ""\n\nAnti-pattern: Calling ray.get unnecessarily harms performance#\nTLDR: Avoid calling ray.get() unnecessarily for intermediate steps. Work with object references directly, and only call ray.get() at the end to get the final result.\nWhen ray.get() is called, objects must be transferred to the worker/node that calls ray.get(). If you don\u2019t need to manipulate the object, you probably don\u2019t need to call ray.get() on it!\nTypically, it\u2019s best practice to wait as long as possible before calling ray.get(), or even design your program to avoid having to call ray.get() at all.\n\nCode "", ""start_index"": 0, ""end_index"": 579}, {""content"": ""ray.get(reduce.remote(rollout_obj_ref))\n\n\n\n\nNotice in the anti-pattern example, we call ray.get() which forces us to transfer the large rollout to the driver, then again to the reduce worker.\nIn the fixed version, we only pass the reference to the object to the reduce task.\nThe reduce worker will implicitly call ray.get() to fetch the actual rollout data directly from the generate_rollout "", ""start_index"": 1088, ""end_index"": 1480}]",datasets/raydocs_full/ray-core_patterns_unnecessary-ray-get.txt
What is the difference between the anti-pattern and better approach in handling ray.get()?,"[{""content"": ""ray.get(reduce.remote(rollout_obj_ref))\n\n\n\n\nNotice in the anti-pattern example, we call ray.get() which forces us to transfer the large rollout to the driver, then again to the reduce worker.\nIn the fixed version, we only pass the reference to the object to the reduce task.\nThe reduce worker will implicitly call ray.get() to fetch the actual rollout data directly from the generate_rollout worker, avoiding the extra copy to the driver.\nOther ray.get() related anti-patterns "", ""start_index"": 1088, ""end_index"": 1565}]",datasets/raydocs_full/ray-core_patterns_unnecessary-ray-get.txt
What is the detailed process for creating a Docker image with the Faker package?,"[{""content"": ""rayproject/ray:2.9.0 image hosted by rayproject/ray.\nYou can extend these images and add your own dependencies to them by using them as a base layer in a Dockerfile. For instance, the working example application uses Ray 2.9.0 and Faker 18.13.0. You can create a Dockerfile that extends the rayproject/ray:2.9.0 by adding the Faker package:\n# File name: Dockerfile\nFROM rayproject/ray:2.9.0\n\nRUN pip install Faker==18.13.0\n\n\nIn general, the rayproject/ray images "", ""start_index"": 1242, ""end_index"": 1705}, {""content"": ""on of the package. You can also replace latest with a specific version if you prefer.\n\n\nAdding your Serve application to the Docker image#\nDuring development, it\u2019s useful to package your Serve application into a zip file and pull it i"", ""start_index"": 2389, ""end_index"": 2623}]",datasets/raydocs_full/serve_production-guide_docker.txt
How can you set up a Docker image to include a Serve application for Ray?,"[{""content"": ""commands inside the Dockerfile to install the example Serve application code in your image:\n# File name: Dockerfile\nFROM rayproject/ray:2.9.0\n\nRUN pip install Faker==18.13.0\n\n# Set the working dir for the container to /serve_app\nWORKDIR /serve_app\n\n# Copies the local `fake.py` file into the WORKDIR\nCOPY fake.py /serve_app/fake.py\n\n\nKubeRay starts Ray with the ray start command inside the "", ""start_index"": 2906, ""end_index"": 3297}]",datasets/raydocs_full/serve_production-guide_docker.txt
What steps are involved in deploying a custom Docker image in KubeRay?,"[{""content"": ""custom Docker images in KubeRay#\nRun these custom Docker images in KubeRay by adding them to the RayService config. Make the following changes:\n\nSet the rayVersion in the rayClusterConfig to the Ray version used in your custom Docker image.\nSet the ray-head container\u2019s image to the custom image\u2019s name on Dockerhub.\nSet the ray-worker container\u2019s image to the custom image\u2019s name on Dockerhub.\nUpdate the  serveConfigV2 field to remove any runtime_env dependencies that are in the container.\n\nA pre-built version of this image is available at "", ""start_index"": 3768, ""end_index"": 4312}]",datasets/raydocs_full/serve_production-guide_docker.txt
What are the three ways to start the Ray runtime?,"[{""content"": ""single server, or multiple servers.\nThere are three ways of starting the Ray runtime:\n\nImplicitly via ray.init() (Starting Ray on a single machine)\nExplicitly via CLI (Starting Ray via the CLI (ray start))\nExplicitly via the cluster launcher (Launching a Ray cluster (ray up))\n\nIn all cases, "", ""start_index"": 485, ""end_index"": 777}]",datasets/raydocs_full/ray-core_starting-ray.txt
What happens to the Ray runtime when the process calling ray.init() terminates?,"[{""content"": ""`ray::Init()` is called.\nray::Init()\n\n\n\n\nWhen the process calling ray.init() terminates, the Ray runtime will also terminate. To explicitly stop or restart Ray, use the shutdown "", ""start_index"": 1717, ""end_index"": 1895}]",datasets/raydocs_full/ray-core_starting-ray.txt
What config settings can be set for fault tolerance in this AlgorithmConfig?,"[{""content"": ""config's evaluation settings.\n\nexperimental\nSets the config's experimental settings.\n\nfault_tolerance\nSets the config's fault tolerance settings.\n\nframework\nSets the config's "", ""start_index"": 1802, ""end_index"": 1977}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.txt
What method in ray.rllib.core.rl_module.rl_module.RLModule should be overridden for generic behavior across all training and evaluation phases?,"[{""content"": ""exploratory\naction computation behavior. If you have only one generic behavior for all\nphases of training and evaluation, override self._forward() instead.\nBy default, this calls the generic "", ""start_index"": 288, ""end_index"": 479}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.txt
How can a user manually specify resources when starting a single node Ray cluster using ray.init()?,"[{""content"": ""depending on how you start the Ray cluster:\n\n\n\nray.init()\nIf you are using ray.init() to start a single node Ray cluster, you can do the following to manually specify node resources:\n# This will start a Ray node with 3 logical cpus, 4 logical gpus,\n# 1 special_hardware resource and 1 custom_label resource.\nray.init(num_cpus=3, num_gpus=4, resources={\""special_hardware\"": 1, \""custom_label\"": 1})\n\n\n\n\n\nray start\nIf you are using ray start to start a Ray node, you can run:\nray "", ""start_index"": 5011, ""end_index"": 5486}]",datasets/raydocs_full/ray-core_scheduling_resources.txt
What is the default setting for logical CPU resources when starting Ray?,"[{""content"": ""been started on a node.\n\n\nNumber of logical CPUs (``num_cpus``): Set to the number of CPUs of the machine/container.\nNumber of logical GPUs (``num_gpus``): Set to the number of GPUs of the "", ""start_index"": 4355, ""end_index"": 4544}]",datasets/raydocs_full/ray-core_scheduling_resources.txt
How does the Ray autoscaler determine when to add nodes to the cluster?,"[{""content"": ""up and down based on resource demand.\nThe autoscaler does this by adjusting the number of nodes in the cluster based on the resources required by tasks, actors or placement groups.\nNote that the "", ""start_index"": 195, ""end_index"": 390}, {""content"": ""queued. The autoscaler adds nodes to satisfy resource demands in this queue.\nThe autoscaler also removes nodes after they become idle for some time.\nA node is considered idle if it has no active tasks, actors, or objects.\n\nTip\nWhen to use Autoscaling?\nAutoscaling can reduce workload costs, b"", ""start_index"": 675, ""end_index"": 967}]",datasets/raydocs_full/cluster_vms_user-guides_configuring-autoscaling.txt
What is the main purpose of the Ray autoscaler?,"[{""content"": ""cluster launcher.\nThe Ray autoscaler is a Ray cluster process that automatically scales a cluster up and down based on resource demand.\nThe autoscaler does this by adjusting the number of nodes in "", ""start_index"": 97, ""end_index"": 294}]",datasets/raydocs_full/cluster_vms_user-guides_configuring-autoscaling.txt
What are the conditions for AlgorithmConfig.is_multi_agent to return True?,"[{""content"": ""AlgorithmConfig.is_multi_agent: bool#\nReturns whether this config specifies a multi-agent setup.\n\nReturns:\nTrue, if a) >1 policies defined OR b) 1 policy defined, but its ID is NOT\nDEFAULT_POLICY_ID.\n\n\n\n\n"", ""start_index"": 83, ""end_index"": 360}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.txt
What does the component parameter control in the Algorithm.restore_from_path method?,"[{""content"": ""component arg is provided, path refers to a checkpoint of a\nsubcomponent of self, thus allowing the user to load only the subcomponent\u2019s\nstate into self without affecting any of the other state "", ""start_index"": 197, ""end_index"": 391}, {""content"": ""ns unchanged in this\ncase.\nfilesystem \u2013 PyArrow FileSystem to use to access data at the path. If not\nspecified, this is inferred from the URI scheme of path.\n**kwargs \u2013 Forward compatibility k"", ""start_index"": 1351, ""end_index"": 1543}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.txt
What is the function of the double method within the Actor class in Python?,"[{""content"": ""fixed pool of actors.\nimport ray\nfrom ray.util import ActorPool\n\n\n@ray.remote\nclass Actor:\n    def double(self, n):\n        return n * 2\n\n\na1, a2 = Actor.remote(), Actor.remote()\npool = "", ""start_index"": 184, ""end_index"": 370}]",datasets/raydocs_full/ray-core_actors_actor-utils.txt
How does Python's ActorPool manage task scheduling among actors?,"[{""content"": ""\n\nUtility Classes#\n\nActor Pool#\n\n\n\nPython\nThe ray.util module contains a utility class, ActorPool.\nThis class is similar to multiprocessing.Pool and lets you schedule Ray tasks over a fixed pool of actors.\nimport ray\nfrom ray.util import ActorPool\n\n\n@ray.remote\nclass Actor:\n    def "", ""start_index"": 0, ""end_index"": 283}]",datasets/raydocs_full/ray-core_actors_actor-utils.txt
What is the purpose of the reduce_type parameter in the convert_to_numpy function?,"[{""content"": ""numpy types.\nreduce_type \u2013 Whether to automatically reduce all float64 and int64 data\ninto float32 and int32 data, respectively.\n\n\nReturns:\nA new struct with the same structure as x, but with "", ""start_index"": 482, ""end_index"": 674}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.convert_to_numpy.txt
What types of data structures can be converted using the convert_to_numpy function?,"[{""content"": ""\n\nray.rllib.utils.numpy.convert_to_numpy#\n\n\nray.rllib.utils.numpy.convert_to_numpy(x: numpy.array | jnp.ndarray | tf.Tensor | torch.Tensor | dict | tuple, reduce_type: bool = True) \u2192 numpy.array | "", ""start_index"": 0, ""end_index"": 197}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.convert_to_numpy.txt
What is returned by the ray.rllib.utils.numpy.convert_to_numpy function?,"[{""content"": ""and int32 data, respectively.\n\n\nReturns:\nA new struct with the same structure as x, but with all\nvalues converted to numpy arrays (on CPU).\n\n\n\n\n"", ""start_index"": 581, ""end_index"": 774}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.convert_to_numpy.txt
What are the valid values for the deletion policy of a RayJob?,"[{""content"": ""RayJob.\ndeletionPolicy (Optional, alpha in v1.3.0): Indicates what resources of the RayJob are deleted upon job completion. Valid values are DeleteCluster, DeleteWorkers, DeleteSelf or "", ""start_index"": 5016, ""end_index"": 5201}]",datasets/raydocs_full/cluster_kubernetes_getting-started_rayjob-quick-start.txt
What happens when the environment variable DELETE_RAYJOB_CR_AFTER_JOB_FINISHES is set to true for the KubeRay operator?,"[{""content"": ""reason.\nDELETE_RAYJOB_CR_AFTER_JOB_FINISHES (Optional, added in version 1.2.0): Set this environment variable for the KubeRay operator, not the RayJob resource. If you set this environment variable to true, the RayJob custom resource itself is deleted if you also set shutdownAfterJobFinishes to true. Note that KubeRay deletes all resources created by the RayJob, "", ""start_index"": 4374, ""end_index"": 4739}]",datasets/raydocs_full/cluster_kubernetes_getting-started_rayjob-quick-start.txt
What is the impact of setting the parameter shutdownAfterJobFinishes to true in a RayJob configuration?,"[{""content"": ""tdown.yaml\n\n\n\n\nThe ray-job.shutdown.yaml defines a RayJob custom resource with shutdownAfterJobFinishes: true and ttlSecondsAfterFinished: 10.\nHence, the KubeRay operator deletes the RayCluster 10 seconds after the Ray job finishes. Note that the submitter job isn\u2019t "", ""start_index"": 9604, ""end_index"": 9871}]",datasets/raydocs_full/cluster_kubernetes_getting-started_rayjob-quick-start.txt
What are the major components listed under Ray Core API?,"[{""content"": ""\n\nRay Core API#\n\n\nCore API\nTasks\nActors\nObjects\nRuntime Context\nCross Language\n\n\nScheduling API\nScheduling Strategy\nPlacement Group\n\n\nRuntime Env API\n\n\nUtility\nCustom Metrics\nDebugging\n\n\nExceptions\n\n\nRay Core CLI\nDebugging applications\nUsage Stats\n\n\nState CLI\nState\nLog\n\n\nState API\nState Python SDK\nState APIs Schema\nState APIs Exceptions\n\n\n\n\n\n"", ""start_index"": 0, ""end_index"": 356}]",datasets/raydocs_full/ray-core_api_index.txt
What features are included under the Scheduling API of Ray Core?,"[{""content"": ""API\nScheduling Strategy\nPlacement Group\n\n\nRuntime Env API\n\n\nUtility\nCustom "", ""start_index"": 92, ""end_index"": 167}]",datasets/raydocs_full/ray-core_api_index.txt
What is the default storage unit for the ReservoirReplayBuffer in ray.rllib?,"[{""content"": ""ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer(capacity: int = 10000, storage_unit: str = 'timesteps', **kwargs)[source]#\nBases: ReplayBuffer\nThis buffer implements "", ""start_index"": 88, ""end_index"": 283}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.txt
What are some methods implemented by ReservoirReplayBuffer in ray.rllib?,"[{""content"": ""a reservoir\u201d.\nMethods\n\n\n__init__\nInitializes a ReservoirBuffer instance.\n\nadd\nAdds a batch of experiences or other data to this buffer.\n\napply\nCalls the given function with this Actor instance.\n\nget_host\nReturns the computer's network name.\n\nget_state\nReturns all local state.\n\nping\nPing the actor.\n\nsample\nSamples num_items items from this buffer.\n\nset_state\nRestores all local state to the provided state.\n\nstats\nReturns the stats of this buffer.\n\n\n\n\n\n"", ""start_index"": 382, ""end_index"": 851}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.txt
What types of data can Result object components report after training?,"[{""content"": ""object contains, among other information:\n\nThe last reported checkpoint (to load the model) and its attached metrics\nError messages, if any errors occurred\n\n\nViewing metrics#\nYou can retrieve "", ""start_index"": 96, ""end_index"": 288}, {""content"": ""rted metrics that were attached to a checkpoint from the Result object.\nCommon metrics inclu"", ""start_index"": 292, ""end_index"": 384}]",datasets/raydocs_full/train_user-guides_results.txt
How can one access the storage location of the training run using the Result object?,"[{""content"": ""checkpointing.\n\n\n\n\nAccessing storage location#\nIf you need to retrieve the results later, you can get the storage location\nof the training run with Result.path.\nThis path will correspond to the storage_path you configured\nin the RunConfig. It will be a\n(nested) subdirectory within that path, usually\nof the form TrainerName_date-string/TrainerName_id_00000_0_....\nThe result also contains a pyarrow.fs.FileSystem that can be used to\naccess the storage location, which is useful if the path is on cloud storage.\nresult_path: str = result.path\nresult_filesystem: pyarrow.fs.FileSystem = result.filesystem\n\nprint(f\""Results location (fs, path) = ({result_filesystem}, "", ""start_index"": 2760, ""end_index"": 3425}]",datasets/raydocs_full/train_user-guides_results.txt
What are the key features of Ray as a machine learning platform?,"[{""content"": ""oduction. With Ray and its libraries, the same Python code scales seamlessly from a laptop to a large cluster.\n2. Unified ML API and Runtime: Ray\u2019s APIs enables swapping between popular "", ""start_index"": 767, ""end_index"": 953}, {""content"": ""large cluster.\n2. Unified ML API and Runtime: Ray\u2019s APIs enables swapping between popular frameworks, such as XGBoost, PyTorch, and Hugging Face, with minimal code changes. Everything from training to serving runs on a single runtime (Ray + KubeRay).\n3. Open and Extensible: Ray is fully "", ""start_index"": 863, ""end_index"": 1151}, {""content"": ""ly open-source and can run on any cluster, cloud, or Kubernetes. Build custom components and integrations on top of scalable developer APIs.\n\n\nExample ML Platforms built on Ray#\nMerlin is S"", ""start_index"": 1148, ""end_index"": 1337}]",datasets/raydocs_full/ray-air_getting-started.txt
What are the three areas where Ray simplifies the machine learning ecosystem?,"[{""content"": ""for ML Infrastructure?#\nRay\u2019s AI libraries simplify the ecosystem of machine learning frameworks, platforms, and tools, by providing a seamless, unified, and open experience for scalable ML:\n1. "", ""start_index"": 487, ""end_index"": 681}]",datasets/raydocs_full/ray-air_getting-started.txt
"What are the unique features of Ray Train, Ray Data, and Ray Serve?","[{""content"": ""teams looking to simplify their ML platform.\nRay\u2019s libraries such as Ray Train, Ray Data, and Ray Serve can be used to compose end-to-end ML workflows, providing features and APIs for\ndata preprocessing as part of training, and transitioning from training to serving.\n\nWhy Ray for ML "", ""start_index"": 210, ""end_index"": 494}]",datasets/raydocs_full/ray-air_getting-started.txt
What Kubernetes versions are required to utilize KubeRay operator for serving a MobileNet image classifier?,"[{""content"": ""the Helm repository.\nNote that the YAML file in this example uses serveConfigV2. You need KubeRay version v0.6.0 or later to use this feature.\n\n\nStep 3: Install a RayService#\n# Create a "", ""start_index"": 386, ""end_index"": 572}]",datasets/raydocs_full/cluster_kubernetes_examples_mobilenet-rayservice.txt
What are the specific types of output batches that the 'map_batches' function can handle in Ray Data?,"[{""content"": "".map_batches(increase_brightness)\n)\n\n\n\nConfiguring batch format#\nRay Data represents batches as dicts of NumPy ndarrays or pandas DataFrames. By\ndefault, Ray Data represents batches as dicts of "", ""start_index"": 2929, ""end_index"": 3123}, {""content"": ""ct[str, np.ndarray]]:\n    # yield the same batch multiple times\n    for _ in range(10):\n        yield batch\n\n\n\n\nConfiguring batch size#\nIncreasing batch_size improves the performance of vectorized transformations like\nNumPy functions and model inference. However, if your batch size is too large, your\nprogram might run out of memory. If you encounter an out-of-memory error, decrease your\nba"", ""start_index"": 4950, ""end_index"": 5342}]",datasets/raydocs_full/data_transforming-data.txt
How does the batch size impact system performance in Ray Data processing?,"[{""content"": ""batch_size improves the performance of vectorized transformations like\nNumPy functions and model inference. However, if your batch size is too large, your\nprogram might run out of memory. If you encounter an out-of-memory error, decrease your\nbatch_size.\n\n\n\nOrdering of rows#\nWhen transforming "", ""start_index"": 5097, ""end_index"": 5391}]",datasets/raydocs_full/data_transforming-data.txt
How can one make the execution of transforms in Ray Data preserve the order of data rows?,"[{""content"": ""by default.\nIf the order of blocks needs to be preserved/deterministic,\nyou can use sort() method, or set ray.data.ExecutionOptions.preserve_order to True.\nNote that setting this flag may negatively impact performance on larger cluster setups where stragglers are more likely.\nimport ray\n\nctx = "", ""start_index"": 5433, ""end_index"": 5728}]",datasets/raydocs_full/data_transforming-data.txt
How can you make the Ray Dashboard accessible via a reverse proxy?,"[{""content"": ""notes.\n\n\n\n\nRunning behind a reverse proxy#\nRay Dashboard should work out-of-the-box when accessed via a reverse proxy. API requests don\u2019t need to be proxied individually.\nAlways access the dashboard with a trailing / at the end of the URL.\nFor example, if your proxy is set up to handle requests to /ray/dashboard, view the dashboard at www.my-website.com/ray/dashboard/.\nThe dashboard sends HTTP requests with relative URL paths. Browsers handle these requests as expected when the window.location.href ends in a trailing /.\nThis is a peculiarity of how many browsers handle requests with relative URLs, despite what MDN defines as the expected behavior.\nMake your dashboard visible without a trailing / by including a rule in your reverse proxy that redirects the user\u2019s browser to /, i.e. /ray/dashboard \u2013> /ray/dashboard/.\nBelow is an example with a traefik TOML file that accomplishes this:\n[http]\n  [http.routers]\n    [http.routers.to-dashboard]\n      rule = \""PathPrefix(`/ray/dashboard`)\""\n      middlewares = [\""test-redirectregex\"", \""strip\""]\n      service = "", ""start_index"": 3407, ""end_index"": 4471}]",datasets/raydocs_full/cluster_configure-manage-dashboard.txt
What are the methods mentioned for accessing the Ray Dashboard from outside a Kubernetes cluster?,"[{""content"": ""Port forwarding \nYou can also view the dashboard from outside the Kubernetes cluster by using port-forwarding:\n$ kubectl port-forward service/${RAYCLUSTER_NAME}-head-svc 8265:8265\n# Visit "", ""start_index"": 2904, ""end_index"": 3092}, {""content"": ""d with Ingress.\n\nFor more information about configuring network access to a Ray cluster on Kubernetes, see the networking notes.\n\n\n\n\nRunning behind a reverse proxy#\nRay Dashboard should w"", ""start_index"": 3285, ""end_index"": 3472}]",datasets/raydocs_full/cluster_configure-manage-dashboard.txt
What are the default ports for Prometheus and Grafana as assumed by the Ray Dashboard?,"[{""content"": ""up Grafana.\n\n\nAlternate Prometheus host location#\nBy default, Ray Dashboard assumes Prometheus is hosted at localhost:9090. You can choose to run Prometheus on a non-default port or on a different "", ""start_index"": 8590, ""end_index"": 8787}, {""content"": ""lth)and visit it.\ncheck Head Node connection to Grafana server: add api/grafana_health to the end of Ray Dashboard URL (for example: http://127.0.0.1:8265/api/grafana_health) and visit it.\ncheck b"", ""start_index"": 10760, ""end_index"": 10956}]",datasets/raydocs_full/cluster_configure-manage-dashboard.txt
What are some resources for learning more about Ray Serve?,"[{""content"": ""patterns with Ray Serve. (Click image to enlarge.)#\n\n\nLearn more about model serving with the following resources.\n\n[Talk] Productionizing ML at Scale with Ray Serve\n[Blog] Simplify your MLOps with Ray & Ray Serve\n[Guide] Getting Started with Ray Serve\n[Guide] Model Composition in Serve\n[Gallery] Serve Examples Gallery\n[Gallery] More Serve Use Cases on the Blog\n\n\n\nHyperparameter "", ""start_index"": 1727, ""end_index"": 2109}]",datasets/raydocs_full/ray-overview_use-cases.txt
How does setting resources in tune.with_resources impact trial concurrency with specific CPU allocations?,"[{""content"": ""tune_config=tune.TuneConfig(num_samples=10)\n)\nresults = tuner.fit()\n\n# Fractional values are also supported, (i.e., {\""cpu\"": 0.5}).\n# If you have 4 CPUs on your machine, this will run 8 concurrent trials at a time.\ntrainable_with_resources = tune.with_resources(trainable, {\""cpu\"": 0.5})\ntuner = "", ""start_index"": 1229, ""end_index"": 1523}]",datasets/raydocs_full/tune_tutorials_tune-resources.txt
How does Ray Tune handle trials if there are not enough resources available immediately?,"[{""content"": ""specified GPU and CPU as specified by tune.with_resources to each individual trial.\nEven if the trial cannot be scheduled right now, Ray Tune will still try to start the respective placement group. If not enough resources are available, this will trigger\nautoscaling behavior if you\u2019re "", ""start_index"": 2075, ""end_index"": 2361}]",datasets/raydocs_full/tune_tutorials_tune-resources.txt
How does fractional CPU allocation affect the number of concurrent trials in Ray Tune?,"[{""content"": ""supported, (i.e., {\""cpu\"": 0.5}).\n# If you have 4 CPUs on your machine, this will run 8 concurrent trials at a time.\ntrainable_with_resources = tune.with_resources(trainable, {\""cpu\"": 0.5})\ntuner = "", ""start_index"": 1327, ""end_index"": 1523}]",datasets/raydocs_full/tune_tutorials_tune-resources.txt
What can you do with the 'result' parameter in the RLlibCallback.on_train_result method?,"[{""content"": ""metrics after traing results are available.\nresult \u2013 Dict of results returned from Algorithm.train() call.\nYou can mutate this object to add additional metrics.\nkwargs \u2013 Forward "", ""start_index"": 389, ""end_index"": 567}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result.txt
What are the necessary steps to recreate an entire Algorithm instance from a checkpoint?,"[{""content"": ""examples:\n\n\n\nCreate a new Algorithm from a checkpoint\nTo recreate an entire Algorithm\ninstance from a checkpoint, you can do the following:\n# Import the correct class to create from scratch using the checkpoint.\nfrom ray.rllib.algorithms.algorithm import Algorithm\n\n# Use the already existing checkpoint in `checkpoint_dir`.\nnew_ppo = Algorithm.from_checkpoint(checkpoint_dir)\n# Confirm the `new_ppo` matches the originally checkpointed one.\nassert new_ppo.config.env == \""Pendulum-v1\""\n\n# Continue training.\nnew_ppo.train()\n\n\n\n\n\nCreate a new RLModule from an Algorithm checkpoint\nCreating "", ""start_index"": 9100, ""end_index"": 9688}]",datasets/raydocs_full/rllib_checkpoints.txt
What information is stored in the class_and_ctor_args.pkl file used by RLlib?,"[{""content"": ""    \""ray_commit\"": ..,\n    \""checkpoint_version\"": \""2.1\""\n}\n\n\n\nThe class_and_ctor_args.pkl file stores meta information needed to construct a \u201cfresh\u201d object, without any particular state.\nThis "", ""start_index"": 6616, ""end_index"": 6805}, {""content"": ""information, as the filename suggests, contains the class of the saved object and its constructor arguments and keyword arguments.\nRLlib uses this file to create the initial new object when calling "", ""start_index"": 6805, ""end_index"": 7003}]",datasets/raydocs_full/rllib_checkpoints.txt
What file format does RLlib use to store the state information of saved objects?,"[{""content"": ""initial new object when calling from_checkpoint().\nFinally, the .._state.[pkl|msgpack] file contains the pickled or msgpacked state dict of the saved object.\nRLlib obtains this state dict, "", ""start_index"": 6971, ""end_index"": 7160}]",datasets/raydocs_full/rllib_checkpoints.txt
What are the types of scheduling strategies available in Ray?,"[{""content"": ""\n\nScheduling API#\n\nScheduling Strategy#\n\n\nray.util.scheduling_strategies.PlacementGroupSchedulingStrategy\nPlacement group based scheduling strategy.\n\nray.util.scheduling_strategies.NodeAffinitySchedulingStrategy\nStatic "", ""start_index"": 0, ""end_index"": 219}, {""content"": ""scheduling strategy.\n\nray.util.scheduling_strategies.NodeAffinitySchedulingStrategy\nStatic scheduling strategy used to run a task or actor on a particular node.\n\n\n\n\n\nPlacement "", ""start_index"": 128, ""end_index"": 304}]",datasets/raydocs_full/ray-core_api_scheduling.txt
What are the two methods of defining a trainable in Ray Tune?,"[{""content"": ""Trainables#\nIn short, a Trainable is an object that you can pass into a Tune run.\nRay Tune has two ways of defining a trainable, namely the Function API\nand the Class API.\nBoth are valid ways of "", ""start_index"": 902, ""end_index"": 1097}]",datasets/raydocs_full/tune_key-concepts.txt
What is the default scheduler used by Tune if none is specified?,"[{""content"": ""whether to stop the trial early or not.\nIf you don\u2019t specify a scheduler, Tune will use a first-in-first-out (FIFO) scheduler by default, which simply\npasses through the trials selected by your search algorithm in the order they were picked and does not perform any early stopping.\nIn "", ""start_index"": 11031, ""end_index"": 11316}]",datasets/raydocs_full/tune_key-concepts.txt
What is the purpose of the ReplayBuffer.set_state method?,"[{""content"": ""\n\nray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state#\n\n\nReplayBuffer.set_state(state: Dict[str, Any]) \u2192 None[source]#\nRestores all local state to the provided "", ""start_index"": 0, ""end_index"": 176}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.txt
What are the stability implications of using the ReplayBuffer.set_state API in Ray?,"[{""content"": ""calling\nself.get_state().\n\n\nDeveloperAPI: This API may change across minor Ray releases.\n\n\n"", ""start_index"": 257, ""end_index"": 357}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.txt
How is a Python function parallelized across a Ray cluster?,"[{""content"": ""the first use of a Ray remote API.\n\n\n\nRunning a Task#\nTasks are the simplest way to parallelize your Python functions across a Ray cluster. To create a task:\n\nDecorate your function with @ray.remote to indicate it should run remotely\nCall the function with .remote() instead of a normal function call\nUse ray.get() to retrieve the result from the returned future (Ray object reference)\n\nHere\u2019s a simple example:\n# Define the square task.\n@ray.remote\ndef square(x):\n    "", ""start_index"": 904, ""end_index"": 1373}]",datasets/raydocs_full/ray-core_walkthrough.txt
What are the steps to initialize Ray in a Python application?,"[{""content"": ""using multiple GPUs.\nSee Ray Compiled Graph for more details.\n\n\nGetting Started#\nTo get started, install Ray using pip install -U ray. For additional installation options, see Installing Ray.\nThe first step is to import and initialize Ray:\nimport ray\n\nray.init()\n\n\n\nNote\nIn recent versions of "", ""start_index"": 560, ""end_index"": 853}, {""content"": ""e the result from the returned future (Ray object reference)\n\nHere\u2019s a simple example:\n# Define the square task.\n@ray.remote\ndef square(x):\n    return x * x\n\n# Launch four parallel square tasks.\nf"", ""start_index"": 1229, ""end_index"": 1425}]",datasets/raydocs_full/ray-core_walkthrough.txt
What are some specific examples of tasks and actors in Ray?,"[{""content"": ""reference)\n\nHere\u2019s a simple example:\n# Define the square task.\n@ray.remote\ndef square(x):\n    return x * x\n\n# Launch four parallel square tasks.\nfutures = [square.remote(i) for i in range(4)]\n\n# Retrieve results.\nprint(ray.get(futures))\n# -> [0, 1, 4, 9]\n\n\n\n\nCalling an "", ""start_index"": 1279, ""end_index"": 1549}, {""content"": ""tion.\n\nHere\u2019s an example showing these techniques:\nimport numpy as np\n\n# Define a task that sums the values in a matrix.\n@ray.remote\ndef sum_matrix(matrix):\n    return np.sum(matrix)\n\n# Call the task with a literal argument value.\nprint(ray.get(sum_matrix.remote(np.ones((100, 100)))))\n# -> 10000.0\n\n# Put a large array into the object store.\nmatrix_ref = ray.put(np.ones((1000, 1000)))\n\n# Call the task with the object reference as an argument.\nprint(ray.get(sum_matrix.remote(matrix_ref)))\n# -> 1000000.0\n\n\n\n\nNext Steps#\n\nTip\nTo monitor your application\u2019s performance and resource usage"", ""start_index"": 3174, ""end_index"": 3762}]",datasets/raydocs_full/ray-core_walkthrough.txt
What happens when policies are no longer in the list or the callable returns False during a remove_policy operation?,"[{""content"": ""episode.\npolicies_to_train \u2013 An optional list of policy IDs to be trained\nor a callable taking PolicyID and SampleBatchType and\nreturning a bool (trainable or not?).\nIf None, will keep the existing setup in place. Policies,\nwhose IDs are not in the list (or for which the callable\nreturns "", ""start_index"": 748, ""end_index"": 1037}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.remove_policy.txt
What are the default values for parameters in the Algorithm.remove_policy method?,"[{""content"": ""\n\nray.rllib.algorithms.algorithm.Algorithm.remove_policy#\n\n\nAlgorithm.remove_policy(policy_id: str = 'default_policy', *, policy_mapping_fn: Callable[[Any], str] | None = None, policies_to_train: Collection[str] | Callable[[str, SampleBatch | MultiAgentBatch | Dict[str, Any] | None], bool] | None = None, remove_from_env_runners: bool = True, remove_from_eval_env_runners: bool = True, evaluation_workers=-1, remove_from_learners=-1) \u2192 None[source]#\nRemoves a policy from this "", ""start_index"": 0, ""end_index"": 478}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.remove_policy.txt
What are the issues associated with using global variables in multiple processes as illustrated in the provided coding example?,"[{""content"": ""places where the state needs to be modified or accessed.\nNote that using class variables to manage state between instances of the same class is not supported.\nEach actor instance is instantiated in its own process, so each actor will have its own copy of the class variables.\n\nCode "", ""start_index"": 584, ""end_index"": 866}, {""content"": ""r):\n        self.global_var_actor = global_var_actor\n\n    def f(self):\n        return ray.get(self.global_var_actor.get_global_var.remote()) + 3\n\n\nglobal_var_actor = GlobalVarActor.remote()\nactor = Actor.remote(global_var_actor)\nray.get(global_var_actor.set_global_var.remote(4))\n# Th"", ""start_index"": 1547, ""end_index"": 1831}]",datasets/raydocs_full/ray-core_patterns_global-variables.txt
"How do Ray drivers, tasks, and actors operate concerning process allocation and address spaces?","[{""content"": ""an actor and pass the actor handle to other tasks and actors.\nRay drivers, tasks and actors are running in\ndifferent processes, so they don\u2019t share the same address space.\nThis means that if you "", ""start_index"": 196, ""end_index"": 391}]",datasets/raydocs_full/ray-core_patterns_global-variables.txt
What are the two ways to start a Mars on Ray runtime as described in the text?,"[{""content"": ""a Ray cluster.\nStarting a new Mars on Ray runtime locally via:\nimport ray\nray.init()\nimport mars\nmars.new_ray_session()\nimport mars.tensor as mt\nmt.random.RandomState(0).rand(1000_0000, 5).sum().execute()\n\n\nOr connecting to a Mars on Ray runtime which is already initialized:\nimport "", ""start_index"": 678, ""end_index"": 961}, {""content"": ""ct.readthedocs.io/en/latest/installation/ray.html#mars-ray for more information.\n\n\n"", ""start_index"": 1542, ""end_index"": 1732}]",datasets/raydocs_full/ray-more-libs_mars-on-ray.txt
How can a Ray Dataset be converted into a Mars DataFrame?,"[{""content"": ""dataset\nimport ray\n# ds = md.to_ray_dataset(df)\nds = ray.data.from_mars(df)\nprint(ds.schema(), ds.count())\nds.filter(lambda row: row[\""a\""] > 0.5).show(5)\n# Convert ray dataset to mars dataframe\n# df2 = md.read_ray_dataset(ds)\ndf2 = ds.to_mars()\nprint(df2.head(5).execute())\n\n\nRefer to Mars on "", ""start_index"": 1227, ""end_index"": 1519}]",datasets/raydocs_full/ray-more-libs_mars-on-ray.txt
What resources were requested during the scheduling process described in the text?,"[{""content"": ""GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.5 GiB heap, "", ""start_index"": 2051, ""end_index"": 2146}]",datasets/raydocs_full/tune_examples_tune-comet.txt
"What are the results logged for train_function_5bf98_00000 including its hostname, process ID (pid), node IP, trial ID?","[{""content"": ""hostname: Kais-MacBook-Pro.local\n  iterations_since_restore: 1\n  loss: 1.1009860426725162\n  node_ip: 127.0.0.1\n  pid: 48140\n  time_since_restore: 0.000125885009765625\n  time_this_iter_s: 0.000125885009765625\n  time_total_s: 0.000125885009765625\n  timestamp: 1658500887\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: 5bf98_00000\n  warmup_time: "", ""start_index"": 4371, ""end_index"": 4732}]",datasets/raydocs_full/tune_examples_tune-comet.txt
What is the training iteration number for the experiment trial with trial_id a052a214?,"[{""content"": ""timesteps_since_restore: 0\n  timesteps_total: 0\n  training_iteration: 4\n  trial_id: a052a214\n  "", ""start_index"": 21085, ""end_index"": 21180}]",datasets/raydocs_full/tune_examples_bohb_example.txt
What are the set hyperparameters that minimized the mean loss for the objective previously defined?,"[{""content"": ""results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the "", ""start_index"": 28465, ""end_index"": 28660}]",datasets/raydocs_full/tune_examples_bohb_example.txt
What is the mean loss for the experiment conducted on 2022-07-22 for the trial_id identified as 2397442c?,"[{""content"": ""objective_2397442c:\n  date: 2022-07-22_15-11-27\n  done: false\n  episodes_total: 0\n  experiment_id: 1a4ebf62df50443492dc6df792fcb67a\n  hostname: Kais-MacBook-Pro.local\n  iterations: 0\n  iterations_since_restore: 1\n  mean_loss: 14.284216630043918\n  neg_mean_loss: -14.284216630043918\n  node_ip: 127.0.0.1\n  pid: 45401\n  time_since_restore: 0.1044008731842041\n  time_this_iter_s: 0.1044008731842041\n  time_total_s: 0.6395547389984131\n  timestamp: 1658499087\n  timesteps_since_restore: 0\n  timesteps_total: 0\n  training_iteration: 1\n  trial_id: 2397442c\n  "", ""start_index"": 50334, ""end_index"": 50886}]",datasets/raydocs_full/tune_examples_bohb_example.txt
What are the stability guarantees for v1 APIs in the KubeRay project?,"[{""content"": ""API reference.\n\nKubeRay API compatibility and guarantees#\nv1 APIs in the KubeRay project are stable and suitable for production environments.\nFields in the v1 APIs will never be removed to maintain "", ""start_index"": 189, ""end_index"": 387}, {""content"": ""lds removed from v1.\nHowever, KubeRay maintainers preserve the right to mark fields as deprecated and remove\nfunctionality associated with deprecated fields after a minimum of two minor release"", ""start_index"": 478, ""end_index"": 671}]",datasets/raydocs_full/cluster_kubernetes_references.txt
What steps can be taken to better understand RayCluster configuration?,"[{""content"": ""\n\nAPI Reference#\nTo learn about RayCluster configuration, we recommend taking a look at\nthe configuration guide.\nFor comprehensive coverage of all supported RayCluster fields,\nrefer to the "", ""start_index"": 0, ""end_index"": 189}, {""content"": ""configuration guide.\nFor comprehensive coverage of all supported RayCluster fields,\nrefer to the API reference.\n\nKubeRay API compatibility and guarantees#\nv1 APIs in the KubeRay project are stable "", ""start_index"": 92, ""end_index"": 289}]",datasets/raydocs_full/cluster_kubernetes_references.txt
What is the dtype and shape of the 'image' column in the dataset schema after using read_tfrecords()?,"[{""content"": ""  ray.data.read_tfrecords(\n        \""s3://anonymous@air-example-data/cifar-10/tfrecords\""\n    )\n    .map(decode_bytes)\n)\n\nprint(ds.schema())\n\n\nColumn  Type\n------  ----\nimage   "", ""start_index"": 1739, ""end_index"": 1914}]",datasets/raydocs_full/data_working-with-images.txt
What are the common properties of images in the CIFAR-10 dataset as used in the examples read using ray.data.read_parquet()?,"[{""content"": ""in Parquet files, call ray.data.read_parquet().\nimport ray\n\nds = ray.data.read_parquet(\""s3://anonymous@air-example-data/cifar-10/parquet\"")\n\nprint(ds.schema())\n\n\nColumn  Type\n------  ----\nimage   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\nlabel   int64\n\n\n\n\nFor "", ""start_index"": 2013, ""end_index"": 2276}]",datasets/raydocs_full/data_working-with-images.txt
What happens if a user-defined schema is passed into input_read_schema?,"[{""content"": ""'unroll_id': 'unroll_id'}#\nThis is the default schema used if no input_read_schema is set in\nthe config. If a user passes in a schema into input_read_schema\nthis user-defined schema has to comply with the keys of SCHEMA,\nwhile values correspond to the columns in the user\u2019s dataset. Note\nthat only the user-defined values will be overridden while all\nother values from SCHEMA remain as defined here.\n\n\n"", ""start_index"": 380, ""end_index"": 866}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_prelearner.SCHEMA.txt
What entries are included in the default SCHEMA for ray.rllib.offline.offline_prelearner?,"[{""content"": ""\n\nray.rllib.offline.offline_prelearner.SCHEMA#\n\n\nray.rllib.offline.offline_prelearner.SCHEMA = {'actions': 'actions', 'agent_id': 'agent_id', 'agent_index': 'agent_index', 'dones': 'dones', 'eps_id': 'eps_id', 'infos': 'infos', 'module_id': 'module_id', 'new_obs': 'new_obs', 'obs': 'obs', 'rewards': 'rewards', 't': 't', 'terminateds': 'terminateds', 'truncateds': 'truncateds', 'unroll_id': 'unroll_id'}#\nThis is the default schema used if no input_read_schema is set in\nthe "", ""start_index"": 0, ""end_index"": 477}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_prelearner.SCHEMA.txt
What types of colors are specified in the Color model?,"[{""content"": "" Color(BaseModel):\n    colors: List[Literal[\""cyan\"", \""magenta\"", \""yellow\""]]\n\n# Request structured "", ""start_index"": 11913, ""end_index"": 12009}]",datasets/raydocs_full/serve_llm_serving-llms.txt
What are the minimum and maximum replicas specified for the Qwen model deployment?,"[{""content"": ""autoscaling_config=dict(\n            min_replicas=1,\n            max_replicas=2,\n        )\n    ),\n  "", ""start_index"": 10510, ""end_index"": 10610}]",datasets/raydocs_full/serve_llm_serving-llms.txt
How can model weights be stored using remote storage options like S3 or GCS?,"[{""content"": ""flush=True)\n\n\n\n\n\n\nUsing remote storage for model weights#\nYou can use remote storage (S3 and GCS) to store your model weights instead of\ndownloading them from Hugging Face.\nFor example, if you have "", ""start_index"": 14250, ""end_index"": 14448}]",datasets/raydocs_full/serve_llm_serving-llms.txt
What are the fault tolerance features available for Ray generator tasks?,"[{""content"": ""tolerance#\nFault tolerance features work with\nRay generator tasks and actor tasks. For example;\n\nTask fault tolerance features: max_retries, retry_exceptions\nActor fault tolerance features: max_restarts, max_task_retries\nObject fault tolerance features: object reconstruction\n\n\n\nCancellation#\nThe ray.cancel() function works with both Ray generator tasks and "", ""start_index"": 6244, ""end_index"": 6603}]",datasets/raydocs_full/ray-core_ray-generator.txt
How can the Ray generator be utilized with asyncio?,"[{""content"": ""print(ray.get(ref))\n\n\n\n\n\nUsing the Ray generator with asyncio#\nThe returned ObjectRefGenerator is also compatible with asyncio. You can\nuse __anext__ or async for loops.\nimport "", ""start_index"": 5165, ""end_index"": 5342}, {""content"": ""s the next object reference immediately without blocking. See the example below for more details.\n@ray.remote\ndef task():\n    for i in range(5):\n        time.sleep(5)\n        yield i\n\ngen = task.remote()\n\n# Because it takes 5 seconds to make the first yield,\n# with 0 timeout, the generato"", ""start_index"": 8211, ""end_index"": 8500}]",datasets/raydocs_full/ray-core_ray-generator.txt
What signals can be used to gracefully terminate a Ray Tune training session?,"[{""content"": ""could result\nin resuming with stale state.\n\nRay Tune also accepts the SIGUSR1 signal to interrupt training gracefully. This\nshould be used when running Ray Tune in a remote Ray task\nas Ray will "", ""start_index"": 1592, ""end_index"": 1786}, {""content"": ""mplest way is to use metric-based criteria. These are a fixed set of thresholds that determine when the experiment should stop.\nYou can implement the stopping criteria using either "", ""start_index"": 1976, ""end_index"": 2157}]",datasets/raydocs_full/tune_tutorials_tune-stopping.txt
How can stopping criteria be implemented using a dictionary in Ray Tune?,"[{""content"": ""stopping criteria using either a dictionary, a function, or a custom Stopper.\n\n\n\nDictionary\nIf a dictionary is passed in, the keys may be any field in the return result of session.report in the\nFunction API or step() in the Class API.\n\nNote\nThis includes auto-filled metrics such as training_iteration.\n\nIn the example below, each trial will be stopped either when it completes 10 iterations or when it\nreaches a mean accuracy of 0.8 or more.\nThese metrics are assumed to be increasing, so the trial will stop once the reported metric has exceeded the threshold specified in the dictionary.\nfrom ray import tune\n\ntuner = tune.Tuner(\n    my_trainable,\n    "", ""start_index"": 2126, ""end_index"": 2781}]",datasets/raydocs_full/tune_tutorials_tune-stopping.txt
What conditions determine when a trial should be stopped in the provided Ray Tune example?,"[{""content"": ""trial should be stopped and False otherwise).\nIn the example below, each trial will be stopped either when it completes 10 iterations or when it\nreaches a mean accuracy of 0.8 or more.\nfrom ray "", ""start_index"": 3090, ""end_index"": 3284}]",datasets/raydocs_full/tune_tutorials_tune-stopping.txt
What does the function ray.rllib.utils.framework.try_import_torch return if torch is successfully imported?,"[{""content"": ""Whether to raise an error if torch cannot be imported.\n\nReturns:\nTuple consisting of the torch- AND torch.nn modules.\n\nRaises:\nImportError \u2013 If error=True and PyTorch is not installed.\n\n\n\n\n"", ""start_index"": 198, ""end_index"": 398}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.framework.try_import_torch.txt
Under which condition does the ray.rllib.utils.framework.try_import_torch function raise an ImportError?,"[{""content"": ""\n\nray.rllib.utils.framework.try_import_torch#\n\n\nray.rllib.utils.framework.try_import_torch(error: bool = False)[source]#\nTries importing torch and returns the module (or None).\n\nParameters:\nerror \u2013 Whether to raise an error if torch cannot be imported.\n\nReturns:\nTuple consisting of the torch- AND torch.nn modules.\n\nRaises:\nImportError \u2013 If error=True and PyTorch is not installed.\n\n\n\n\n"", ""start_index"": 0, ""end_index"": 398}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.framework.try_import_torch.txt
What parameter controls whether an error is raised in the ray.rllib.utils.framework.try_import_torch function if torch cannot be imported?,"[{""content"": ""bool = False)[source]#\nTries importing torch and returns the module (or None).\n\nParameters:\nerror \u2013 Whether to raise an error if torch cannot be imported.\n\nReturns:\nTuple consisting of the torch- AND "", ""start_index"": 98, ""end_index"": 298}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.framework.try_import_torch.txt
Where is the model_config referenced within the RLModule?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModule.model_config#\n\n\nRLModule.model_config#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule.model_config.txt
What information can be found regarding the RLModule model configuration?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModule.model_config#\n\n\nRLModule.model_config#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule.model_config.txt
Which module does the function concat_aligned belong to?,"[{""content"": ""\n\nray.rllib.utils.numpy.concat_aligned#\n\n\nray.rllib.utils.numpy.concat_aligned(*args, **kwargs)#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.concat_aligned.txt
What is the primary function of the Repeater in hyperparameter tuning?,"[{""content"": ""suggestions.\n\n\n\n\n\nRepeated Evaluations (tune.search.Repeater)#\nUse ray.tune.search.Repeater to average over multiple evaluations of the same\nhyperparameter configurations. This is useful in cases where the evaluated\ntraining procedure has high variance (i.e., in reinforcement "", ""start_index"": 4227, ""end_index"": 4504}]",datasets/raydocs_full/tune_api_suggestion.txt
What are the functionalities of the ConcurrencyLimiter in hyperparameter tuning?,"[{""content"": ""of same parameters.\n\n\n\n\n\nConcurrencyLimiter (tune.search.ConcurrencyLimiter)#\nUse ray.tune.search.ConcurrencyLimiter to limit the amount of concurrency when using a search algorithm.\nThis is useful when a given optimization algorithm does not parallelize very well (like "", ""start_index"": 4985, ""end_index"": 5256}]",datasets/raydocs_full/tune_api_suggestion.txt
How do DeploymentHandles manage multiple stages of a pipeline in Ray Serve?,"[{""content"": ""DeploymentHandle calls#\nRay Serve can directly pass the DeploymentResponse object that a DeploymentHandle returns, to another DeploymentHandle call to chain together multiple stages of a pipeline.\nYou don\u2019t need to await the first response, Ray Serve\nmanages the await behavior under the hood. When the first call finishes, Ray Serve passes the output of the first call, instead of the DeploymentResponse object, directly to the second call.\nFor example, the code sample below "", ""start_index"": 5642, ""end_index"": 6119}]",datasets/raydocs_full/serve_model_composition.txt
How is the status summary printed by Tune described?,"[{""content"": ""results. They will look something like this. Tune periodically prints a status summary to stdout "", ""start_index"": 3078, ""end_index"": 3175}]",datasets/raydocs_full/tune_tutorials_tune-run.txt
How does Ray Tune manage the parallelism based on resource availability?,"[{""content"": ""range(NUM_MODELS)\n    ])\n}\n\n\nStep 3: Optionally, configure the resources allocated per trial. Tune uses this resources allocation to control the parallelism. For example, if each trial was configured to use 4 CPUs, and the cluster had only 32 CPUs, then Tune will limit the number of concurrent trials to 8 to avoid overloading the cluster. For more information, see A Guide To "", ""start_index"": 2037, ""end_index"": 2415}]",datasets/raydocs_full/tune_tutorials_tune-run.txt
What are the functions of the ray.runtime_env.RuntimeEnv and ray.runtime_env.RuntimeEnvConfig classes?,"[{""content"": ""\n\nRuntime Env API#\n\n\nray.runtime_env.RuntimeEnvConfig\nUsed to specify configuration options for a runtime environment.\n\nray.runtime_env.RuntimeEnv\nThis class is used to define a runtime environment "", ""start_index"": 0, ""end_index"": 198}, {""content"": ""runtime environment.\n\nray.runtime_env.RuntimeEnv\nThis class is used to define a runtime environment for a job, task, or actor.\n\n\n\n\n"", ""start_index"": 98, ""end_index"": 298}]",datasets/raydocs_full/ray-core_api_runtime-env.txt
What is the procedure for handling interruptions in the Ray Train job driver process?,"[{""content"": ""training.#\n\n\n\n\n\nJob Driver Fault Tolerance#\nJob driver fault tolerance is to handle cases where the Ray Train driver process is interrupted.\nThe Ray Train driver process is the process that calls trainer.fit() and is usually located on the head node of the cluster.\nThe driver process may be interrupted due to one of the following reasons:\n\nThe run is manually interrupted by a user (e.g., Ctrl+C).\nThe node where the driver process is running (head node) crashes (e.g., out of memory, out of disk).\nThe entire cluster goes down (e.g., network error affecting all nodes).\n\nIn these cases, "", ""start_index"": 5171, ""end_index"": 5761}, {""content"": ""e located at the storage path.\nRay Train fetches the latest checkpoint information from storage and passes it to the newly launched worker processes to resume training.\nTo find this run state, Ray Train relies on passing in the same RunConfig(storage_path, name) pair as the previous run.\nIf the storage_path or name do not match, Ray Train will not be able to find the previous run state and will start a new run from scratch.\n\nWarning\nIf name is reused unintentionally, Ray Train will f"", ""start_index"": 6028, ""end_index"": 6516}, {""content"": ""model.load_state_dict(torch.load(...))\n            ...\n\n    # [2] Checkpoint saving and reporting logic.\n    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n        # torch.save(...)\n        ray.train.report(\n            {\""loss\"": 0.1},\n            checkpoint=ray.train.Checkpoint.from_"", ""start_index"": 7275, ""end_index"": 7571}]",datasets/raydocs_full/train_user-guides_fault-tolerance.txt
What levels of fault tolerance does Ray Train provide?,"[{""content"": ""e deprecation and migration.\n\nRay Train provides fault tolerance at three levels:\n\nWorker process fault tolerance handles errors that happen to one or more Train worker processes while they are executing the user defined training function.\nWorker node fault tolerance handles node failures "", ""start_index"": 343, ""end_index"": 633}, {""content"": ""executing the user defined training function.\nWorker node fault tolerance handles node failures that may occur during training.\nJob driver fault tolerance handles the case where Ray Train driver "", ""start_index"": 537, ""end_index"": 732}, {""content"": ""user guide covers how to configure and use these fault tolerance mechanisms.\n\nWorker Process and Node Fault Tolerance#\nWorker process failures are errors that occur within the user defined training f"", ""start_index"": 827, ""end_index"": 1026}]",datasets/raydocs_full/train_user-guides_fault-tolerance.txt
How does Ray Train handle worker process failures during training?,"[{""content"": ""vides fault tolerance at three levels:\n\nWorker process fault tolerance handles errors that happen to one or more Train worker processes while they are executing the user defined training function.\nWorker node fault tolerance handles node failures that may occur during training.\nJob "", ""start_index"": 386, ""end_index"": 669}, {""content"": ""use these fault tolerance mechanisms.\n\nWorker Process and Node Fault Tolerance#\nWorker process failures are errors that occur within the user defined training function of a training worker,\nsuch "", ""start_index"": 866, ""end_index"": 1061}, {""content"": "" a worker process or node failure is considered a retry. The\nnumber of retries is configurable through the max_failures attribute of the\nFailureConfig argument set in the RunConfig\npassed to the Trainer. By default, worker fault tolerance is disabled with max_failures=0.\nimport ray.train\n\n# Tries"", ""start_index"": 1922, ""end_index"": 2219}]",datasets/raydocs_full/train_user-guides_fault-tolerance.txt
What information is provided regarding the final log in the Wandb session?,"[{""content"": ""        1.728275.2814  \n\n\n\n\n\n(train_function_wandb pid=14647) 2022-11-02 16:03:17,149\tINFO wandb.py:282 -- Already logged into W&B.\n\n\n\nTrial Progress\n\n\nTrial name                      date   "", ""start_index"": 9306, ""end_index"": 9497}]",datasets/raydocs_full/tune_examples_tune-wandb.txt
"What memory status is reported for the Tune system as of November 2nd, 2022?","[{""content"": ""Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 "", ""start_index"": 8448, ""end_index"": 8545}]",datasets/raydocs_full/tune_examples_tune-wandb.txt
What is the output of the Streaming method in the GrpcDeployment class when invoked with a UserDefinedMessage?,"[{""content"": "" )\n        return user_response\n\n    def Streaming(\n        self, user_message: UserDefinedMessage\n    ) -> Generator[UserDefinedResponse, None, None]:\n        for i in range(10):\n            greeting = f\""{i}: Hello {user_message.name} from {user_message.origin}\""\n            num = user_message.num * 2 + i\n            user_response = UserDefinedResponse(\n                greeting=greeting,\n                num=num,\n            )\n            yield user_response\n\n         "", ""start_index"": 5871, ""end_index"": 6343}]",datasets/raydocs_full/serve_advanced-guides_grpc-guide.txt
What key functions can Serve perform for large language model applications?,"[{""content"": ""developer\n\n\n\n\nServe enables you to rapidly prototype, develop, and deploy scalable LLM applications to production. Many large language model (LLM) applications combine prompt preprocessing, vector database lookups, LLM API calls, and response validation. Because Serve supports any arbitrary Python code, you can write all these steps as a single Python module, enabling rapid development and easy testing. You can then quickly deploy your Ray Serve LLM application to production, and each application step can independently autoscale to efficiently accommodate user traffic without wasting resources. In order to improve performance of your LLM applications, Ray Serve has features for batching and can integrate with any model optimization technique. Ray Serve also supports streaming responses, a key feature for chatbot-like applications.\n\n\n\nHow does Serve compare to "", ""start_index"": 9541, ""end_index"": 10413}]",datasets/raydocs_full/serve_index.txt
What are the advantages of using Ray Serve for machine learning applications?,"[{""content"": ""often important to combine machine learning with business logic and traditional web serving logic such as database queries.\nRay Serve is unique in that it allows you to build and deploy an end-to-end distributed serving application in a single framework.\nYou can combine multiple ML models, business logic, and expressive HTTP handling using Serve\u2019s FastAPI integration (see FastAPI HTTP Deployments) to build your entire application as one Python program.\n\n\n\nCombine multiple "", ""start_index"": 5425, ""end_index"": 5902}, {""content"": ""epts or cloud configurations to use Serve.\n\n\n\nML engineer\n\n\n\n\nServe helps you scale out your deployment and runs them reliably and efficiently to save costs. With Serve\u2019s first-class model composition API, you can combine models together with business logic and build end-to-end user-facing applications. Additionally, Serve runs natively on Kubernetes with minimal operation overhead.\n\n\n\nML platform engineer\n\n\n\n\nServe specializes in scalable and reliable ML model serving. As such, it can be an important plug-and-play component of your ML platform stack.\nServe supports arbitrary Python code and therefore integrates well with the MLOps ecosystem. You can use it with model optimizers (ONNX, TVM), model monitoring systems (Seldon Alibi, Arize), model registries (MLFlow, Weights and Biases), machine learning frameworks (XGBoost, Scikit-learn), data app UIs (Gradio, Streamli"", ""start_index"": 8611, ""end_index"": 9490}]",datasets/raydocs_full/serve_index.txt
How does Serve aid in model serving scalability and cost efficiency?,"[{""content"": ""number of built-in primitives to help make your ML serving application efficient.\nIt supports dynamically scaling the resources for a model up and down by adjusting the number of replicas, batching requests to take advantage of efficient vectorized operations (especially important on GPUs), and a flexible resource allocation model that enables you to serve many models on limited "", ""start_index"": 7145, ""end_index"": 7527}, {""content"": ""elps you scale out your deployment and runs them reliably and efficiently to save costs. With Serve\u2019s first-class model composition API, you can combine models together with business logic a"", ""start_index"": 8680, ""end_index"": 8870}, {""content"": "".\nCompared to these other offerings, Ray Serve lacks the functionality for\nmanaging the lifecycle of your models, visualizing their performance, etc. Ray\nServe primarily focuses on model serving and providing the primitives for you to\nbuild your own ML platform on top.\n\n\n\nSeldon, KServe, Cortex\n\n\n\n\nYou can develop Ray Serve on your laptop, deploy it on a dev box, and scale it out\nto "", ""start_index"": 11369, ""end_index"": 11755}]",datasets/raydocs_full/serve_index.txt
What can the return value of get_default_learner_class() be?,"[{""content"": ""input framework.\n\nReturns:\nThe Learner class to use for this algorithm either as a class type or as\na string (e.g. \u201cray.rllib.algorithms.ppo.ppo_learner.PPOLearner\u201d).\n\n\n\n\n"", ""start_index"": 294, ""end_index"": 491}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.txt
What purpose does the get_default_learner_class() serve in the context of AlgorithmConfig?,"[{""content"": "".get_default_learner_class() \u2192 Type[Learner] | str[source]#\nReturns the Learner class to use for this algorithm.\nOverride this method in the sub-class to return the Learner class type given\nthe "", ""start_index"": 100, ""end_index"": 294}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.txt
What types of APIs does the Ray Workflows include?,"[{""content"": ""\n\nRay Workflows API#\n\n\nWorkflow Execution API\nWorkflow Management API\n\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/workflows_api_api.txt
What are the two primary features of the Ray Workflows API?,"[{""content"": ""\n\nRay Workflows API#\n\n\nWorkflow Execution API\nWorkflow Management API\n\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/workflows_api_api.txt
What does the MultiRLModule.setup function do?,"[{""content"": ""\n\nray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup#\n\n\nMultiRLModule.setup()[source]#\nSets up the underlying, individual RLModules.\n\n\n"", ""start_index"": 0, ""end_index"": 200}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.txt
What are the potential overheads associated with repeatedly starting and stopping a Ray cluster?,"[{""content"": ""tearDown(self):\n        ray.shutdown()\n\n\nHowever, starting and stopping a Ray cluster can actually incur a non-trivial amount of latency. For example, on a typical Macbook Pro laptop, starting and stopping can take nearly 5 seconds:\npython -c 'import ray; ray.init(); ray.shutdown()'  3.93s user 1.23s system 116% cpu 4.420 total\n\n\nAcross 20 tests, this ends up being 90 seconds of added "", ""start_index"": 1335, ""end_index"": 1723}]",datasets/raydocs_full/ray-contribute_testing-tips.txt
How does the process_rewards function handle game boundaries?,"[{""content"": ""sum, since this was a game boundary (pong specific!).\n        if r[t] != 0:\n            running_add "", ""start_index"": 2988, ""end_index"": 3088}]",datasets/raydocs_full/ray-core_examples_plot_pong_example.txt
How are gradients applied to model parameters using RMSProp in the provided code?,"[{""content"": ""gradients to the model parameters with RMSProp.\""\""\""\n        for k, v in self.weights.items():\n       "", ""start_index"": 6143, ""end_index"": 6243}, {""content"": ""* 2\n            self.weights[k] += lr * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n\n\ndef zero_grads(gr"", ""start_index"": 6342, ""end_index"": 6440}, {""content"": ""nsible for taking a model and an env\nand performing a rollout + computing a gradient u"", ""start_index"": 6639, ""end_index"": 6725}]",datasets/raydocs_full/ray-core_examples_plot_pong_example.txt
What are the stability levels for PublicAPI in Ray?,"[{""content"": ""which are decorated/labeled accordingly.\nAn API can be labeled:\n\nPublicAPI, which means the API is exposed to end users. PublicAPI has three sub-levels (alpha, beta, stable), as described below.\nDeveloperAPI, which means the API is explicitly exposed to advanced Ray users and library "", ""start_index"": 98, ""end_index"": 383}]",datasets/raydocs_full/ray-contribute_stability.txt
What are the expectations for Ray's alpha and beta API components regarding stability and change?,"[{""content"": ""alpha component undergoes rapid iteration with a known set of users who\nmust be tolerant of change. The number of users should be a\ncurated, manageable set, such that it is feasible to communicate with all\nof them individually.\nBreaking changes must be both allowed and expected in alpha components, and\nusers must have no expectation of stability.\n\n\nBeta#\nA beta component must be "", ""start_index"": 581, ""end_index"": 963}, {""content"": ""fetime of the major\nAPI version. Because users expect such stability from components marked stable,\nthere must be no breaking changes to these components within a major version\n(excluding extraordinary circumstances).\n\nDocstrings#\n\n\nray.util.annotations.PublicAPI(*args, **kwargs)[source]#\nAnnotation for documenting public APIs.\nPublic APIs are classes and methods exposed to end users of Ray.\nIf stability=\""alpha\"", the API can be used by advanced users who are\ntolerant to and expect breaking changes.\nIf stability=\""beta\"", the API is still public and c"", ""start_index"": 1544, ""end_index"": 2098}]",datasets/raydocs_full/ray-contribute_stability.txt
What are the default settings for the MultiAgentReplayBuffer parameters?,"[{""content"": ""ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer(capacity: int = 10000, storage_unit: str = 'timesteps', num_shards: int = 1, replay_mode: str = 'independent', "", ""start_index"": 91, ""end_index"": 282}, {""content"": "" = 0, replay_zero_init_states: bool = True, underlying_buffer_config: dict = None, **kwargs)[source]#\nBases: ReplayBuffer\nA replay buffer shard for multiagent setups.\nThis bu"", ""start_index"": 373, ""end_index"": 547}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.txt
How is the MultiAgentReplayBuffer designed to facilitate parallel operations?,"[{""content"": ""**kwargs)[source]#\nBases: ReplayBuffer\nA replay buffer shard for multiagent setups.\nThis buffer is meant to be run in parallel to distribute experiences\nacross num_shards shards. Unlike simpler "", ""start_index"": 456, ""end_index"": 650}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.txt
How can users interact with RLlib's algorithms using the Algorithm class?,"[{""content"": ""network update, and so on.\nThe HOW will be delegated to components such as RolloutWorker, etc..\nIt is the main entry point for RLlib users to interact with RLlib\u2019s algorithms.\nIt allows you to train "", ""start_index"": 475, ""end_index"": 674}, {""content"": ""er.#\n\n\n\nBuilding Custom Algorithm Classes#\n\nWarning\nAs of Ray >= 1.9, it is no longer recommended to"", ""start_index"": 1149, ""end_index"": 1249}]",datasets/raydocs_full/rllib_package_ref_algorithm.txt
What are the key changes to building custom Algorithm classes as recommended from Ray version 1.9 onwards?,"[{""content"": ""Custom Algorithm Classes#\n\nWarning\nAs of Ray >= 1.9, it is no longer recommended to use the build_trainer() utility\nfunction for creating custom Algorithm sub-classes.\nInstead, follow the simple guidelines here for directly sub-classing from\nAlgorithm.\n\nIn order to create a custom "", ""start_index"": 1166, ""end_index"": 1448}]",datasets/raydocs_full/rllib_package_ref_algorithm.txt
What changes does Ray 2.40 introduce regarding its API?,"[{""content"": ""\n\nOffline RL API#\n\nNote\nRay 2.40 uses RLlib\u2019s new API stack by default.\nThe Ray team has mostly completed transitioning algorithms, example scripts, and\ndocumentation to the new code base.\nIf "", ""start_index"": 0, ""end_index"": 192}]",datasets/raydocs_full/rllib_package_ref_offline.txt
What does the method Algorithm.train() return after execution?,"[{""content"": ""training\nprocess.\nnode_ip (str): Node ip of the machine hosting the training\nprocess.\n\n\nReturns:\nA dict that describes training progress.\n\n\n\n\n"", ""start_index"": 942, ""end_index"": 1141}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.train.txt
What specific method does Algorithm.train() call internally during a training iteration?,"[{""content"": ""of training.\nCalls step() internally. Subclasses should override step()\ninstead to return "", ""start_index"": 98, ""end_index"": 188}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.train.txt
What cloud platforms does Ray support for launching clusters?,"[{""content"": ""Ray ships with built-in support\nfor launching AWS and GCP clusters, and also has community-maintained integrations for Azure, Aliyun and vSphere.\nEach Ray cluster consists of a "", ""start_index"": 99, ""end_index"": 276}]",datasets/raydocs_full/cluster_vms_index.txt
What types of nodes comprise a Ray cluster?,"[{""content"": ""community-maintained integrations for Azure, Aliyun and vSphere.\nEach Ray cluster consists of a head node and a collection of worker nodes. Optional\nautoscaling support allows the Ray cluster to "", ""start_index"": 180, ""end_index"": 375}]",datasets/raydocs_full/cluster_vms_index.txt
What are the functions of autoscaling in Ray clusters?,"[{""content"": ""head node and a collection of worker nodes. Optional\nautoscaling support allows the Ray cluster to be sized according to the\nrequirements of your Ray workload, adding and removing worker nodes as needed. Ray supports\nclusters composed of multiple heterogeneous compute nodes (including GPU "", ""start_index"": 276, ""end_index"": 566}]",datasets/raydocs_full/cluster_vms_index.txt
What are the conditions required for send/recv communication in ray.util.collective?,"[{""content"": ""send/recv exhibits the same behavior with the collective functions:\nthey are synchronous blocking calls \u2013 a pair of send and recv must be called together on paired processes in order to specify the entire communication,\nand must successfully rendezvous with each other to proceed. See the code "", ""start_index"": 4721, ""end_index"": 5015}]",datasets/raydocs_full/ray-more-libs_ray-collective.txt
How is a collective communication group created in the declarative example provided?,"[{""content"": ""\""177\"",\n   \""world_size\"": 2,\n   \""ranks\"": [0, 1],\n   \""backend\"": \""nccl\""\n}\ncollective.create_collective_group(workers, **_options)\nresults = "", ""start_index"": 2904, ""end_index"": 3040}]",datasets/raydocs_full/ray-more-libs_ray-collective.txt
How can you run a specific C++ test in the Ray project?,"[{""content"": ""...)')\n\n\nAlternatively, you can also run one specific C++ test. You can use:\nbazel test $(bazel query 'kind(cc_test, ...)') --test_filter=ClientConnectionTest --test_output=streamed\n\n\n\n\n\nCode "", ""start_index"": 4508, ""end_index"": 4700}]",datasets/raydocs_full/ray-contribute_getting-involved.txt
How is the __init__ method documented in the RayClass?,"[{""content"": ""as this one.\n    Do not introduce multi-line first sentences.\n\n    The __init__ method is "", ""start_index"": 6207, ""end_index"": 6297}]",datasets/raydocs_full/ray-contribute_getting-involved.txt
