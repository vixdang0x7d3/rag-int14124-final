question,references,corpus_id
What cloud platforms does Ray support for launching clusters?,"[{""content"": ""Ray ships with built-in support\nfor launching AWS and GCP clusters, and also has community-maintained integrations for Azure, Aliyun and vSphere.\nEach Ray cluster consists of a "", ""start_index"": 99, ""end_index"": 276}]",datasets/raydocs_full/cluster_vms_index.txt
What types of nodes comprise a Ray cluster?,"[{""content"": ""community-maintained integrations for Azure, Aliyun and vSphere.\nEach Ray cluster consists of a head node and a collection of worker nodes. Optional\nautoscaling support allows the Ray cluster to "", ""start_index"": 180, ""end_index"": 375}]",datasets/raydocs_full/cluster_vms_index.txt
What are the functions of autoscaling in Ray clusters?,"[{""content"": ""head node and a collection of worker nodes. Optional\nautoscaling support allows the Ray cluster to be sized according to the\nrequirements of your Ray workload, adding and removing worker nodes as needed. Ray supports\nclusters composed of multiple heterogeneous compute nodes (including GPU "", ""start_index"": 276, ""end_index"": 566}]",datasets/raydocs_full/cluster_vms_index.txt
What are the conditions required for send/recv communication in ray.util.collective?,"[{""content"": ""send/recv exhibits the same behavior with the collective functions:\nthey are synchronous blocking calls \u2013 a pair of send and recv must be called together on paired processes in order to specify the entire communication,\nand must successfully rendezvous with each other to proceed. See the code "", ""start_index"": 4721, ""end_index"": 5015}]",datasets/raydocs_full/ray-more-libs_ray-collective.txt
What are the different collective operations and their related functions in ray.util.collective?,"[{""content"": ""broadcast.\n\n\nReturns:\nNone\n\n\n\n\n\nray.util.collective.collective.broadcast_multigpu(tensor_list, src_rank: int = 0, src_tensor: int = 0, group_name: str = 'default')[source]#\nBroadcast the tensor from a source GPU to all other GPUs.\n\nParameters:\n\ntensor_list \u2013 the tensors to broadcast (src) or "", ""start_index"": 13831, ""end_index"": 14124}, {""content"": ""llgather tensors from each process of the group into a list.\n\nParameters:\n\ntensor_list \u2013 the results, stored as a list of tensors.\ntensor \u2013 the tensor (to be gathered) in the current process\ngroup_name \u2013 the name of the collective group.\n\n\nReturns:\nNone\n\n\n\n\n\nray.util.collective.collective.allgather_multigpu(output_tensor_lists: list, input_tensor_list: list, group_name: str = 'default')[source]#\nAllgather tensors from each gpus of the group into lists.\n\nParameters:"", ""start_index"": 14435, ""end_index"": 14904}, {""content"": ""ess in the group, then\nscatter the reduced list of tensors \u2013 one tensor for each process.\n\nParameters:\n\ntensor \u2013 the resulted tensor on this process.\ntensor_list \u2013 The list of tensors to be reduced and scattered.\ngroup_name \u2013 the name of the collective group.\nop \u2013 The reduce operation.\n\n\nReturns:\nNone\n\n\n\n\n\nray.util.collective.collective.reducescatter_multigpu(output_tensor_list, input_tensor_lists, group_name: str = 'default', op=ReduceOp.SUM)[source]#\nReducescatter a list of tensors across all GPUs.\n\nParameters:\n\noutput_tensor_list \u2013 the resulted list of tensors, w"", ""start_index"": 15409, ""end_index"": 15981}, {""content"": "" send, located on a GPU.\ndst_rank \u2013 the rank of the destination process.\ndst_gpu_index \u2013 the destination gpu index.\ngroup_name \u2013 the name of the collective group.\nn_elements \u2013 if specified, send the next n elements\nfrom the starting address of tensor.\n\n\nReturns:\nNone\n\n\n\n\n\nray.util.collective.collective.recv(tensor, src_rank: int, group_name: str = 'default')[source]#\nReceive a tensor from a remote process synchronously.\n\nParameters:\n\ntensor \u2013 the received tensor.\nsrc_rank \u2013 the rank of the source process.\ngroup_name \u2013 the name of the collective group.\n\n\nReturns:\nNone\n\n\n\n\n\nray.util.collective.collective.recv_multigpu(tensor, src_rank: i"", ""start_index"": 16859, ""end_index"": 17502}]",datasets/raydocs_full/ray-more-libs_ray-collective.txt
How is a collective communication group created in the declarative example provided?,"[{""content"": ""\""177\"",\n   \""world_size\"": 2,\n   \""ranks\"": [0, 1],\n   \""backend\"": \""nccl\""\n}\ncollective.create_collective_group(workers, **_options)\nresults = "", ""start_index"": 2904, ""end_index"": 3040}]",datasets/raydocs_full/ray-more-libs_ray-collective.txt
How can you run a specific C++ test in the Ray project?,"[{""content"": ""...)')\n\n\nAlternatively, you can also run one specific C++ test. You can use:\nbazel test $(bazel query 'kind(cc_test, ...)') --test_filter=ClientConnectionTest --test_output=streamed\n\n\n\n\n\nCode "", ""start_index"": 4508, ""end_index"": 4700}]",datasets/raydocs_full/ray-contribute_getting-involved.txt
How is the __init__ method documented in the RayClass?,"[{""content"": ""as this one.\n    Do not introduce multi-line first sentences.\n\n    The __init__ method is "", ""start_index"": 6207, ""end_index"": 6297}]",datasets/raydocs_full/ray-contribute_getting-involved.txt
What are the guidelines for formatting docstrings in Python code according to the text?,"[{""content"": ""guidelines. Whenever in doubt, follow the local code style of the component.\nFor Python documentation, we follow a subset of the Google pydoc format. The following code snippets "", ""start_index"": 4940, ""end_index"": 5118}, {""content"": ""r) -> bool:\n    \""\""\""First sentence MUST be inline with the quotes and fit on one line.\n\n    Additional explanatory text can be added in paragraphs such as this one.\n    Do not introduce multi-line first sentences.\n\n    Examples:\n        .. doctest::\n\n            >>> # Provide code examples for key use cases, as possible.\n            >>> ray_canonical_doc_style(41, \""hello\"")\n            True\n\n            >>> # A second example.\n            >>> ray_canonical_doc_style(72, \""goodbye\"")\n            False\n\n    Args:\n        param1: The first parameter. Do not include the types in the\n            docstring. They should be defined only in the signature.\n            Multi-line parameter docs should be indented by four spaces.\n        param2: The second parameter.\n\n    Returns:\n        The return value. Do not include types here.\n    \""\""\""\n\n\nclass RayClass:\n    \""\""\""The summary line for a class docstring should fit on one line.\n\n    Additional exp"", ""start_index"": 5217, ""end_index"": 6161}]",datasets/raydocs_full/ray-contribute_getting-involved.txt
What happens during the RLlibCallback.on_evaluate_end method?,"[{""content"": ""**kwargs) \u2192 None[source]#\nRuns when the evaluation is done.\nRuns at the end of Algorithm.evaluate().\n\nParameters:\n\nalgorithm \u2013 Reference to the algorithm instance.\nmetrics_logger "", ""start_index"": 191, ""end_index"": 370}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end.txt
What parameters are involved in the execution of RLlibCallback.on_evaluate_end?,"[{""content"": ""Algorithm.evaluate().\n\nParameters:\n\nalgorithm \u2013 Reference to the algorithm instance.\nmetrics_logger \u2013 The MetricsLogger object inside the Algorithm. Can be\nused to log custom metrics after the most "", ""start_index"": 270, ""end_index"": 468}, {""content"": """", ""start_index"": 738, ""end_index"": 910}, {""content"": """", ""start_index"": 1285, ""end_index"": 1478}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end.txt
What modifications can be made using the evaluation_metrics parameter in RLlibCallback.on_evaluate_end?,"[{""content"": ""recent evaluation round.\nevaluation_metrics \u2013 Results dict to be returned from algorithm.evaluate().\nYou can mutate this object to add additional metrics.\nkwargs \u2013 Forward "", ""start_index"": 468, ""end_index"": 640}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_evaluate_end.txt
What functionality does the ray.util.ActorPool utility provide?,"[{""content"": ""\n\nUtility#\n\n\nray.util.ActorPool\nUtility class to operate on a fixed pool of actors.\n\nray.util.queue.Queue\nA first-in, first-out queue implementation on "", ""start_index"": 0, ""end_index"": 152}]",datasets/raydocs_full/ray-core_api_utility.txt
What are the functionalities of the ray.util.serialization utilities?,"[{""content"": ""system.\n\nray.util.serialization.register_serializer\nUse the given serializer to serialize instances of type cls, and use the deserializer to deserialize the serialized object.\n\nray.util.serialization.deregister_serializer\nDeregister the serializer associated with the "", ""start_index"": 214, ""end_index"": 482}, {""content"": ""TPU pod that the worker belongs to.\n\nray.util.accelerators.tpu.get_current_pod_name\nReturn the name of the TPU pod that the worker is a part of.\n\nray.util.accelerators.tpu.get_num_tpu_chips_on_n"", ""start_index"": 596, ""end_index"": 790}]",datasets/raydocs_full/ray-core_api_utility.txt
What methods are available for debugging in Ray as described in the text?,"[{""content"": ""of events in buckets.\n\n\n\n\n\nDebugging#\n\n\nray.util.rpdb.set_trace\nInterrupt the flow of the program and drop into the Ray debugger.\n\nray.util.inspect_serializability\nIdentifies what objects are "", ""start_index"": 1485, ""end_index"": 1677}, {""content"": """", ""start_index"": 3068, ""end_index"": 3257}, {""content"": """", ""start_index"": 4745, ""end_index"": 4940}]",datasets/raydocs_full/ray-core_api_utility.txt
What does the MultiRLModule class assume about the communication between its underlying RLModules?,"[{""content"": ""corresponding RLModule object with the associated batch within the\ninput.\nIt also assumes that the underlying RLModules do not share any parameters or\ncommunication with one another. The behavior of "", ""start_index"": 1315, ""end_index"": 1514}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.txt
What is the purpose of the MultiRLModule class?,"[{""content"": ""Dict[str, RLModuleSpec] | None = None, **kwargs)[source]#\nBases: RLModule\nBase class for an RLModule that contains n sub-RLModules.\nThis class holds a mapping from ModuleID to underlying RLModules. It provides\na convenient way of accessing each individual module, as well as accessing all of\nthem with only one API call. Whether a given module is trainable is\ndetermined by the caller "", ""start_index"": 351, ""end_index"": 736}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.txt
What is the role of MultiRLModule in handling different RLModules?,"[{""content"": ""RLModule that contains n sub-RLModules.\nThis class holds a mapping from ModuleID to underlying RLModules. It provides\na convenient way of accessing each individual module, as well as accessing all of\nthem with only one API call. Whether a given module is trainable is\ndetermined by the caller "", ""start_index"": 443, ""end_index"": 736}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.txt
What are the primary attributes of MultiRLModuleSpec?,"[{""content"": ""sub-RLModule under module_id.\n\n\n\n\n\nMultiRLModuleSpec.multi_rl_module_class: Type[MultiRLModule] = <class 'ray.rllib.core.rl_module.multi_rl_module.MultiRLModule'>\nThe class of the MultiRLModule to construct. By default,\nthis is the base MultiRLModule "", ""start_index"": 872, ""end_index"": 1123}, {""content"": ""\n\n\nMultiRLModuleSpec.action_space: gymnasium.Space | None = None\nOptional global action space for the MultiRLModule. Useful for\nshared network components that live only inside the MultiRLModule and don\u2019t\nhave their own ModuleID and own RLModule within self._rl_m"", ""start_index"": 1410, ""end_index"": 1672}, {""content"": ""ec.rl_module_specs: RLModuleSpec | Dict[str, RLModuleSpec] = None\nThe module specs for each individual module. It can be either\nan RLModuleSpec used for all module_ids or a dictionary mapping from module\nIDs to RLModuleSpecs for each individual module.\n\n\n\nDefaultModelConfig#\n\n"", ""start_index"": 2210, ""end_index"": 2487}, {""content"": ""rd_train.\n\n\nforward_exploration\nDO NOT OVERRIDE! Forward-pass during exploration, called from the sampler.\n\nforward_inference\nDO NOT OVERRIDE! Forward-pass during evaluation, called from the sampler.\n\nforward_train\nDO NOT OVERRIDE! Forward-pass during training called from the learner.\n\n\n\nOverride these private methods to define your custom model\u2019s forward behavior.\n"", ""start_index"": 3287, ""end_index"": 3655}, {""content"": ""ltiRLModule API#\n\nConstructor#\n\n\nMultiRLModule\nBase class for an RLModule that contains n sub-RLModules.\n\nMultiRLModule.setup\nSets up the underlying, individual RLModules.\n\nMultiRLModule.as_multi_rl_module\nReturns self in order to match RLMod"", ""start_index"": 4634, ""end_index"": 4876}]",datasets/raydocs_full/rllib_package_ref_rl_modules.txt
What are the required functions for implementing the InferenceOnlyAPI in an RLModule?,"[{""content"": ""ray.rllib.core.rl_module.apis.inference_only_api.InferenceOnlyAPI[source]#\nAn API to be implemented by RLModules that have an inference-only mode.\nOnly the get_non_inference_attributes method needs to get implemented for\nan RLModule to have the following functionality:\n- On EnvRunners (or when "", ""start_index"": 5535, ""end_index"": 5830}, {""content"": ""stribution for\neither Q-values or advantages (in case of a dueling architecture),\n(\u201catoms\u201d), the logits per action and atom and the probabilities\nof the discrete distribution (per action and atom of the support).\n\n\n\n\n\n\nSelfSupervisedLossAPI#\n\n\nclass ray.rllib.core.rl_module.apis.self_supervised_l"", ""start_index"": 9280, ""end_index"": 9577}]",datasets/raydocs_full/rllib_package_ref_rl_modules.txt
What are the primary attributes of the RLModule API?,"[{""content"": ""modules.\n\nRLModule.observation_space\n\n\nRLModule.action_space\n\n\nRLModule.inference_only\n\n\nRLModule.mo"", ""start_index"": 2636, ""end_index"": 2736}, {""content"": ""ores the state of the implementing class from the given path.\n\nfrom_checkpoint\nCreates a new Checkpo"", ""start_index"": 5217, ""end_index"": 5317}, {""content"": ""I[source]#\nAn API to be implemented by RLModules used for (distributional) Q-learning.\nRLModules imp"", ""start_index"": 7798, ""end_index"": 7898}, {""content"": "" loss for a single agent. For multi-agent use-cases\nthat require more complicated computation for loss, consider overriding the\ncompute_losses method instead."", ""start_index"": 10379, ""end_index"": 10537}]",datasets/raydocs_full/rllib_package_ref_rl_modules.txt
How can the learner class for an AlgorithmConfig be set?,"[{""content"": ""Algorithm.\nEither\na) User sets a specific learner class via calling .training(learner_class=...)\nb) "", ""start_index"": 173, ""end_index"": 273}, {""content"": """", ""start_index"": 446, ""end_index"": 639}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.txt
What happens if the learner class in AlgorithmConfig is not set by the user?,"[{""content"": ""User leaves learner class unset (None) and the AlgorithmConfig itself\nfigures out the actual learner class by calling its own\n.get_default_learner_class() method.\n\n\n"", ""start_index"": 273, ""end_index"": 466}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.txt
What are the alternative methods to set a learner class for the AlgorithmConfig?,"[{""content"": ""Algorithm.\nEither\na) User sets a specific learner class via calling .training(learner_class=...)\nb) User leaves learner class unset (None) and the AlgorithmConfig itself\nfigures out the actual learner class by calling its own\n.get_default_learner_class() method.\n\n\n"", ""start_index"": 173, ""end_index"": 466}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.learner_class.txt
Why should secret credentials not be included in URIs for runtime_env in Ray?,"[{""content"": ""secret credential that authenticates this URI. While Ray can successfully access your dependencies using authenticated URIs, you should not include secret credentials in your URIs for two reasons:\n\nRay may log the URIs used in your runtime_env, which means the Ray logs could contain your credentials.\nRay stores your remote dependency package in a local directory, and it uses a parsed version of the remote URI\u2013including your credential\u2013as the directory\u2019s name.\n\nIn short, your "", ""start_index"": 924, ""end_index"": 1404}]",datasets/raydocs_full/ray-core_runtime_env_auth.txt
What is the preferred method for authenticating remote URIs in a Ray runtime environment?,"[{""content"": ""parsed version of the remote URI\u2013including your credential\u2013as the directory\u2019s name.\n\nIn short, your remote URI is not treated as a secret, so it should not contain secret info. Instead, use a netrc file.\n\n\nRunning on VMs: the netrc File#\nThe netrc file contains credentials that Ray uses to "", ""start_index"": 1304, ""end_index"": 1595}]",datasets/raydocs_full/ray-core_runtime_env_auth.txt
How can KubeRay utilize the netrc file to authenticate remote URIs?,"[{""content"": ""when they don\u2019t contain credentials.\n\n\nRunning on KubeRay: Secrets with netrc#\nKubeRay can also obtain credentials from a netrc file for remote URIs. Supply your netrc file using a Kubernetes secret and a Kubernetes volume with these steps:\n1. Launch your Kubernetes cluster.\n2. Create the netrc file locally in your home directory.\n3. Store the netrc file\u2019s contents as a Kubernetes secret on your cluster:\nkubectl create secret generic netrc-secret --from-file=.netrc=\""$HOME/.netrc\""\n\n\n4. Expose the secret to your KubeRay application using a mounted volume, and update the NETRC environment variable to point to the netrc file. Include the following YAML in your KubeRay config.\nheadGroupSpec:\n    ...\n    containers:\n        - name: ...\n          image: rayproject/ray:latest\n          ...\n          volumeMounts:\n            - mountPath: \""/home/ray/netrcvolume/\""\n              name: netrc-kuberay\n              readOnly: true\n          env:\n            - name: NETRC\n              value: \""/home/ray/netrcvolume/.netrc\""\n    volumes:\n        - name: netrc-kuberay\n          secret:\n            secretName: netrc-secret\n\nworkerGroupSpecs:\n    ...\n    containers:\n        - name: ...\n          image: rayproject/ray:latest\n          ...\n          volumeMounts:\n            - mountPath: \""/home/ray/netrcvolume/\""\n              name: netrc-kuberay\n              readOnly: true\n          env:\n            - name: NETRC\n              value: \""/home/ray/netrcvolume/.netrc\""\n    volumes:\n        - name: netrc-kuberay\n          secret:\n            secretName: netrc-secret\n\n\n5. Apply your KubeRay config.\nYour KubeRay application can use the netrc file to access private remote URIs, even when they don\u2019t contain credentials.\n\n\n"", ""start_index"": 2274, ""end_index"": 4055}]",datasets/raydocs_full/ray-core_runtime_env_auth.txt
What are the examples of batch inference workloads using PyTorch mentioned in the text?,"[{""content"": ""data processing with a variety of frameworks and use cases.BeginnerFrameworkExamplePyTorchImage Classification Batch Inference with PyTorch ResNet152PyTorchObject Detection Batch Inference with "", ""start_index"": 97, ""end_index"": 291}, {""content"": "" PyTorch FasterRCNN_ResNet50TransformersImage Classification Batch Inference with Hugging Face Vision TransformervLLMBatch Inference with LoRA AdaptervLLMBatch Inference with Structural Output"", ""start_index"": 290, ""end_index"": 482}]",datasets/raydocs_full/data_examples.txt
What tasks can be accomplished with Hugging Face Vision Transformer according to the text?,"[{""content"": ""PyTorch FasterRCNN_ResNet50TransformersImage Classification Batch Inference with Hugging Face Vision TransformervLLMBatch Inference with LoRA AdaptervLLMBatch Inference with Structural "", ""start_index"": 291, ""end_index"": 476}]",datasets/raydocs_full/data_examples.txt
Which framework is used for tabular data training in the text?,"[{""content"": ""Vision TransformervLLMBatch Inference with LoRA AdaptervLLMBatch Inference with Structural OutputXGBoostTabular Data Training and Batch Inference with XGBoost\n"", ""start_index"": 385, ""end_index"": 576}]",datasets/raydocs_full/data_examples.txt
What does 'MultiRLModuleSpec.build' return when 'module_id' is not provided?,"[{""content"": ""provided, otherwise the built\nMultiRLModule.\n\n\n\n\n"", ""start_index"": 387, ""end_index"": 487}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.txt
What does MultiRLModuleSpec.build return if a module_id is specified?,"[{""content"": ""built. If None\n(default), builds the MultiRLModule.\n\nReturns:\nThe built RLModule if module_id is provided, otherwise the built\nMultiRLModule.\n\n\n\n\n"", ""start_index"": 290, ""end_index"": 487}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.txt
What is built by MultiRLModuleSpec.build in the absence of a provided module_id?,"[{""content"": ""built. If None\n(default), builds the MultiRLModule.\n\nReturns:\nThe built RLModule if module_id is "", ""start_index"": 290, ""end_index"": 387}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.txt
What kinds of data will not be collected according to the collection policy?,"[{""content"": ""have control over your data, and we will honor requests to delete your data.\nWe will not collect any personally identifiable data or proprietary code/data\nWe will not sell data or buy data about "", ""start_index"": 667, ""end_index"": 862}]",datasets/raydocs_full/cluster_usage-stats.txt
How can a user disable usage stats collection for a Ray cluster?,"[{""content"": ""UsageStatsToReport class to see the data we collect.\n\n\nHow to disable it#\nThere are multiple ways to disable usage stats collection before starting a cluster:\n\nAdd --disable-usage-stats option to the command that starts the Ray cluster (e.g., ray start --head --disable-usage-stats command).\nRun ray disable-usage-stats to disable collection for all future clusters. This won\u2019t affect currently running clusters. Under the hood, this command writes {\""usage_stats\"": true} to the global config file ~/.ray/config.json.\nSet the environment variable RAY_USAGE_STATS_ENABLED to 0 (e.g., RAY_USAGE_STATS_ENABLED=0 ray start --head command).\nIf you\u2019re using KubeRay, you can add disable-usage-stats: 'true' to .spec.[headGroupSpec|workerGroupSpecs].rayStartParams..\n\nCurrently "", ""start_index"": 1181, ""end_index"": 1951}]",datasets/raydocs_full/cluster_usage-stats.txt
What are the key principles of the Ray engineering team's data collection policy?,"[{""content"": ""principles of our collection policy:\n\nNo surprises \u2014 you will be notified before we begin collecting data. You will be notified of any changes to the data being collected or how it is used.\nEasy opt-out: You will be able to easily opt-out of data collection\nTransparency \u2014 you will "", ""start_index"": 320, ""end_index"": 602}, {""content"": ""we will honor requests to delete your data.\nWe will not collect any personally identifiable data o"", ""start_index"": 700, ""end_index"": 798}, {""content"": ""hich Ray libraries are used).\nPersonally identifiable data will never be collected. Please check the UsageStatsToReport class to see the data we collect.\n\n\nHow to disable it#\nThere are multiple wa"", ""start_index"": 1080, ""end_index"": 1276}, {""content"": "" affect currently running clusters. Under the hood, this command writes {\""usage_stats\"": true} to the global config file ~/.ray/config.json.\nSet the environment variable RAY_USAGE_STATS_ENABLED to 0 "", ""start_index"": 1558, ""end_index"": 1756}]",datasets/raydocs_full/cluster_usage-stats.txt
What are the specific drawbacks of online inference solutions like Bento ML and Sagemaker Batch Transform?,"[{""content"": ""it easy to write performant inference code and can abstract away infrastructure complexities. But they are designed for online inference rather than offline batch inference, which are two different problems with different sets of requirements. These solutions introduce additional complexity like HTTP, and cannot effectively handle large datasets leading inference service providers like Bento ML to integrating with Apache Spark for offline inference.\nRay Data is built for offline batch "", ""start_index"": 1087, ""end_index"": 1577}]",datasets/raydocs_full/data_comparisons.txt
Which specific solutions provide online inference capabilities?,"[{""content"": ""fer of data from storage to CPU to GPU.\n\n\n\nOnline inference solutions: Bento ML, Sagemaker Batch Transform\n\n\n\n\nSolutions like Bento ML, Sagemaker Batch Transform, or Ray Serve provide APIs to make "", ""start_index"": 890, ""end_index"": 1087}]",datasets/raydocs_full/data_comparisons.txt
What are the advantages of using Ray Data for batch processing?,"[{""content"": ""service providers like Bento ML to integrating with Apache Spark for offline inference.\nRay Data is built for offline batch jobs, without all the extra complexities of starting servers or sending HTTP requests.\nFor a more detailed performance comparison between Ray Data and Sagemaker Batch "", ""start_index"": 1453, ""end_index"": 1744}, {""content"": "" Ray Data compare to other solutions for ML training ingest?#\n\n\nPyTorch Dataset and DataLoader\n\n\n\n\n\nFramework-agnostic: Datasets is framework-agnostic and portable between different distributed"", ""start_index"": 2315, ""end_index"": 2508}]",datasets/raydocs_full/data_comparisons.txt
What are the primary functions of Ray Tune in hyperparameter tuning?,"[{""content"": ""tuning comes into play. By using tuning libraries such as\nRay Tune we can try out combinations of hyperparameters. Using sophisticated search\nstrategies, these parameters can be selected so that they are likely to lead to good\nresults (avoiding an expensive exhaustive search). Also, trials that do not perform\nwell can be preemptively stopped to reduce waste of computing resources. Lastly, Ray Tune\nalso takes care of training these runs in parallel, greatly increasing search "", ""start_index"": 11075, ""end_index"": 11554}]",datasets/raydocs_full/tune_examples_tune-xgboost.txt
What is the default value for the maximum tree depth in XGBoost?,"[{""content"": ""and 6 is often a good starting point for this parameter.\nXGBoost\u2019s default value is 3.\n\n\nMinimum "", ""start_index"": 6573, ""end_index"": 6670}]",datasets/raydocs_full/tune_examples_tune-xgboost.txt
What are some key advantages of using XGBoost for machine learning?,"[{""content"": ""xgboost\n\n\n\n\nWhat is XGBoost#\nXGBoost (eXtreme Gradient Boosting) is a powerful and efficient implementation of gradient boosted decision trees. It has become one of the most popular machine learning algorithms due to its:\n\nPerformance: Consistently strong results across many types of problems\nSpeed: Highly optimized implementation that can leverage GPU acceleration\nFlexibility: Works with many types of prediction problems (classification, regression, ranking)\n\nKey "", ""start_index"": 967, ""end_index"": 1436}]",datasets/raydocs_full/tune_examples_tune-xgboost.txt
What issues arise from forking new processes in Ray application code?,"[{""content"": ""processes for you. Ray Objects, Tasks, and\nActors manages sockets to communicate with the Raylet and the GCS. If you fork new\nprocesses in your application code, the processes could share the same sockets without\nany synchronization. This can lead to corrupted message and unexpected\nbehavior.\nThe solution is to:\n1. use \u201cspawn\u201d method to start new processes so that "", ""start_index"": 295, ""end_index"": 662}]",datasets/raydocs_full/ray-core_patterns_fork-new-processes.txt
What are the recommended methods to start new processes in Ray application code?,"[{""content"": ""application code-for example, in\ndriver, tasks or actors. Instead, use \u201cspawn\u201d method to start new processes or use Ray\ntasks and actors to parallelize your workload\nRay manages the lifecycle of "", ""start_index"": 100, ""end_index"": 295}, {""content"": "" process\u2019s\nmemory space isn\u2019t copied to the child processes or\n2. use Ray tasks and\nactors to parallelize your workload and let Ray to manage the lifecycle of the\nprocesses for you.\n\nCode example#\nimport os\n\nos.environ[\""RAY_DEDUP_LOGS\""] = \""0\""\n\nimport ray\nfrom concurrent.futures import Pr"", ""start_index"": 668, ""end_index"": 956}]",datasets/raydocs_full/ray-core_patterns_fork-new-processes.txt
What are the consequences of forking new processes in Ray application code?,"[{""content"": ""processes for you. Ray Objects, Tasks, and\nActors manages sockets to communicate with the Raylet and the GCS. If you fork new\nprocesses in your application code, the processes could share the same sockets without\nany synchronization. This can lead to corrupted message and unexpected\nbehavior.\nThe solution is to:\n1. use \u201cspawn\u201d method to start new processes so that "", ""start_index"": 295, ""end_index"": 662}]",datasets/raydocs_full/ray-core_patterns_fork-new-processes.txt
"What issue arises with scheduling tasks in Placement Groups using Ray, and how can it be fixed?","[{""content"": ""files to be consistent across machines.\n\n\nPlacement Groups aren\u2019t composable#\nIssue: If you schedule a new task from the tasks or actors running within a Placement Group, the system might fail to allocate resources properly, causing the operation to hang.\nExample: Imagine you are using "", ""start_index"": 2622, ""end_index"": 2909}, {""content"": ""it only when you need it for debugging.\nExample:\nimport ray\n\n# Enable stack trace capture\nray.init(runtime_env={\""env_vars\"": {\""RAY_record_task_actor_creation_sites\"": \""true\""}})\n\n@ray.remote\ndef my_task():\n    return 42\n\n# Capture the stack trace upon task invocation.\nfuture = my_task.remote()\nresult = ray.get(future)\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value "", ""start_index"": 6075, ""end_index"": 6512}]",datasets/raydocs_full/ray-observability_user-guides_debug-apps_general-debugging.txt
What is the expected behavior when setting environment variables on the Driver in Ray?,"[{""content"": ""1\n\n\nray.get(myfunc.remote())\n# this prints: \""myenv is None\""\n\n\nExpected behavior: Users may expect that setting environment variables on the Driver sends them to all Worker processes as if running "", ""start_index"": 815, ""end_index"": 1011}]",datasets/raydocs_full/ray-observability_user-guides_debug-apps_general-debugging.txt
How can environment variables be successfully passed to Worker processes in Ray?,"[{""content"": ""Worker processes as if running on a single machine, but it doesn\u2019t.\nFix: Enable Runtime Environments to explicitly pass environment variables. When you call ray.init(runtime_env=...), it sends the specified environment variables to the Workers.\nAlternatively, you can set the "", ""start_index"": 980, ""end_index"": 1256}, {""content"": "" Actor may sometimes succeed and sometimes fail.\nThis inconsistency arises because the Task or Actor finds the file when running on the Head Node, but the file might not exist on other machines.\nExample: Consider the following scenario:\n% "", ""start_index"": 1640, ""end_index"": 1879}]",datasets/raydocs_full/ray-observability_user-guides_debug-apps_general-debugging.txt
What exception is raised by MultiRLModule.remove_module when a specified module ID does not exist?,"[{""content"": ""not\nfound.\n\n\nRaises:\nValueError \u2013 If the module ID does not exist and raise_err_if_not_found is\n    True.\n\n\n\n\n"", ""start_index"": 353, ""end_index"": 553}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.txt
What parameter determines if an error is raised when a module ID is not found using MultiRLModule.remove_module?,"[{""content"": ""remove.\nraise_err_if_not_found \u2013 Whether to raise an error if the module ID is not\nfound.\n\n\nRaises:\nValueError \u2013 If the module ID does not exist and raise_err_if_not_found is\n    "", ""start_index"": 274, ""end_index"": 453}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.txt
What function in MultiRLModule is used to remove a module at runtime?,"[{""content"": ""\n\nray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module#\n\n\nMultiRLModule.remove_module(module_id: str, *, raise_err_if_not_found: bool = True) \u2192 None[source]#\nRemoves a module at "", ""start_index"": 0, ""end_index"": 194}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.txt
What are the key configurations in the Fluent Bit ConfigMap?,"[{""content"": ""to set up Grafana Loki datasource and deploy Grafana.\n\n\nConfiguring log processing#\nThe first step is to create a ConfigMap with configuration\nfor Fluent Bit.\nThe following ConfigMap example configures a Fluent Bit sidecar to:\n\nTail Ray logs.\nSend logs to a Grafana Loki endpoint.\nAdd metadata to the logs for filtering by labels, for example, RayCluster.\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentbit-config\ndata:\n  fluent-bit.conf: |\n    [INPUT]\n        Name tail\n        Path /tmp/ray/session_latest/logs/*\n        Tag ray\n        Path_Key true\n        Refresh_Interval 5\n    [FILTER]\n        Name modify\n        Match ray\n        Add POD_LABELS ${POD_LABELS}\n    [OUTPUT]\n        Name loki\n        Match *\n        Host loki-gateway\n        Port 80\n        Labels RayCluster=${POD_LABELS}\n        tenant_id test\n\n\nA few notes on the above config:\n\nThe Path_Key true line above ensures that file names are included in the log records\nemitted by Fluent Bit.\nThe Refresh_Interval 5 line asks Fluent Bit to refresh the list of files\nin the log directory once per 5 seconds, rather than the default 60.\nThe reason is that the directory /tmp/ray/session_latest/logs/ does not exist\ninitially (Ray must create it first). Setting the Refresh_Interval low allows us to see logs\nin the Fluent Bit container\u2019s stdout sooner.\nThe Kubernetes downward API populates the POD_LABELS variable used in the FILTER section. It pulls the label from the pod\u2019s metadata label ray.io/cluster, which is defined in the Fluent Bit sidecar container\u2019s environment.\nThe tenant_id field allows you to assign logs to different tenants. In this example, Fluent Bit sidecar sends the logs to the test tenant. You can adjust this "", ""start_index"": 2440, ""end_index"": 4153}]",datasets/raydocs_full/cluster_kubernetes_user-guides_persist-kuberay-custom-resource-logs.txt
What are the volume mount configurations for the sidecars in the RayCluster setup?,"[{""content"": ""       - mountPath: /tmp/ray\n            name: ray-logs\n          - mountPath: /fluent-bit/etc/fluent-bit.conf\n            subPath: fluent-bit.conf\n            name: "", ""start_index"": 5834, ""end_index"": 6000}, {""content"": ""fluentbit-config\n\n\nMounting the ray-logs volume gives the sidecar container access to Ray\u2019s logs.\nThe fluentbit-config volume gives the sidecar access to logging configuration.\n\n\nPutting "", ""start_index"": 6000, ""end_index"": 6187}, {""content"": ""     - name: fluentbit\n          image: fluent/fluent-bit:3.2.2\n          # Get Kubernetes metadata via downward API\n          env:\n          - name: POD_LABELS\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['ray.io/cluster']\n          # These resource requests for Fluent Bit should be sufficient in production.\n          resources:\n            requests:\n              cpu: 100m\n              memory: 128Mi\n            limits:\n              cpu: 100m\n              memory: 128Mi\n          volumeMounts:\n          - mountPath: /tmp/ray\n            name: ray-logs\n          - mountPath: /fluent-bit/etc/fluent-bit.conf\n            subPath: fluent-bit.conf\n            name: fluentbit-config\n        # Log and config volumes\n        volumes:\n        - name: ray-logs\n          emptyDir: {}\n        - name: fluentbit-config\n          configMa"", ""start_index"": 7593, ""end_index"": 8475}]",datasets/raydocs_full/cluster_kubernetes_user-guides_persist-kuberay-custom-resource-logs.txt
What steps are involved in setting up Fluent Bit sidecar for RayCluster in Kubernetes?,"[{""content"": ""log processing#\nThe first step is to create a ConfigMap with configuration\nfor Fluent Bit.\nThe following ConfigMap example configures a Fluent Bit sidecar to:\n\nTail Ray logs.\nSend logs to a Grafana Loki endpoint.\nAdd metadata to the logs for filtering by labels, for example, "", ""start_index"": 2508, ""end_index"": 2784}, {""content"": ""             fieldRef:\n                fieldPath: metadata.labels['ray.io/cluster']\n          # These resource requests for Fluent Bit should be sufficient in production.\n          resources:\n    "", ""start_index"": 5469, ""end_index"": 5665}, {""content"": ""esources:\n            requests:\n              cpu: 100m\n              memory: 128Mi\n            limits:\n              cpu: 100m\n              memory: 128Mi\n          volumeMounts:\n          - mountPath: /tmp/ray\n            name: ray-logs\n          - mountPath: /fluent-bit/etc/flue"", ""start_index"": 7960, ""end_index"": 8242}, {""content"": ""Ray Pod.\nUse this example YAML file as a reference.\n\n\n\n\n"", ""start_index"": 11806, ""end_index"": 12006}]",datasets/raydocs_full/cluster_kubernetes_user-guides_persist-kuberay-custom-resource-logs.txt
What does the function 'convert_to_torch_tensor' do with None values in the input structure?,"[{""content"": ""Tensor.to().\n\n\nReturns:\nA new structure with the same layout as x but with all leaves converted\nto torch.Tensors. Leaves that are None are left unchanged.\n\n\n\n\n"", ""start_index"": 664, ""end_index"": 863}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.torch_utils.convert_to_torch_tensor.txt
What optional parameters does the function 'convert_to_torch_tensor' accept?,"[{""content"": ""h_tensor(x, device: str | None = None, pin_memory: bool = False, use_stream: bool = False, stream: torch.cuda.Stream | torch.cuda.classes.Stream | None = None)[source]#\nConverts any (possibly nested) structure to torch.Tensors.\n\nParameters:\n\nx \u2013 The input structure whose leaves will be converted.\ndevice \u2013 The device to create the tensor on (e.g. \u201ccuda:0\u201d or \u201ccpu\u201d).\npin_memory \u2013 If True, calls pin_memory() on the created tensors.\nuse_stream \u2013 If True, uses a separate CUDA stream for Tensor.to().\nstream \u2013 An optional CUDA stream for the host-to-device copy in "", ""start_index"": 100, ""end_index"": 664}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.torch_utils.convert_to_torch_tensor.txt
What does the 'convert_to_torch_tensor' function do to the input structure?,"[{""content"": ""torch.cuda.Stream | torch.cuda.classes.Stream | None = None)[source]#\nConverts any (possibly nested) structure to torch.Tensors.\n\nParameters:\n\nx \u2013 The input structure whose leaves will be "", ""start_index"": 199, ""end_index"": 387}, {""content"": """", ""start_index"": 863, ""end_index"": 1062}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.torch_utils.convert_to_torch_tensor.txt
What patterns promote efficiency in Ray applications?,"[{""content"": ""anti-patterns for writing Ray applications.\n\n\nPattern: Using nested tasks to achieve nested parallelism\nPattern: Using generators to reduce heap memory usage\nPattern: Using ray.wait to limit "", ""start_index"": 94, ""end_index"": 285}, {""content"": ""imit the number of pending tasks\nPattern: Using resources to limit the number of concurrently runni"", ""start_index"": 280, ""end_index"": 379}, {""content"": ""harms parallelism\nAnti-pattern: Calling ray.get unnecessarily harms performance\nAnti-pattern: Processing results in submission order using ray.get increases runtime\nAnti-pattern: "", ""start_index"": 752, ""end_index"": 931}, {""content"": ""ne-grained tasks harms speedup\nAnti-pattern: Redefining the same remote function or class harms performance\nAnti-pattern: Passing the same large argument by value repeatedly harms perform"", ""start_index"": 1037, ""end_index"": 1224}, {""content"": ""mance\nAnti-pattern: Closure capturing large objects harms performance\nAnti-pattern: Using global variables to share state between tasks and actors\nAnti-pattern: Serialize ray.ObjectRef out of band"", ""start_index"": 1223, ""end_index"": 1419}]",datasets/raydocs_full/ray-core_patterns_index.txt
What design techniques are described in the text as harmful to Ray applications?,"[{""content"": ""actors\nPattern: Using pipelining to increase throughput\nAnti-pattern: Returning ray.put() ObjectRefs from a task harms performance and fault tolerance\nAnti-pattern: Calling ray.get in a "", ""start_index"": 561, ""end_index"": 747}, {""content"": ""arms performance\nAnti-pattern: Closure capturing large objects harms performance\nAnti-pattern: Using global variables to share state between tasks and actors\nAnti-pattern: Serialize ray.ObjectRef"", ""start_index"": 1212, ""end_index"": 1407}, {""content"": """", ""start_index"": 1959, ""end_index"": 2058}, {""content"": """", ""start_index"": 2706, ""end_index"": 2903}, {""content"": """", ""start_index"": 3552, ""end_index"": 3741}]",datasets/raydocs_full/ray-core_patterns_index.txt
What are the methods for achieving concurrency in Ray applications?,"[{""content"": ""the number of pending tasks\nPattern: Using resources to limit the number of concurrently running tasks\nPattern: Using asyncio to run actor methods concurrently\nPattern: Using an actor to "", ""start_index"": 285, ""end_index"": 472}, {""content"": ""a task harms performance and fault tolerance\nAnti-pattern: Calling ray.get in a loop harms parallelism\nAnti-pattern: Calling ray.get unnecessarily harms performance\nAnti-pattern: "", ""start_index"": 667, ""end_index"": 846}]",datasets/raydocs_full/ray-core_patterns_index.txt
How does Ray Data handle tensors of variable shapes?,"[{""content"": ""batch[\""image\""].dtype\ndtype('uint8')\n\n\n\n\nBatches of variable-shape tensors#\nIf your tensors vary in shape, Ray Data represents batches as arrays of object dtype.\n>>> import ray\n>>> ds = "", ""start_index"": 742, ""end_index"": 927}]",datasets/raydocs_full/data_working-with-tensors.txt
What are the different data transformation methods available for tensor data in Ray Data?,"[{""content"": ""3)\n>>> batch[\""image\""][3].shape  \n(333, 465, 3)\n\n\n\n\n\nTransforming tensor data#\nCall map() or map_batches() to transform tensor data.\nfrom typing import Any, Dict\n\nimport ray\nimport numpy as "", ""start_index"": 1262, ""end_index"": 1451}, {""content"": ""t ray\n\nds = ray.data.read_images(\""s3://anonymous@ray-example-data/image-datasets/simple\"")\nds.write_numpy(\""/tmp/simple\"", column=\""image\"")\n\n\n\n\n\nJSON\nTo save images in a JSON file, call write_json().\nimport ray\n\nds = ray.data.read_images(\""s3://anonymous@ray-example-data/image-datasets/simple\"")\nds.write_json(\""/tmp/simple\"")\n\n\n\n\nFor more information on saving data, read Saving data.\n\n\n"", ""start_index"": 2616, ""end_index"": 3255}]",datasets/raydocs_full/data_working-with-tensors.txt
How does Ray Data handle the representation of fixed-shape tensor batches?,"[{""content"": ""fixed-shape tensors#\nIf your tensors have a fixed shape, Ray Data represents batches as regular ndarrays.\n>>> import ray\n>>> ds = "", ""start_index"": 466, ""end_index"": 596}]",datasets/raydocs_full/data_working-with-tensors.txt
What action occurs in PopulationBasedTraining when cloning a lower scoring trial into a higher scoring one?,"[{""content"": ""\n\n[PopulationBasedTraining] [Exploit] Cloning trial 942f2_00000 (score = 0.090282) into trial 942f2_00001 (score = -0.168306)\n\n2025-02-24 16:22:22,343\tINFO pbt.py:905 -- "", ""start_index"": 44427, ""end_index"": 44597}, {""content"": ""03 (score = 0.239975)\n\n2025-02-24 16:22:25,541\tINFO pbt.py:905 -- \n\n[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial942f2_00003:\nlr : 0.09"", ""start_index"": 46913, ""end_index"": 47083}]",datasets/raydocs_full/tune_examples_pbt_visualization_pbt_visualization.txt
What are the significant indicators shown in the training progress animation for PBT?,"[{""content"": ""filename=\""pbt.gif\"",\n)\n\n\n\n\nWe can also animate the training progress to see what\u2019s happening to the model parameters at each step. The animation shows:\n\nHow parameters move through space during training\nWhen exploitation occurs (jumps in parameter space)\nHow gradient directions change after hyperparameter perturbation\nBoth trials eventually converging to the optimal parameter "", ""start_index"": 32541, ""end_index"": 32919}]",datasets/raydocs_full/tune_examples_pbt_visualization_pbt_visualization.txt
"What are the changes to the hyperparameters h0, h1 in trial 942f2_00003?","[{""content"": ""trial942f2_00003:\nlr : 0.092 --- (* 0.8) --> 0.0736\nh0 : 0.21859874791501244 --- (resample) --> 0.007051230918609708\nh1 : 0.14995290392498006 --- (* 0.8) --> 0.11996232313998406\n\n(train_func "", ""start_index"": 50914, ""end_index"": 51105}, {""content"": "" 0.21859874791501244 --- (resample) --> 0.6821981346240038\nh1 : 0.14995290392498006 --- (* 0.8) --> 0.11996232313998406\n\n2025-02-24 16:22:38,261\tINFO pbt.py:878 -- \n\n[PopulationBasedTraining] [Ex"", ""start_index"": 52453, ""end_index"": 52648}, {""content"": "" 0.4475766741292116 --- (* 0.8) --> 0.3580613393033693\n\n2025-02-24 16:22:44,761\tINFO pbt.py:878 -- \n\n[PopulationBasedTraining] [Exploit] Cloning trial 942f2_00002 (score = 1.179472) into trial 942"", ""start_index"": 54923, ""end_index"": 55119}]",datasets/raydocs_full/tune_examples_pbt_visualization_pbt_visualization.txt
What types of data can the Distribution.logp method accept?,"[{""content"": ""\n\nray.rllib.models.distributions.Distribution.logp#\n\n\nabstract Distribution.logp(value: numpy.array | jnp.ndarray | tf.Tensor | torch.Tensor, **kwargs) \u2192 numpy.array | jnp.ndarray | tf.Tensor | "", ""start_index"": 0, ""end_index"": 194}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.models.distributions.Distribution.logp.txt
What does the Distribution.logp method return?,"[{""content"": ""placeholder.\n\n\nReturns:\nThe log-likelihood of the value.\n\n\n\n\n"", ""start_index"": 372, ""end_index"": 472}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.models.distributions.Distribution.logp.txt
What is computed by the Distribution.logp method?,"[{""content"": ""torch.Tensor[source]#\nThe log-likelihood of the distribution computed at value\n\nParameters:\n\nvalue "", ""start_index"": 194, ""end_index"": 293}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.models.distributions.Distribution.logp.txt
What are the benefits of the streaming loading interface patch developed by Data-Juicer for Apache Arrow?,"[{""content"": ""18.1.0), doesn\u2019t support streaming reading of JSON files.\nTo address the lack of native support for streaming JSON data, the Data-Juicer team developed a streaming loading interface and contributed an in-house patch for Apache Arrow (PR to the repository). This patch helps alleviate Out-of-Memory issues. With this patch, Data-Juicer in Ray mode, by default, uses the streaming loading interface "", ""start_index"": 2896, ""end_index"": 3293}]",datasets/raydocs_full/ray-more-libs_data_juicer_distributed_data_processing.txt
What is the general size of datasets deduplicated by Data-Juicer's MinHash-based operator?,"[{""content"": ""streaming-read support for CSV and Parquet files is already enabled.\n\n\nDeduplication#\nData-Juicer provides an optimized MinHash-LSH-based deduplication operator in Ray mode. It\u2019s a multiprocessing Union-Find set in Ray Actors and a load-balanced distributed algorithm, BTS, to complete equivalence class merging. This operator can deduplicate terabyte-sized datasets on 1280 CPU cores "", ""start_index"": 3326, ""end_index"": 3711}]",datasets/raydocs_full/ray-more-libs_data_juicer_distributed_data_processing.txt
What features of Ray and Arrow are considered for pre-splitting datasets in Data-Juicer?,"[{""content"": ""for such cases, Data-Juicer automatically splits the original dataset into smaller files in advance, taking into consideration the features of Ray and Arrow. When you encounter such "", ""start_index"": 2068, ""end_index"": 2250}]",datasets/raydocs_full/ray-more-libs_data_juicer_distributed_data_processing.txt
How can GPU resources be allocated to a deployment using Ray Serve?,"[{""content"": ""replica uses a single GPU, you can do the\nfollowing:\n@serve.deployment(ray_actor_options={\""num_gpus\"": 1})\ndef func(*args):\n    return "", ""start_index"": 733, ""end_index"": 867}, {""content"": "" \""num_cpus\"".\n\n\nCustom resources, accelerator types, and more#\nYou can also specify custom resources in ray_actor_options, for example to ensure that a deployment is scheduled on a specific node.\nFor example, if you have a deployment that requires 2 units of the \""custom_resource\"" resource, you can specify it like this:\n@serve.deployment(ray_actor_options={\""resources\"": {\""custom_resou"", ""start_index"": 1898, ""end_index"": 2282}]",datasets/raydocs_full/serve_resource-allocation.txt
What types of resource management does Ray Serve support for its deployments?,"[{""content"": "" helps you configure Ray Serve to:\n\nScale your deployments horizontally by specifying a number of replicas\nScale up and down automatically to react to changing traffic\nAllocate hardware resources (CPUs, GPUs, other accelerators, etc) for each deployment\n\n\nResource management (CPUs, GPUs, "", ""start_index"": 33, ""end_index"": 322}, {""content"": ""(CPUs, GPUs, other accelerators, etc) for each deployment\n\n\nResource management (CPUs, GPUs, accelerators)#\nYou may want to specify a deployment\u2019s resource requirements to reserve cluster resources like GPUs or other accelerators.  To assign hardware resources per replica, you can pass "", ""start_index"": 229, ""end_index"": 516}]",datasets/raydocs_full/serve_resource-allocation.txt
How does Ray Serve manage CPU resource allocations for replicas by default?,"[{""content"": ""resource requirements to\nray_actor_options.\nBy default, each replica reserves one CPU.\nTo learn "", ""start_index"": 516, ""end_index"": 612}]",datasets/raydocs_full/serve_resource-allocation.txt
What are the specific integration examples listed for using Ray Tune with different machine learning frameworks?,"[{""content"": ""frameworks#\n\n\nRay Tune integrates with many popular machine learning frameworks. Here you find a few practical examples showing you how to tune your models. At the end of these guides you will often find links to even more examples.\n\n\nHow to use Tune with Keras and TensorFlow models\n\nHow to use Tune with PyTorch models\n\nHow to tune PyTorch Lightning models\n\nTuning RL experiments with Ray Tune and Ray Serve\n\nTuning XGBoost parameters with Tune\n\nTuning LightGBM parameters with Tune\n\nTuning Horovod parameters with Tune\n\nTuning Hugging Face Transformers with Tune\n\nEnd-to-end example for tuning a TensorFlow model\n\nEnd-to-end example for tuning a PyTorch model with PBT\n\n\n\n\n\nExperiment tracking tools#\n\n\nRay Tune integrates with some popular Experiment tracking and "", ""start_index"": 273, ""end_index"": 1041}]",datasets/raydocs_full/tune_examples_index.txt
What are the categories of use cases for Ray Tune as outlined in the text?,"[{""content"": ""frameworks\nExperiment tracking tools\nHyperparameter optimization frameworks\nOthers\nExercises\n\n\nML "", ""start_index"": 175, ""end_index"": 273}]",datasets/raydocs_full/tune_examples_index.txt
What are the examples of end-to-end tuning explained for specific machine learning frameworks using Ray Tune?,"[{""content"": ""Tune\n\nTuning Horovod parameters with Tune\n\nTuning Hugging Face Transformers with Tune\n\nEnd-to-end "", ""start_index"": 753, ""end_index"": 851}, {""content"": ""Running Tune experiments with AxSearch\n\nRunning Tune experiments with HyperOpt\n\nRunning Tune experiments with BayesOpt\n\nRunning Tune experiments with BOHB\n\nRunning Tune experiments with Neve"", ""start_index"": 1604, ""end_index"": 1794}]",datasets/raydocs_full/tune_examples_index.txt
What occurs in the RLlibCallback.on_episode_created method before and after SingleAgentEpisode or MultiAgentEpisode is created?,"[{""content"": ""rl_module: RLModule | None = None, env_index: int, **kwargs) \u2192 None[source]#\nCallback run when a new episode is created (but has not started yet!).\nThis method gets called after a new SingleAgentEpisode or MultiAgentEpisode\ninstance has been created. This happens before the respective sub-environment\u2019s\nreset() is called by RLlib.\n\nSingleAgentEpisode/MultiAgentEpisode "", ""start_index"": 390, ""end_index"": 760}, {""content"": ""k will be called.\nenv_runner \u2013 Reference to the current EnvRunner.\nmetrics_logger \u2013 The Metr"", ""start_index"": 1150, ""end_index"": 1242}, {""content"": """", ""start_index"": 2002, ""end_index"": 2071}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created.txt
What are the parameters of the RLlibCallback.on_episode_created method?,"[{""content"": ""\n\nray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created#\n\n\nRLlibCallback.on_episode_created(*, episode: SingleAgentEpisode | MultiAgentEpisode | EpisodeV2, worker: EnvRunner | None = None, env_runner: EnvRunner | None = None, metrics_logger: MetricsLogger | None = None, base_env: BaseEnv | None = None, env: gymnasium.Env | None = None, policies: Dict[str, Policy] | None = None, rl_module: RLModule | None = None, env_index: int, **kwargs) \u2192 None[source]#\nCallback run when a "", ""start_index"": 0, ""end_index"": 487}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created.txt
What is the purpose of the RLlibCallback.on_episode_created method?,"[{""content"": ""rl_module: RLModule | None = None, env_index: int, **kwargs) \u2192 None[source]#\nCallback run when a new episode is created (but has not started yet!).\nThis method gets called after a new SingleAgentEpisode or MultiAgentEpisode\ninstance has been created. This happens before the "", ""start_index"": 390, ""end_index"": 665}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_episode_created.txt
What topics are covered in the user guides for Ray applications?,"[{""content"": ""guides include:\n\nDebugging Applications\nMonitoring with the CLI or SDK\nConfiguring Logging\nAdding Application-Level Metrics\nTracing\n\n\n"", ""start_index"": 97, ""end_index"": 295}]",datasets/raydocs_full/ray-observability_user-guides_index.txt
What specific guides are provided for monitoring Ray applications?,"[{""content"": ""guides include:\n\nDebugging Applications\nMonitoring with the CLI or SDK\nConfiguring Logging\nAdding "", ""start_index"": 97, ""end_index"": 195}]",datasets/raydocs_full/ray-observability_user-guides_index.txt
What are the contents of the user guides for managing Ray applications?,"[{""content"": ""\n\nUser Guides#\n\n\nThese guides help you monitor and debug your Ray applications and clusters.\nThe guides include:\n\nDebugging Applications\nMonitoring with the CLI or SDK\nConfiguring Logging\nAdding Application-Level Metrics\nTracing\n\n\n"", ""start_index"": 0, ""end_index"": 295}]",datasets/raydocs_full/ray-observability_user-guides_index.txt
What are the specific configurations for enabling model parallelism in the vLLMEngineProcessorConfig?,"[{""content"": ""ls, specify model parallelism.\nconfig = vLLMEngineProcessorConfig(\n    model_source=\""unsloth/Llama-3.1-8B-Instruct\"",\n    engine_kwargs={\n        \""max_model_len\"": 16384,\n        \""tensor_parallel_size\"": 2,\n        \""pipeline_parallel_size\"": 2,\n        \""enable_chunked_prefill\"": True,\n        \""max_num_batched_tokens\"": 2048,\n    },\n    concurrency=1,\n    batch_size=64,\n)\n\n\nThe underlying Processor object instantiates replicas of the vLLM engine and "", ""start_index"": 3324, ""end_index"": 3771}]",datasets/raydocs_full/data_working-with-llms.txt
What parameters can be set for engine_kwargs in the second instantiation of vLLMEngineProcessorConfig?,"[{""content"": ""     \""max_model_len\"": 16384,\n        \""tensor_parallel_size\"": 2,\n        \""pipeline_parallel_size\"": 2,\n        \""enable_chunked_prefill\"": True,\n        \""max_num_batched_tokens\"": 2048,\n    },\n    "", ""start_index"": 3464, ""end_index"": 3656}]",datasets/raydocs_full/data_working-with-llms.txt
What is the specified model source for the vLLMEngineProcessorConfig that includes AWS S3 details?,"[{""content"": ""argument.\nconfig = vLLMEngineProcessorConfig(\n    model_source=\""s3://your-bucket/your-model/\"",  # Make sure adding the trailing slash!\n    engine_kwargs={\""load_format\"": \""runai_streamer\""},\n    runtime_env={\""env_vars\"": {\n        \""AWS_ACCESS_KEY_ID\"": \""your_access_key_id\"",\n        \""AWS_SECRET_ACCESS_KEY\"": \""your_secret_access_key\"",\n        \""AWS_REGION\"": \""your_region\"",\n    }},\n    concurrency=1,\n    batch_size=64,\n)\n\n\nTo do multi-LoRA batch inference, you need to set LoRA "", ""start_index"": 4430, ""end_index"": 4901}]",datasets/raydocs_full/data_working-with-llms.txt
What steps are involved in setting up a RayJob using KubeRay?,"[{""content"": ""is step creates a local Kubernetes cluster using Kind. If you already have a Kubernetes cluster, you can skip this step.\nkind create cluster --image=kindest/node:v1.26.0\n\n\n\n\nStep 2: Install "", ""start_index"": 258, ""end_index"": 448}, {""content"": ""KubeRay operator#\nFollow this document to install the latest stable KubeRay operator from the Helm repository.\n\n\nStep 3: Create a RayJob#\nA RayJob consists of a RayCluster custom resource and a job "", ""start_index"": 448, ""end_index"": 646}, {""content"": ""bmits a job when the cluster is ready. The following is a CPU-only RayJob description YAML file for MNIST training on a PyTorch model.\n# Download `ray-job.pytorch-mnist.yaml`\ncurl -LO https://raw.gi"", ""start_index"": 737, ""end_index"": 935}, {""content"": ""US      RESTARTS   AGE\n# kuberay-operator-6dddd689fb-ksmcs                                1/1     Running     0          113m\n# rayjob-pytorch-mnist-raycluster-rkdmq-small-group-"", ""start_index"": 3801, ""end_index"": 3979}]",datasets/raydocs_full/cluster_kubernetes_examples_mnist-training-example.txt
What are the requirements for the worker and head Pods in a RayJob using KubeRay?,"[{""content"": ""Each worker Pod requires 3 CPUs, and the head Pod requires 1 CPU, as described in the template "", ""start_index"": 1315, ""end_index"": 1410}]",datasets/raydocs_full/cluster_kubernetes_examples_mnist-training-example.txt
How long did the RayJob's training process take?,"[{""content"": ""Training completed after 10 iterations at 2024-06-16 22:33:06. Total running time: 7min 10s\n\n# "", ""start_index"": 5521, ""end_index"": 5616}]",datasets/raydocs_full/cluster_kubernetes_examples_mnist-training-example.txt
What is the property mentioned in the context of ray.rllib.offline.offline_data.OfflineData?,"[{""content"": ""OfflineData.default_map_batches_kwargs#\n\n\n"", ""start_index"": 84, ""end_index"": 184}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.txt
Which property of OfflineData is described in the text?,"[{""content"": ""\n\nray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs#\n\n\nproperty OfflineData.default_map_batches_kwargs#\n\n\n"", ""start_index"": 0, ""end_index"": 184}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.txt
What is the name of the property associated with ray.rllib.offline.offline_data.OfflineData?,"[{""content"": ""\n\nray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs#\n\n\nproperty OfflineData.default_map_batches_kwargs#\n\n\n"", ""start_index"": 0, ""end_index"": 184}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_data.OfflineData.default_map_batches_kwargs.txt
What action space does an RL environment define?,"[{""content"": ""which is the structure and shape of observable tensors at each timestep,\nan action space, which defines the available actions for the agents at each time step, a reward function,\nand the rules "", ""start_index"": 3797, ""end_index"": 3990}]",datasets/raydocs_full/rllib_key-concepts.txt
What does the SingleAgentEpisode object contain?,"[{""content"": ""s around all training data the form of Episodes.\nThe SingleAgentEpisode class describes\nsingle-agent trajectories. The MultiAgentEpisode class contains several\nsuch single-agent episodes and describes the stepping times- and patterns of the individual agents with respect to each other.\nBoth Episode classes store the entire trajectory data generated while stepping through an RL environment.\nThis data includes the observations, info dicts, actions, rewards, termination signals, and any\nmodel computations along the way, like recurrent states, action logits, or action log probabilities.\n\nTip\nSee here for RLlib\u2019s standardized column names.\nNote that episodes "", ""start_index"": 7371, ""end_index"": 8033}, {""content"": ""training batch of exactly size\nconfig.train_batch_size_per_learner.\nA typical SingleAgentEpisode object roughly looks as follows:\n# A SingleAgentEpisode of length 20 has roughly the following schematic structure.\n# Note that after these 20 steps, you have 20 actions and rewards, but 21 observations and info dicts\n# due to the initial \""reset\"" observation/infos.\nepisode = {\n    'obs': np.ndarray((21, 4), dtype=float32),  # 21 due to additional reset obs\n    'infos': [{}, {}, {}, {}, .., {}, {}],  # infos are always lists of dicts\n    'actions': np.ndarray((20,), dtype=int64),  # Discrete(4) action space\n    'rewards': np.ndarray((20,), dtype=float32),\n    'extra_model_outputs': {\n        'action_dist_inputs': np.ndarray((20, 4), dtype=float32),  # Discrete(4) action space\n    },\n    'is_terminated': False,  # <- single bool\n    'is_truncated': True,  # <- single bool\n}\n\n\nFor complex observations, for example gym.spaces.Dict, the episode "", ""start_index"": 8608, ""end_index"": 9557}]",datasets/raydocs_full/rllib_key-concepts.txt
How are complex observations handled in an episode with gym.spaces.Dict?,"[{""content"": ""complex observations, for example gym.spaces.Dict, the episode holds all observations in a struct entirely analogous\nto the observation space, with NumPy arrays at the leafs of that dict. For "", ""start_index"": 9494, ""end_index"": 9686}, {""content"": ""be burdensome to get right, especially when leveraging environment vectorization,\nstateful recurrent neural networks, or when operating in a multi-agent setting.\nRLlib provides two built-in EnvRunner classes,\nSingleAgentEnvRunner and\nMultiAgentEnvRunner that\nautomatically handle these complex"", ""start_index"": 10545, ""end_index"": 10838}]",datasets/raydocs_full/rllib_key-concepts.txt
How can you mitigate the issue of Torch tensors on GPU devices when saving to disk?,"[{""content"": ""NumPy.\nFor more information on saving data, read\nSaving data.\n\nCaution\nTorch tensors that are on GPU devices can\u2019t be serialized and written to disk. Convert the tensors to CPU (tensor.to(\""cpu\"")) before saving the data.\n\n\n\n\nParquet\nimport torch\nimport ray\n\ntensor = torch.Tensor(1)\nds = "", ""start_index"": 7807, ""end_index"": 8094}]",datasets/raydocs_full/data_working-with-pytorch.txt
What are the key features of Ray Data's integration with Ray Train?,"[{""content"": ""Train#\nRay Data integrates with Ray Train for easy data ingest for data parallel training, with support for PyTorch, PyTorch Lightning, or Hugging Face training.\nimport torch\nfrom torch import "", ""start_index"": 1080, ""end_index"": 1273}]",datasets/raydocs_full/data_working-with-pytorch.txt
How does the operation of iterating over batches differ between PyTorch DataLoader and Ray Data?,"[{""content"": ""ds.map(extract_label).map(transform_image)\n\n\n\n\n\n\nPyTorch DataLoader#\nThe PyTorch DataLoader can be replaced by calling Dataset.iter_torch_batches() to iterate over batches of the dataset.\nThe "", ""start_index"": 11977, ""end_index"": 12169}]",datasets/raydocs_full/data_working-with-pytorch.txt
What is the return value of get_module if the specified module_id is not found?,"[{""content"": ""'default_policy') \u2192 RLModule | None[source]#\nReturns the (single-agent) RLModule with model_id (None if ID not found).\n\nParameters:\nmodule_id \u2013 ID of the (single-agent) RLModule to return from "", ""start_index"": 95, ""end_index"": 288}, {""content"": "" inside the local EnvRunner\u2019s\nMultiRLModule. None if module_id doesn\u2019t exist.\n\n\n\n\n"", ""start_index"": 383, ""end_index"": 579}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.get_module.txt
What does the get_module function return when the specified module_id is 'default_policy'?,"[{""content"": ""\n\nray.rllib.algorithms.algorithm.Algorithm.get_module#\n\n\nAlgorithm.get_module(module_id: str = 'default_policy') \u2192 RLModule | None[source]#\nReturns the (single-agent) RLModule with model_id "", ""start_index"": 0, ""end_index"": 190}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.get_module.txt
What parameter is used by the Algorithm.get_module function to identify the RLModule to return?,"[{""content"": ""(None if ID not found).\n\nParameters:\nmodule_id \u2013 ID of the (single-agent) RLModule to return from the MARLModule\nused by the local EnvRunner.\n\nReturns:\nThe RLModule found under the ModuleID key "", ""start_index"": 190, ""end_index"": 384}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.get_module.txt
How does KubeRay manage the Redis cleanup process?,"[{""content"": ""Redis cleanup.\nKubeRay only removes this finalizer after the Kubernetes Job successfully cleans up Redis.\n\nIn other words, if the Kubernetes Job fails, the RayCluster won\u2019t be deleted. In that case, "", ""start_index"": 14895, ""end_index"": 15094}, {""content"": ""a best-effort basis.\nKubeRay still removes the Kubernetes finalizer from the RayCluster if the Kubernetes Job fails, thereby unblocking the deletion of the RayCluster.\nUsers can turn off this by "", ""start_index"": 15387, ""end_index"": 15582}, {""content"": ""tion in conjunction with the YAML file.\n\nThese configurations require KubeRay 1.3.0+\nThe following section uses the new gcsFaultToleranceOptions field introduced in KubeRay 1.3.0. For the old GCS"", ""start_index"": 15979, ""end_index"": 16174}]",datasets/raydocs_full/cluster_kubernetes_user-guides_kuberay-gcs-ft.txt
What are the different timeout values set for the head Pod and the worker Pods in RayCluster?,"[{""content"": ""RayCluster.\nTherefore, KubeRay automatically injects the RAY_gcs_rpc_server_reconnect_timeout_s environment variable with the value 600 to the worker Pod and uses the default value 60 for the head Pod.\nThe timeout value for worker Pods must be longer than the timeout value for the head Pod "", ""start_index"": 12553, ""end_index"": 12844}]",datasets/raydocs_full/cluster_kubernetes_user-guides_kuberay-gcs-ft.txt
What is the dashboard URL for the connected Ray cluster?,"[{""content"": ""-- Connected to Ray cluster. View the dashboard at 10.244.0.8:8265 \n2025-04-18 02:51:29,069\tINFO "", ""start_index"": 4798, ""end_index"": 4895}]",datasets/raydocs_full/cluster_kubernetes_user-guides_kuberay-gcs-ft.txt
What methods must subclasses of ray.rllib.utils.checkpoints.Checkpointable implement?,"[{""content"": ""of RLlib that can be checkpointed to disk.\nSubclasses must implement the following APIs:\n- save_to_path()\n- restore_from_path()\n- from_checkpoint()\n- get_state()\n- set_state()\n- get_ctor_args_and_kwargs()\n- get_metadata()\n- get_checkpointable_components()\nPublicAPI (alpha): "", ""start_index"": 153, ""end_index"": 428}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.checkpoints.Checkpointable.txt
What are the file names defined as attributes in the class ray.rllib.utils.checkpoints.Checkpointable?,"[{""content"": ""dict.\n\n\n\nAttributes\n\n\nCLASS_AND_CTOR_ARGS_FILE_NAME\n\n\nMETADATA_FILE_NAME\n\n\nSTATE_FILE_NAME\n\n\n\n\n\n\n"", ""start_index"": 1176, ""end_index"": 1276}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.checkpoints.Checkpointable.txt
What class does ray.rllib.utils.checkpoints.Checkpointable inherit from?,"[{""content"": ""ray.rllib.utils.checkpoints.Checkpointable[source]#\nBases: ABC\nAbstract base class for a component "", ""start_index"": 54, ""end_index"": 153}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.checkpoints.Checkpointable.txt
What is the purpose of the set_state method in MultiRLModule?,"[{""content"": "" Dict[str, Any]) \u2192 None[source]#\nSets the state of the multi-agent module.\nIt is assumed that the state_dict is a mapping from module IDs to the\ncorresponding module\u2019s state. This method sets the state of each module by\ncalling their set_state method. If you want to set the state of some of "", ""start_index"": 100, ""end_index"": 392}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.txt
What does the state_dict represent in the context of MultiRLModule.set_state method?,"[{""content"": "" Dict[str, Any]) \u2192 None[source]#\nSets the state of the multi-agent module.\nIt is assumed that the state_dict is a mapping from module IDs to the\ncorresponding module\u2019s state. This method sets the "", ""start_index"": 100, ""end_index"": 296}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.txt
How does the MultiRLModule.set_state method interact with the individual RLModules?,"[{""content"": ""state_dict is a mapping from module IDs to the\ncorresponding module\u2019s state. This method sets the state of each module by\ncalling their set_state method. If you want to set the state of some of "", ""start_index"": 198, ""end_index"": 392}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.set_state.txt
What is the default value of the inference_only attribute in RLModuleSpec?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only#\n\n\nRLModuleSpec.inference_only: bool = False#\n\n\n"", ""start_index"": 0, ""end_index"": 197}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only.txt
Is the inference_only attribute in RLModuleSpec set to True by default?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only#\n\n\nRLModuleSpec.inference_only: bool = False#\n\n\n"", ""start_index"": 0, ""end_index"": 197}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only.txt
What type of data represents the inference_only attribute in RLModuleSpec?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only#\n\n\nRLModuleSpec.inference_only: bool = False#\n\n\n"", ""start_index"": 0, ""end_index"": 197}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModuleSpec.inference_only.txt
What is the eval_env_runner in the context of the Algorithm?,"[{""content"": ""Algorithm.eval_env_runner#\nThe local EnvRunner instance within the algo\u2019s evaluation EnvRunnerGroup.\n\n\n"", ""start_index"": 71, ""end_index"": 256}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner.txt
What does Algorithm.eval_env_runner represent?,"[{""content"": ""Algorithm.eval_env_runner#\nThe local EnvRunner instance within the algo\u2019s evaluation EnvRunnerGroup.\n\n\n"", ""start_index"": 71, ""end_index"": 256}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner.txt
What does the eval_env_runner property refer to within the Algorithm?,"[{""content"": ""Algorithm.eval_env_runner#\nThe local EnvRunner instance within the algo\u2019s evaluation EnvRunnerGroup.\n\n\n"", ""start_index"": 71, ""end_index"": 256}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.eval_env_runner.txt
What details are provided about the MultiAgentEnv class in the ray.rllib.env module?,"[{""content"": ""ray.rllib.env.multi_agent_env.MultiAgentEnv(*args: Any, **kwargs: Any)[source]#\nAn environment that hosts multiple independent agents.\nAgents are identified by AgentIDs (string).\nPublicAPI (beta): "", ""start_index"": 349, ""end_index"": 546}]",datasets/raydocs_full/rllib_package_ref_env_multi_agent_env.txt
What does the step method return in the context of MultiAgentEnv?,"[{""content"": ""Dict[Any, Any]) \u2192 Tuple[Dict[Any, Any], Dict[Any, Any], Dict[Any, Any], Dict[Any, Any], Dict[Any, Any]][source]#\nReturns observations from ready agents.\nThe returns are dicts mapping from agent_id strings to values. The\nnumber of agents in the env can vary over time.\n\nReturns:\nTuple containing 1) new observations for\neach ready agent, 2) reward values for each ready agent. If\nthe episode is just started, the value will be None.\n3) Terminated values for each ready agent. The special key\n\u201c__all__\u201d (required) is used to indicate env termination.\n4) Truncated values for each ready agent.\n5) Info values for each agent id (may be empty dicts).\n\n\nenv = ...\nobs, rewards, "", ""start_index"": 1235, ""end_index"": 1907}]",datasets/raydocs_full/rllib_package_ref_env_multi_agent_env.txt
What are the output values provided by the step method for the MultiAgentEnv instance?,"[{""content"": ""env can vary over time.\n\nReturns:\nTuple containing 1) new observations for\neach ready agent, 2) reward values for each ready agent. If\nthe episode is just started, the value will be None.\n3) Terminated values for each ready agent. The special key\n\u201c__all__\u201d (required) is used to indicate env termination.\n4) Truncated values for each ready agent.\n5) Info values for each agent id (may be empty dicts).\n\n\nenv = ...\nobs, rewards, terminateds, truncateds, infos = env.step(action_dict={\n    "", ""start_index"": 1479, ""end_index"": 1967}]",datasets/raydocs_full/rllib_package_ref_env_multi_agent_env.txt
What is the environment variable associated with the NEURON_RT_VISIBLE_CORES for the Neuron Core in the Ray framework?,"[{""content"": ""print(\""NEURON_RT_VISIBLE_CORES: {}\"".format(os.environ[\""NEURON_RT_VISIBLE_CORES\""]))\n\n@ray.remote(resources={\""neuron_cores\"": 1})\ndef "", ""start_index"": 6690, ""end_index"": 6821}]",datasets/raydocs_full/ray-core_scheduling_accelerators.txt
What is the environment variable associated with the ONEAPI_DEVICE_SELECTOR for both the GPUActor and gpu_task in the Ray framework?,"[{""content"": ""the second one.\nray.get(gpu_task.remote())\n\n\n(GPUActor pid=52420) GPU IDs: [0]\n(GPUActor pid=52420) ONEAPI_DEVICE_SELECTOR: 0\n(gpu_task pid=51830) GPU IDs: [1]\n(gpu_task pid=51830) "", ""start_index"": 6196, ""end_index"": 6377}, {""content"": ""task.remote())\n\n\n(NeuronCoreActor pid=52420) Neuron Core IDs: [0]\n(NeuronCoreActor pid=52420) NEURON_RT_VISIBLE_CORES: 0\n(neuron_core_task pid=51"", ""start_index"": 7212, ""end_index"": 7357}]",datasets/raydocs_full/ray-core_scheduling_accelerators.txt
How does Ray assign accelerators to tasks or actors that require them?,"[{""content"": ""and Actors#\nIf a task or actor requires accelerators, you can specify the corresponding resource requirements (e.g. @ray.remote(num_gpus=1)).\nRay then schedules the task or actor to a node that has enough free accelerator resources\nand assign accelerators to the task or actor by setting the corresponding environment variable (e.g. CUDA_VISIBLE_DEVICES) before running the task or actor "", ""start_index"": 3537, ""end_index"": 3925}]",datasets/raydocs_full/ray-core_scheduling_accelerators.txt
What does ExecutionResources specify in the context of ExecutionOptions?,"[{""content"": ""execution.\n\n\n\n\n\nResource Options#\n\n\nExecutionResources\nSpecifies resources usage or resource limits for execution.\n\n\n\n\n\n"", ""start_index"": 76, ""end_index"": 276}]",datasets/raydocs_full/data_api_execution_options.txt
What does the ExecutionOptions API include?,"[{""content"": ""\n\nExecutionOptions API#\n\nConstructor#\n\n\nExecutionOptions\nCommon options for execution.\n\n\n\n\n\nResource Options#\n\n\nExecutionResources\nSpecifies resources usage or resource limits "", ""start_index"": 0, ""end_index"": 176}, {""content"": ""execution.\n\n\n\n\n\nResource Options#\n\n\nExecutionResources\nSpecifies resources usage or resource limits for execution.\n\n\n\n\n\n"", ""start_index"": 76, ""end_index"": 276}]",datasets/raydocs_full/data_api_execution_options.txt
What is the purpose of ExecutionOptions in the API?,"[{""content"": ""\n\nExecutionOptions API#\n\nConstructor#\n\n\nExecutionOptions\nCommon options for execution.\n\n\n\n\n\nResource Options#\n\n\nExecutionResources\nSpecifies resources usage or resource limits "", ""start_index"": 0, ""end_index"": 176}]",datasets/raydocs_full/data_api_execution_options.txt
What methods are available for iterating over batches in the Ray Data dataset?,"[{""content"": ""batches#\nA batch contains data from multiple rows. Iterate over batches of dataset in different\nformats by calling one of the following methods:\n\nDataset.iter_batches() <ray.data.Dataset.iter_batches>\nDataset.iter_torch_batches() <ray.data.Dataset.iter_torch_batches>\nDataset.to_tf() <ray.data.Dataset.to_tf>\n\n\n\n\nNumPy\nimport "", ""start_index"": 951, ""end_index"": 1277}]",datasets/raydocs_full/data_iterating-over-data.txt
What data formats are used for iterating over batches in the Ray Data dataset?,"[{""content"": ""ray\n\nds = ray.data.read_images(\""s3://anonymous@ray-example-data/image-datasets/simple\"")\n\nfor batch in ds.iter_batches(batch_size=2, batch_format=\""numpy\""):\n    print(batch)\n\n\n{'image': "", ""start_index"": 1277, ""end_index"": 1461}, {""content"": ""\n\n\nTensorFlow\nimport ray\n\nds = ray.data.read_csv(\""s3://anonymous@air-example-data/iris.csv\"")\n\ntf_dataset = ds.to_tf(\n    feature_columns=\""sepal length (cm)\"",\n    label_columns=\""targ"", ""start_index"": 2509, ""end_index"": 2690}]",datasets/raydocs_full/data_iterating-over-data.txt
What are some techniques to optimize the shuffling of batches?,"[{""content"": ""shuffling#\nDataset.random_shuffle is slow because it\nshuffles all rows. If a full global shuffle isn\u2019t required, you can shuffle a subset of\nrows up to a provided buffer size during iteration by specifying\nlocal_shuffle_buffer_size. While this isn\u2019t a true global shuffle like\nrandom_shuffle, it\u2019s more performant because it doesn\u2019t require excessive data\nmovement. For more details about these options, see Shuffling Data.\n\nTip\nTo configure local_shuffle_buffer_size, choose the smallest value that achieves\nsufficient randomness. Higher values result in more randomness at the cost of slower\niteration. See Local shuffle when iterating over batches\non how to diagnose slowdowns.\n\n\n\n\nNumPy\nimport ray\n\nds = "", ""start_index"": 3093, ""end_index"": 3801}]",datasets/raydocs_full/data_iterating-over-data.txt
What are the benefits of passing object references in ray tasks?,"[{""content"": ""ray.get(reduce.remote(rollout))\n\n\n\n\nBetter approach:\n# Don't need ray.get here.\nrollout_obj_ref = generate_rollout.remote()\n# Rollout object is passed by reference.\nreduced = ray.get(reduce.remote(rollout_obj_ref))\n\n\n\n\nNotice in the anti-pattern example, we call ray.get() "", ""start_index"": 913, ""end_index"": 1186}, {""content"": """", ""start_index"": 2099, ""end_index"": 2393}]",datasets/raydocs_full/ray-core_patterns_unnecessary-ray-get.txt
What are the consequences of calling ray.get() unnecessarily?,"[{""content"": ""\n\nAnti-pattern: Calling ray.get unnecessarily harms performance#\nTLDR: Avoid calling ray.get() unnecessarily for intermediate steps. Work with object references directly, and only call ray.get() at the end to get the final result.\nWhen ray.get() is called, objects must be transferred to the worker/node that calls ray.get(). If you don\u2019t need to manipulate the object, you probably don\u2019t need to call ray.get() on it!\nTypically, it\u2019s best practice to wait as long as possible before calling ray.get(), or even design your program to avoid having to call ray.get() at all.\n\nCode "", ""start_index"": 0, ""end_index"": 579}, {""content"": ""ray.get(reduce.remote(rollout_obj_ref))\n\n\n\n\nNotice in the anti-pattern example, we call ray.get() which forces us to transfer the large rollout to the driver, then again to the reduce worker.\nIn the fixed version, we only pass the reference to the object to the reduce task.\nThe reduce worker will implicitly call ray.get() to fetch the actual rollout data directly from the generate_rollout "", ""start_index"": 1088, ""end_index"": 1480}]",datasets/raydocs_full/ray-core_patterns_unnecessary-ray-get.txt
What is the difference between the anti-pattern and better approach in handling ray.get()?,"[{""content"": ""ray.get(reduce.remote(rollout_obj_ref))\n\n\n\n\nNotice in the anti-pattern example, we call ray.get() which forces us to transfer the large rollout to the driver, then again to the reduce worker.\nIn the fixed version, we only pass the reference to the object to the reduce task.\nThe reduce worker will implicitly call ray.get() to fetch the actual rollout data directly from the generate_rollout worker, avoiding the extra copy to the driver.\nOther ray.get() related anti-patterns "", ""start_index"": 1088, ""end_index"": 1565}]",datasets/raydocs_full/ray-core_patterns_unnecessary-ray-get.txt
What methods can be overridden in RLModule for specific forward pass logic?,"[{""content"": ""of training and evaluation, override the following methods instead:\nFor distinct action computation logic w/o exploration, override the\nself._forward_inference() method.\nFor distinct action "", ""start_index"": 297, ""end_index"": 487}, {""content"": ""The input batch.\n**kwargs \u2013 Additional keyword arguments.\n\n\nReturns:\nThe output of the forward pass.\n\n\n\n\n"", ""start_index"": 694, ""end_index"": 882}, {""content"": """", ""start_index"": 1181, ""end_index"": 1358}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule._forward.txt
What is the general purpose of the RLModule._forward method?,"[{""content"": ""**kwargs) \u2192 Dict[str, Any][source]#\nGeneric forward pass method, used in all phases of training and "", ""start_index"": 99, ""end_index"": 199}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule._forward.txt
What parameters does RLModule._forward accept?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModule._forward#\n\n\nRLModule._forward(batch: Dict[str, Any], "", ""start_index"": 0, ""end_index"": 99}, {""content"": ""method.\n\nParameters:\n\nbatch \u2013 The input batch.\n**kwargs \u2013 Additional keyword "", ""start_index"": 664, ""end_index"": 741}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule._forward.txt
What is the detailed process for creating a Docker image with the Faker package?,"[{""content"": ""rayproject/ray:2.9.0 image hosted by rayproject/ray.\nYou can extend these images and add your own dependencies to them by using them as a base layer in a Dockerfile. For instance, the working example application uses Ray 2.9.0 and Faker 18.13.0. You can create a Dockerfile that extends the rayproject/ray:2.9.0 by adding the Faker package:\n# File name: Dockerfile\nFROM rayproject/ray:2.9.0\n\nRUN pip install Faker==18.13.0\n\n\nIn general, the rayproject/ray images "", ""start_index"": 1242, ""end_index"": 1705}, {""content"": ""on of the package. You can also replace latest with a specific version if you prefer.\n\n\nAdding your Serve application to the Docker image#\nDuring development, it\u2019s useful to package your Serve application into a zip file and pull it i"", ""start_index"": 2389, ""end_index"": 2623}]",datasets/raydocs_full/serve_production-guide_docker.txt
How can you set up a Docker image to include a Serve application for Ray?,"[{""content"": ""commands inside the Dockerfile to install the example Serve application code in your image:\n# File name: Dockerfile\nFROM rayproject/ray:2.9.0\n\nRUN pip install Faker==18.13.0\n\n# Set the working dir for the container to /serve_app\nWORKDIR /serve_app\n\n# Copies the local `fake.py` file into the WORKDIR\nCOPY fake.py /serve_app/fake.py\n\n\nKubeRay starts Ray with the ray start command inside the "", ""start_index"": 2906, ""end_index"": 3297}]",datasets/raydocs_full/serve_production-guide_docker.txt
What steps are involved in deploying a custom Docker image in KubeRay?,"[{""content"": ""custom Docker images in KubeRay#\nRun these custom Docker images in KubeRay by adding them to the RayService config. Make the following changes:\n\nSet the rayVersion in the rayClusterConfig to the Ray version used in your custom Docker image.\nSet the ray-head container\u2019s image to the custom image\u2019s name on Dockerhub.\nSet the ray-worker container\u2019s image to the custom image\u2019s name on Dockerhub.\nUpdate the  serveConfigV2 field to remove any runtime_env dependencies that are in the container.\n\nA pre-built version of this image is available at "", ""start_index"": 3768, ""end_index"": 4312}]",datasets/raydocs_full/serve_production-guide_docker.txt
What are the steps for verifying if Ray is initialized in different programming languages?,"[{""content"": ""the is_initialized API.\n\n\n\nPython\nimport ray\nray.init()\nassert ray.is_initialized()\n\nray.shutdown()\nassert not ray.is_initialized()\n\n\n\n\n\nJava\nimport "", ""start_index"": 2249, ""end_index"": 2398}, {""content"": ""rker nodes. Underneath the hood, it automatically calls ray start to create a Ray cluster.\nYour code only needs to execute on one machine in the cluster (usually the head node). Read more about running programs on a Ray cluster.\nTo connect to the Ray cluster, call ray.init from one"", ""start_index"": 4289, ""end_index"": 4571}, {""content"": """", ""start_index"": 6512, ""end_index"": 6800}]",datasets/raydocs_full/ray-core_starting-ray.txt
What are the three ways to start the Ray runtime?,"[{""content"": ""single server, or multiple servers.\nThere are three ways of starting the Ray runtime:\n\nImplicitly via ray.init() (Starting Ray on a single machine)\nExplicitly via CLI (Starting Ray via the CLI (ray start))\nExplicitly via the cluster launcher (Launching a Ray cluster (ray up))\n\nIn all cases, "", ""start_index"": 485, ""end_index"": 777}]",datasets/raydocs_full/ray-core_starting-ray.txt
What happens to the Ray runtime when the process calling ray.init() terminates?,"[{""content"": ""`ray::Init()` is called.\nray::Init()\n\n\n\n\nWhen the process calling ray.init() terminates, the Ray runtime will also terminate. To explicitly stop or restart Ray, use the shutdown "", ""start_index"": 1717, ""end_index"": 1895}]",datasets/raydocs_full/ray-core_starting-ray.txt
What config settings can be set for fault tolerance in this AlgorithmConfig?,"[{""content"": ""config's evaluation settings.\n\nexperimental\nSets the config's experimental settings.\n\nfault_tolerance\nSets the config's fault tolerance settings.\n\nframework\nSets the config's "", ""start_index"": 1802, ""end_index"": 1977}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.txt
What methods are available to manipulate the configuration of an AlgorithmConfig?,"[{""content"": ""param_space=config.to_dict())\n\n\nMethods\n\n\n__init__\nInitializes an AlgorithmConfig instance.\n\napi_stack\nSets the config's API stack settings.\n\nbuild_algo\nBuilds an Algorithm from this AlgorithmConfig (or a copy thereof).\n\nbuild_learner\nBuilds and returns a new Learner object based on settings in self.\n\nbuild_learner_group\nBuilds and returns a new LearnerGroup object based on settings in self.\n\ncallbacks\nSets the callbacks configuration.\n\ncheckpointing\nSets the config's checkpointing settings.\n\ncopy\nCreates a deep copy of this config and (un)freezes if necessary.\n\ndebugging\nSets the config's debugging settings.\n\nenv_runners\nSets the rollout worker configuration.\n\nenvironment\nSets the config's RL-environment settings.\n\nevaluation\nSets the config's evaluation settings.\n\nexperimental\nSets the config's experimental settings.\n\nfault_tolerance\nSets the config's fault tolerance settings.\n\nframework\nSets the config's DL framework settings.\n\nfreeze\nFreezes this config object, such that no attributes can be set "", ""start_index"": 1056, ""end_index"": 2071}, {""content"": "".\n\nget_evaluation_config_object\nCreates a full AlgorithmConfig object from self.evaluation_config.\n\nget_multi_agent_setup\nCompiles complete multi-agent config (dict) from the"", ""start_index"": 2510, ""end_index"": 2684}, {""content"": ""ts all settings into a legacy config dict for backward compatibility.\n\ntraining\nSets the training related configuration.\n\nupdate_from_dict\nModifies this AlgorithmConfig via the provided python config dict.\n\nvalidate\nValidates all values in this config.\n\nvalidate_train_batch_size_vs_rollout_fragment_length\nDetects mismatches for train_batch_size vs rollout_fragm"", ""start_index"": 3870, ""end_index"": 4233}, {""content"": """", ""start_index"": 7005, ""end_index"": 7178}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.txt
What methods in the AlgorithmConfig can create new instances or modify existing settings?,"[{""content"": ""param_space=config.to_dict())\n\n\nMethods\n\n\n__init__\nInitializes an AlgorithmConfig instance.\n\napi_stack\nSets the config's API stack settings.\n\nbuild_algo\nBuilds an Algorithm from "", ""start_index"": 1056, ""end_index"": 1234}, {""content"": ""zes if necessary.\n\ndebugging\nSets the config's debugging settings.\n\nenv_runners\nSets the rollout worker configuration.\n\nenvironment\nSets the config's RL-environment settings.\n\nevalu"", ""start_index"": 1606, ""end_index"": 1787}, {""content"": ""n self.\n\nget_multi_rl_module_spec\nReturns the MultiRLModuleSpec based on the given env/spaces.\n\nget_rl_module_spec\nReturns the RLModuleSpec based on the given env/spaces.\n\nge"", ""start_index"": 2698, ""end_index"": 2872}, {""content"": """", ""start_index"": 5644, ""end_index"": 5834}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.txt
What method in ray.rllib.core.rl_module.rl_module.RLModule should be overridden for generic behavior across all training and evaluation phases?,"[{""content"": ""exploratory\naction computation behavior. If you have only one generic behavior for all\nphases of training and evaluation, override self._forward() instead.\nBy default, this calls the generic "", ""start_index"": 288, ""end_index"": 479}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.txt
What is the purpose of the RLModule._forward_exploration method in ray.rllib?,"[{""content"": ""batch: Dict[str, Any], **kwargs) \u2192 Dict[str, Any][source]#\nForward-pass used for action computation "", ""start_index"": 100, ""end_index"": 200}, {""content"": ""action computation behavior. If you have only one generic behavior for all\nphases of training and evaluation, override self._forward() instead.\nBy default, this calls the generic self._"", ""start_index"": 300, ""end_index"": 485}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.txt
When should RLModule._forward_exploration be overridden in ray.rllib?,"[{""content"": ""batch: Dict[str, Any], **kwargs) \u2192 Dict[str, Any][source]#\nForward-pass used for action computation with exploration behavior.\nOverride this method only, if you need specific behavior for exploratory\naction computation behavior. If you have only one generic behavior for all\nphases of "", ""start_index"": 100, ""end_index"": 385}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.txt
How can a user manually specify resources when starting a single node Ray cluster using ray.init()?,"[{""content"": ""depending on how you start the Ray cluster:\n\n\n\nray.init()\nIf you are using ray.init() to start a single node Ray cluster, you can do the following to manually specify node resources:\n# This will start a Ray node with 3 logical cpus, 4 logical gpus,\n# 1 special_hardware resource and 1 custom_label resource.\nray.init(num_cpus=3, num_gpus=4, resources={\""special_hardware\"": 1, \""custom_label\"": 1})\n\n\n\n\n\nray start\nIf you are using ray start to start a Ray node, you can run:\nray "", ""start_index"": 5011, ""end_index"": 5486}]",datasets/raydocs_full/ray-core_scheduling_resources.txt
What is the default setting for logical CPU resources when starting Ray?,"[{""content"": ""been started on a node.\n\n\nNumber of logical CPUs (``num_cpus``): Set to the number of CPUs of the machine/container.\nNumber of logical GPUs (``num_gpus``): Set to the number of GPUs of the "", ""start_index"": 4355, ""end_index"": 4544}]",datasets/raydocs_full/ray-core_scheduling_resources.txt
What are the ways to start a Ray cluster as detailed in the text?,"[{""content"": ""several ways to do that depending on how you start the Ray cluster:\n\n\n\nray.init()\nIf you are using ray.init() to start a single node Ray cluster, you can do the following to manually specify node "", ""start_index"": 4987, ""end_index"": 5183}, {""content"": ""an set the resources field in the yaml file:\navailable_node_types:\n  head:\n    ...\n    resources:\n      CPU: 3\n      GPU: 4\n      special_hardware: 1\n      custom_label: 1\n\n\n\n\n\nKubeRay\nIf you are using KubeRay to start a Ray cluster, you can set the rayStartParams fiel"", ""start_index"": 5647, ""end_index"": 5916}, {""content"": ""for running.\n(This means, by default, actors cannot get scheduled on a zero-cpu node, but an infinite number of them can run on any non-zero cpu node.\nThe default resource requirements for ac"", ""start_index"": 6480, ""end_index"": 6671}, {""content"": ""m_gpus=0).remote()\n\n\n\n\n\nJava\n// Specify required resources.\nRay.task(MyRayApp::myFunction).setResource(\""CPU\"", 1.0).setResource(\""GPU\"", 1.0).setResource(\""special_hardware\"", 1.0).remote();\n\nR"", ""start_index"": 7591, ""end_index"": 7779}]",datasets/raydocs_full/ray-core_scheduling_resources.txt
How does the Ray autoscaler determine when to add nodes to the cluster?,"[{""content"": ""up and down based on resource demand.\nThe autoscaler does this by adjusting the number of nodes in the cluster based on the resources required by tasks, actors or placement groups.\nNote that the "", ""start_index"": 195, ""end_index"": 390}, {""content"": ""queued. The autoscaler adds nodes to satisfy resource demands in this queue.\nThe autoscaler also removes nodes after they become idle for some time.\nA node is considered idle if it has no active tasks, actors, or objects.\n\nTip\nWhen to use Autoscaling?\nAutoscaling can reduce workload costs, b"", ""start_index"": 675, ""end_index"": 967}]",datasets/raydocs_full/cluster_vms_user-guides_configuring-autoscaling.txt
What factors dictate the removal of nodes by the Ray autoscaler?,"[{""content"": ""queued. The autoscaler adds nodes to satisfy resource demands in this queue.\nThe autoscaler also removes nodes after they become idle for some time.\nA node is considered idle if it has no active tasks, actors, or objects.\n\nTip\nWhen to use Autoscaling?\nAutoscaling can reduce workload costs, but "", ""start_index"": 675, ""end_index"": 970}, {""content"": """", ""start_index"": 3857, ""end_index"": 4047}]",datasets/raydocs_full/cluster_vms_user-guides_configuring-autoscaling.txt
What is the main purpose of the Ray autoscaler?,"[{""content"": ""cluster launcher.\nThe Ray autoscaler is a Ray cluster process that automatically scales a cluster up and down based on resource demand.\nThe autoscaler does this by adjusting the number of nodes in "", ""start_index"": 97, ""end_index"": 294}]",datasets/raydocs_full/cluster_vms_user-guides_configuring-autoscaling.txt
What are the conditions for AlgorithmConfig.is_multi_agent to return True?,"[{""content"": ""AlgorithmConfig.is_multi_agent: bool#\nReturns whether this config specifies a multi-agent setup.\n\nReturns:\nTrue, if a) >1 policies defined OR b) 1 policy defined, but its ID is NOT\nDEFAULT_POLICY_ID.\n\n\n\n\n"", ""start_index"": 83, ""end_index"": 360}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.txt
What does the property AlgorithmConfig.is_multi_agent indicate?,"[{""content"": ""AlgorithmConfig.is_multi_agent: bool#\nReturns whether this config specifies a multi-agent setup.\n\nReturns:\nTrue, if a) >1 policies defined OR b) 1 policy defined, but its ID is "", ""start_index"": 83, ""end_index"": 260}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.txt
What type of configuration does AlgorithmConfig.is_multi_agent determine?,"[{""content"": ""AlgorithmConfig.is_multi_agent: bool#\nReturns whether this config specifies a multi-agent setup.\n\nReturns:\nTrue, if a) >1 policies defined OR b) 1 policy defined, but its ID is "", ""start_index"": 83, ""end_index"": 260}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.is_multi_agent.txt
What are the steps involved in evaluating the performance of hyperparameters in the given context?,"[{""content"": ""hp\n\n\n\n\n\nLet\u2019s start by defining a simple evaluation function.\nWe artificially sleep for a bit (0.1 seconds) to simulate a long-running ML experiment.\nThis setup assumes that we\u2019re running multiple steps of an experiment and try to tune two hyperparameters,\nnamely width and height.\n\n\ndef "", ""start_index"": 2814, ""end_index"": 3102}, {""content"": ""TuneConfig(\n        metric=\""mean_loss\"",\n        mode=\""min\"",\n        search_alg=algo,\n        num_samples=num_samples,\n    ),\n    param_space=search_config,\n)\nresults = tuner.fit()\n\n\n\n\n\n\n\nTune Status\n\n\nCurrent time:2025-02-18 13:14:59\nRunning for: 00:00:36.03        \nMemory:      22.1/36.0 GiB      \n\n\n\n\n\nSystem Info\n      Using FIFO scheduling algorithm.Logical resour"", ""start_index"": 5124, ""end_index"": 5493}]",datasets/raydocs_full/tune_examples_hyperopt_example.txt
What are the key components included in the imports required for the presented hyperparameter tuning example?,"[{""content"": ""source\nHide code cell source\n\n\nimport time\n\nimport ray\nfrom ray import tune\nfrom ray.tune.search import ConcurrencyLimiter\nfrom ray.tune.search.hyperopt import HyperOptSearch\nfrom hyperopt import hp\n\n\n\n\n\nLet\u2019s start by defining a simple evaluation function.\nWe artificially sleep for a bit (0.1 "", ""start_index"": 2618, ""end_index"": 2913}]",datasets/raydocs_full/tune_examples_hyperopt_example.txt
What configuration was used for running the hyperparameter tuning experiment?,"[{""content"": ""hyperparameters may be difficult to find in a short amount of time.\n\n\nsearch_config = {\n    \""steps\"": 100,\n    \""width\"": tune.uniform(0, 20),\n    \""height\"": tune.uniform(-100, 100),\n    \""activation\"": tune.choice([\""relu\"", \""tanh\""])\n}\n\n\n\n\nFinally, we run the experiment to \""min\""imize the "", ""start_index"": 4547, ""end_index"": 4829}, {""content"": "" width     loss  iter  total time (s)  iterations\n\n\nobjective_5b05c00aTERMINATED127.0.0.1:50205relu          2         100 1        1.11743    100         10.335           99\nobjective_b813f49dTERMINATED127.0.0.1:50207tanh          2         100 4        0.446305   "", ""start_index"": 5621, ""end_index"": 5887}]",datasets/raydocs_full/tune_examples_hyperopt_example.txt
What does the component parameter control in the Algorithm.restore_from_path method?,"[{""content"": ""component arg is provided, path refers to a checkpoint of a\nsubcomponent of self, thus allowing the user to load only the subcomponent\u2019s\nstate into self without affecting any of the other state "", ""start_index"": 197, ""end_index"": 391}, {""content"": ""ns unchanged in this\ncase.\nfilesystem \u2013 PyArrow FileSystem to use to access data at the path. If not\nspecified, this is inferred from the URI scheme of path.\n**kwargs \u2013 Forward compatibility k"", ""start_index"": 1351, ""end_index"": 1543}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.txt
What files should the path structure contain when using Algorithm.restore_from_path?,"[{""content"": ""given path should have the following structure and contain the following\nfiles:\npath/\n    [component1]/\n        [component1 subcomponentA]/\n            ...\n        [component1 subcomponentB]/\n            ...\n    [component2]/\n            ...\n    [cls.METADATA_FILE_NAME] (json)\n    [cls.STATE_FILE_NAME] (pkl|msgpack)\n\n\nNote that the self.METADATA_FILE_NAME file is not "", ""start_index"": 586, ""end_index"": 956}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.txt
What happens if the component parameter is not provided in Algorithm.restore_from_path?,"[{""content"": ""*args, **kwargs)[source]#\nRestores the state of the implementing class from the given path.\nIf the component arg is provided, path refers to a checkpoint of a\nsubcomponent of self, thus allowing the user to load only the subcomponent\u2019s\nstate into self without affecting any of the other state "", ""start_index"": 98, ""end_index"": 391}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.restore_from_path.txt
What programming languages do not yet have an implementation of the Actor Pool feature?,"[{""content"": ""package reference for more information.\n\n\n\nJava\nActor pool hasn\u2019t been implemented in Java yet.\n\n\n\nC++\nActor pool hasn\u2019t been implemented in C++ yet.\n\n\n\n\nMessage passing using Ray "", ""start_index"": 558, ""end_index"": 738}, {""content"": ""work {next_item}\"")\n    except Empty:\n        pass\n\n\n[queue.put(i) for i in range(10)]\npri"", ""start_index"": 1207, ""end_index"": 1296}]",datasets/raydocs_full/ray-core_actors_actor-utils.txt
What is the function of the double method within the Actor class in Python?,"[{""content"": ""fixed pool of actors.\nimport ray\nfrom ray.util import ActorPool\n\n\n@ray.remote\nclass Actor:\n    def double(self, n):\n        return n * 2\n\n\na1, a2 = Actor.remote(), Actor.remote()\npool = "", ""start_index"": 184, ""end_index"": 370}]",datasets/raydocs_full/ray-core_actors_actor-utils.txt
How does Python's ActorPool manage task scheduling among actors?,"[{""content"": ""\n\nUtility Classes#\n\nActor Pool#\n\n\n\nPython\nThe ray.util module contains a utility class, ActorPool.\nThis class is similar to multiprocessing.Pool and lets you schedule Ray tasks over a fixed pool of actors.\nimport ray\nfrom ray.util import ActorPool\n\n\n@ray.remote\nclass Actor:\n    def "", ""start_index"": 0, ""end_index"": 283}]",datasets/raydocs_full/ray-core_actors_actor-utils.txt
What is the purpose of the reduce_type parameter in the convert_to_numpy function?,"[{""content"": ""numpy types.\nreduce_type \u2013 Whether to automatically reduce all float64 and int64 data\ninto float32 and int32 data, respectively.\n\n\nReturns:\nA new struct with the same structure as x, but with "", ""start_index"": 482, ""end_index"": 674}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.convert_to_numpy.txt
What types of data structures can be converted using the convert_to_numpy function?,"[{""content"": ""\n\nray.rllib.utils.numpy.convert_to_numpy#\n\n\nray.rllib.utils.numpy.convert_to_numpy(x: numpy.array | jnp.ndarray | tf.Tensor | torch.Tensor | dict | tuple, reduce_type: bool = True) \u2192 numpy.array | "", ""start_index"": 0, ""end_index"": 197}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.convert_to_numpy.txt
What is returned by the ray.rllib.utils.numpy.convert_to_numpy function?,"[{""content"": ""and int32 data, respectively.\n\n\nReturns:\nA new struct with the same structure as x, but with all\nvalues converted to numpy arrays (on CPU).\n\n\n\n\n"", ""start_index"": 581, ""end_index"": 774}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.convert_to_numpy.txt
What are the valid values for the deletion policy of a RayJob?,"[{""content"": ""RayJob.\ndeletionPolicy (Optional, alpha in v1.3.0): Indicates what resources of the RayJob are deleted upon job completion. Valid values are DeleteCluster, DeleteWorkers, DeleteSelf or "", ""start_index"": 5016, ""end_index"": 5201}]",datasets/raydocs_full/cluster_kubernetes_getting-started_rayjob-quick-start.txt
What happens when the environment variable DELETE_RAYJOB_CR_AFTER_JOB_FINISHES is set to true for the KubeRay operator?,"[{""content"": ""reason.\nDELETE_RAYJOB_CR_AFTER_JOB_FINISHES (Optional, added in version 1.2.0): Set this environment variable for the KubeRay operator, not the RayJob resource. If you set this environment variable to true, the RayJob custom resource itself is deleted if you also set shutdownAfterJobFinishes to true. Note that KubeRay deletes all resources created by the RayJob, "", ""start_index"": 4374, ""end_index"": 4739}]",datasets/raydocs_full/cluster_kubernetes_getting-started_rayjob-quick-start.txt
What is the impact of setting the parameter shutdownAfterJobFinishes to true in a RayJob configuration?,"[{""content"": ""tdown.yaml\n\n\n\n\nThe ray-job.shutdown.yaml defines a RayJob custom resource with shutdownAfterJobFinishes: true and ttlSecondsAfterFinished: 10.\nHence, the KubeRay operator deletes the RayCluster 10 seconds after the Ray job finishes. Note that the submitter job isn\u2019t "", ""start_index"": 9604, ""end_index"": 9871}]",datasets/raydocs_full/cluster_kubernetes_getting-started_rayjob-quick-start.txt
What are the major components listed under Ray Core API?,"[{""content"": ""\n\nRay Core API#\n\n\nCore API\nTasks\nActors\nObjects\nRuntime Context\nCross Language\n\n\nScheduling API\nScheduling Strategy\nPlacement Group\n\n\nRuntime Env API\n\n\nUtility\nCustom Metrics\nDebugging\n\n\nExceptions\n\n\nRay Core CLI\nDebugging applications\nUsage Stats\n\n\nState CLI\nState\nLog\n\n\nState API\nState Python SDK\nState APIs Schema\nState APIs Exceptions\n\n\n\n\n\n"", ""start_index"": 0, ""end_index"": 356}]",datasets/raydocs_full/ray-core_api_index.txt
What features are included under the Scheduling API of Ray Core?,"[{""content"": ""API\nScheduling Strategy\nPlacement Group\n\n\nRuntime Env API\n\n\nUtility\nCustom "", ""start_index"": 92, ""end_index"": 167}]",datasets/raydocs_full/ray-core_api_index.txt
What are the different APIs listed under Ray Core?,"[{""content"": ""\n\nRay Core API#\n\n\nCore API\nTasks\nActors\nObjects\nRuntime Context\nCross Language\n\n\nScheduling API\nScheduling Strategy\nPlacement Group\n\n\nRuntime Env API\n\n\nUtility\nCustom Metrics\nDebugging\n\n\nExceptions\n\n\nRay Core CLI\nDebugging applications\nUsage Stats\n\n\nState CLI\nState\nLog\n\n\nState API\nState Python SDK\nState APIs Schema\nState APIs Exceptions\n\n\n\n\n\n"", ""start_index"": 0, ""end_index"": 356}]",datasets/raydocs_full/ray-core_api_index.txt
What is the default storage unit for the ReservoirReplayBuffer in ray.rllib?,"[{""content"": ""ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer(capacity: int = 10000, storage_unit: str = 'timesteps', **kwargs)[source]#\nBases: ReplayBuffer\nThis buffer implements "", ""start_index"": 88, ""end_index"": 283}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.txt
What is the default capacity of the ReservoirReplayBuffer in ray.rllib?,"[{""content"": ""ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer(capacity: int = 10000, "", ""start_index"": 88, ""end_index"": 188}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.txt
What are some methods implemented by ReservoirReplayBuffer in ray.rllib?,"[{""content"": ""a reservoir\u201d.\nMethods\n\n\n__init__\nInitializes a ReservoirBuffer instance.\n\nadd\nAdds a batch of experiences or other data to this buffer.\n\napply\nCalls the given function with this Actor instance.\n\nget_host\nReturns the computer's network name.\n\nget_state\nReturns all local state.\n\nping\nPing the actor.\n\nsample\nSamples num_items items from this buffer.\n\nset_state\nRestores all local state to the provided state.\n\nstats\nReturns the stats of this buffer.\n\n\n\n\n\n"", ""start_index"": 382, ""end_index"": 851}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.txt
What methods allow retrieval of checkpoints from the Result object?,"[{""content"": ""min(df[\""loss\""]))\n\n\n\n\n\nRetrieving checkpoints#\nYou can retrieve checkpoints reported to Ray Train from the Result\nobject.\nCheckpoints contain all the information that is needed\nto restore the "", ""start_index"": 1268, ""end_index"": 1459}, {""content"": ""ll be a\n(nested) subdirectory within that path, usually\nof the form TrainerName_date-string/TrainerName_id_00000_0_....\nThe result also contains a pyarrow.fs.Fi"", ""start_index"": 3005, ""end_index"": 3165}, {""content"": """", ""start_index"": 5188, ""end_index"": 5475}]",datasets/raydocs_full/train_user-guides_results.txt
What types of data can Result object components report after training?,"[{""content"": ""object contains, among other information:\n\nThe last reported checkpoint (to load the model) and its attached metrics\nError messages, if any errors occurred\n\n\nViewing metrics#\nYou can retrieve "", ""start_index"": 96, ""end_index"": 288}, {""content"": ""rted metrics that were attached to a checkpoint from the Result object.\nCommon metrics inclu"", ""start_index"": 292, ""end_index"": 384}]",datasets/raydocs_full/train_user-guides_results.txt
How can one access the storage location of the training run using the Result object?,"[{""content"": ""checkpointing.\n\n\n\n\nAccessing storage location#\nIf you need to retrieve the results later, you can get the storage location\nof the training run with Result.path.\nThis path will correspond to the storage_path you configured\nin the RunConfig. It will be a\n(nested) subdirectory within that path, usually\nof the form TrainerName_date-string/TrainerName_id_00000_0_....\nThe result also contains a pyarrow.fs.FileSystem that can be used to\naccess the storage location, which is useful if the path is on cloud storage.\nresult_path: str = result.path\nresult_filesystem: pyarrow.fs.FileSystem = result.filesystem\n\nprint(f\""Results location (fs, path) = ({result_filesystem}, "", ""start_index"": 2760, ""end_index"": 3425}]",datasets/raydocs_full/train_user-guides_results.txt
What are the key features of Ray as a machine learning platform?,"[{""content"": ""oduction. With Ray and its libraries, the same Python code scales seamlessly from a laptop to a large cluster.\n2. Unified ML API and Runtime: Ray\u2019s APIs enables swapping between popular "", ""start_index"": 767, ""end_index"": 953}, {""content"": ""large cluster.\n2. Unified ML API and Runtime: Ray\u2019s APIs enables swapping between popular frameworks, such as XGBoost, PyTorch, and Hugging Face, with minimal code changes. Everything from training to serving runs on a single runtime (Ray + KubeRay).\n3. Open and Extensible: Ray is fully "", ""start_index"": 863, ""end_index"": 1151}, {""content"": ""ly open-source and can run on any cluster, cloud, or Kubernetes. Build custom components and integrations on top of scalable developer APIs.\n\n\nExample ML Platforms built on Ray#\nMerlin is S"", ""start_index"": 1148, ""end_index"": 1337}]",datasets/raydocs_full/ray-air_getting-started.txt
What are the three areas where Ray simplifies the machine learning ecosystem?,"[{""content"": ""for ML Infrastructure?#\nRay\u2019s AI libraries simplify the ecosystem of machine learning frameworks, platforms, and tools, by providing a seamless, unified, and open experience for scalable ML:\n1. "", ""start_index"": 487, ""end_index"": 681}]",datasets/raydocs_full/ray-air_getting-started.txt
"What are the unique features of Ray Train, Ray Data, and Ray Serve?","[{""content"": ""teams looking to simplify their ML platform.\nRay\u2019s libraries such as Ray Train, Ray Data, and Ray Serve can be used to compose end-to-end ML workflows, providing features and APIs for\ndata preprocessing as part of training, and transitioning from training to serving.\n\nWhy Ray for ML "", ""start_index"": 210, ""end_index"": 494}]",datasets/raydocs_full/ray-air_getting-started.txt
What are the detailed steps to serve a MobileNet image classifier on Kubernetes using Ray Serve?,"[{""content"": ""Create a Kubernetes cluster with Kind#\nkind create cluster --image=kindest/node:v1.26.0\n\n\n\n\nStep 2: "", ""start_index"": 188, ""end_index"": 288}, {""content"": ""KubeRay version v0.6.0 or later to use this feature.\n\n\nStep 3: Install a RayService#\n# Create a RayService\nkubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/v1.3.0/ray-operato"", ""start_index"": 476, ""end_index"": 672}, {""content"": ""es python-multipart in the runtime environment.\n\n\n\nStep 4: Forward the port for Ray Serve#\n# Wait for the RayService to be ready to serve requests\nkubectl describe rayservice/rayservice-mobilenet\n#  Conditions:\n#   Last Transition Time:  2025-02-13T02:29:26Z\n#   Message:               Number of serve endpoints "", ""start_index"": 960, ""end_index"": 1272}, {""content"": """", ""start_index"": 1947, ""end_index"": 2526}, {""content"": """", ""start_index"": 3513, ""end_index"": 3698}]",datasets/raydocs_full/cluster_kubernetes_examples_mobilenet-rayservice.txt
What Kubernetes versions are required to utilize KubeRay operator for serving a MobileNet image classifier?,"[{""content"": ""the Helm repository.\nNote that the YAML file in this example uses serveConfigV2. You need KubeRay version v0.6.0 or later to use this feature.\n\n\nStep 3: Install a RayService#\n# Create a "", ""start_index"": 386, ""end_index"": 572}]",datasets/raydocs_full/cluster_kubernetes_examples_mobilenet-rayservice.txt
What are the image dependencies required in the YAML file for serving the MobileNet image classifier on Kubernetes?,"[{""content"": "".mobilenet.yaml\n\n\n\nThe mobilenet.py file needs tensorflow as a dependency. Hence, the YAML file "", ""start_index"": 700, ""end_index"": 796}, {""content"": ""#   Type:                  Ready\n\n# Forward the port for Ray Serve service\nkubectl port-forward svc/rayservice-mobilenet-serve-svc 8000\n\n\n\n\nStep 5: Send a request to the ImageClassifier#\n"", ""start_index"": 1400, ""end_index"": 1587}, {""content"": """", ""start_index"": 2287, ""end_index"": 2483}]",datasets/raydocs_full/cluster_kubernetes_examples_mobilenet-rayservice.txt
What are the specific types of output batches that the 'map_batches' function can handle in Ray Data?,"[{""content"": "".map_batches(increase_brightness)\n)\n\n\n\nConfiguring batch format#\nRay Data represents batches as dicts of NumPy ndarrays or pandas DataFrames. By\ndefault, Ray Data represents batches as dicts of "", ""start_index"": 2929, ""end_index"": 3123}, {""content"": ""ct[str, np.ndarray]]:\n    # yield the same batch multiple times\n    for _ in range(10):\n        yield batch\n\n\n\n\nConfiguring batch size#\nIncreasing batch_size improves the performance of vectorized transformations like\nNumPy functions and model inference. However, if your batch size is too large, your\nprogram might run out of memory. If you encounter an out-of-memory error, decrease your\nba"", ""start_index"": 4950, ""end_index"": 5342}]",datasets/raydocs_full/data_transforming-data.txt
How does the batch size impact system performance in Ray Data processing?,"[{""content"": ""batch_size improves the performance of vectorized transformations like\nNumPy functions and model inference. However, if your batch size is too large, your\nprogram might run out of memory. If you encounter an out-of-memory error, decrease your\nbatch_size.\n\n\n\nOrdering of rows#\nWhen transforming "", ""start_index"": 5097, ""end_index"": 5391}]",datasets/raydocs_full/data_transforming-data.txt
How can one make the execution of transforms in Ray Data preserve the order of data rows?,"[{""content"": ""by default.\nIf the order of blocks needs to be preserved/deterministic,\nyou can use sort() method, or set ray.data.ExecutionOptions.preserve_order to True.\nNote that setting this flag may negatively impact performance on larger cluster setups where stragglers are more likely.\nimport ray\n\nctx = "", ""start_index"": 5433, ""end_index"": 5728}]",datasets/raydocs_full/data_transforming-data.txt
What is the module function referenced in the text?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModule.inference_only#\n\n\nRLModule.inference_only#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule.inference_only.txt
What function name is included in the text?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModule.inference_only#\n\n\nRLModule.inference_only#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule.inference_only.txt
What is the complete name of the referenced module function in the text?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModule.inference_only#\n\n\nRLModule.inference_only#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule.inference_only.txt
How can you make the Ray Dashboard accessible via a reverse proxy?,"[{""content"": ""notes.\n\n\n\n\nRunning behind a reverse proxy#\nRay Dashboard should work out-of-the-box when accessed via a reverse proxy. API requests don\u2019t need to be proxied individually.\nAlways access the dashboard with a trailing / at the end of the URL.\nFor example, if your proxy is set up to handle requests to /ray/dashboard, view the dashboard at www.my-website.com/ray/dashboard/.\nThe dashboard sends HTTP requests with relative URL paths. Browsers handle these requests as expected when the window.location.href ends in a trailing /.\nThis is a peculiarity of how many browsers handle requests with relative URLs, despite what MDN defines as the expected behavior.\nMake your dashboard visible without a trailing / by including a rule in your reverse proxy that redirects the user\u2019s browser to /, i.e. /ray/dashboard \u2013> /ray/dashboard/.\nBelow is an example with a traefik TOML file that accomplishes this:\n[http]\n  [http.routers]\n    [http.routers.to-dashboard]\n      rule = \""PathPrefix(`/ray/dashboard`)\""\n      middlewares = [\""test-redirectregex\"", \""strip\""]\n      service = "", ""start_index"": 3407, ""end_index"": 4471}]",datasets/raydocs_full/cluster_configure-manage-dashboard.txt
What are the methods mentioned for accessing the Ray Dashboard from outside a Kubernetes cluster?,"[{""content"": ""Port forwarding \nYou can also view the dashboard from outside the Kubernetes cluster by using port-forwarding:\n$ kubectl port-forward service/${RAYCLUSTER_NAME}-head-svc 8265:8265\n# Visit "", ""start_index"": 2904, ""end_index"": 3092}, {""content"": ""d with Ingress.\n\nFor more information about configuring network access to a Ray cluster on Kubernetes, see the networking notes.\n\n\n\n\nRunning behind a reverse proxy#\nRay Dashboard should w"", ""start_index"": 3285, ""end_index"": 3472}]",datasets/raydocs_full/cluster_configure-manage-dashboard.txt
What are the default ports for Prometheus and Grafana as assumed by the Ray Dashboard?,"[{""content"": ""up Grafana.\n\n\nAlternate Prometheus host location#\nBy default, Ray Dashboard assumes Prometheus is hosted at localhost:9090. You can choose to run Prometheus on a non-default port or on a different "", ""start_index"": 8590, ""end_index"": 8787}, {""content"": ""lth)and visit it.\ncheck Head Node connection to Grafana server: add api/grafana_health to the end of Ray Dashboard URL (for example: http://127.0.0.1:8265/api/grafana_health) and visit it.\ncheck b"", ""start_index"": 10760, ""end_index"": 10956}]",datasets/raydocs_full/cluster_configure-manage-dashboard.txt
What are some resources for learning more about Ray Serve?,"[{""content"": ""patterns with Ray Serve. (Click image to enlarge.)#\n\n\nLearn more about model serving with the following resources.\n\n[Talk] Productionizing ML at Scale with Ray Serve\n[Blog] Simplify your MLOps with Ray & Ray Serve\n[Guide] Getting Started with Ray Serve\n[Guide] Model Composition in Serve\n[Gallery] Serve Examples Gallery\n[Gallery] More Serve Use Cases on the Blog\n\n\n\nHyperparameter "", ""start_index"": 1727, ""end_index"": 2109}]",datasets/raydocs_full/ray-overview_use-cases.txt
What capabilities does Ray Tune offer for hyperparameter tuning?,"[{""content"": ""Serve Use Cases on the Blog\n\n\n\nHyperparameter Tuning#\nThe Ray Tune library enables any parallel Ray workload to be run under a hyperparameter tuning algorithm.\nRunning multiple hyperparameter tuning "", ""start_index"": 2063, ""end_index"": 2262}, {""content"": ""while maintaining unified and simple APIs for a large variety of industry applications. RLlib is used by industry leaders in many different verticals, such as climate control, industrial control, manufacturing and logistics, finance, gaming, automobile, robotics, boat design, and many others.\n\n\n\nDecentralized distributed proximal polixy optimiation (DD-PPO) architecture.#\n\n\nLearn more about"", ""start_index"": 3998, ""end_index"": 4391}]",datasets/raydocs_full/ray-overview_use-cases.txt
What features does Ray Train offer for distributed training?,"[{""content"": ""Blog\n\n\n\nDistributed Training#\nThe Ray Train library integrates many distributed training frameworks under a simple Trainer API,\nproviding distributed orchestration and management capabilities out of "", ""start_index"": 2977, ""end_index"": 3176}, {""content"": ""d on the CloudSort benchmark using Ray\n[Example] Speed up your web crawler by parallelizing it with Ray\n\n\n\n"", ""start_index"": 5746, ""end_index"": 5940}]",datasets/raydocs_full/ray-overview_use-cases.txt
How does setting resources in tune.with_resources impact trial concurrency with specific CPU allocations?,"[{""content"": ""tune_config=tune.TuneConfig(num_samples=10)\n)\nresults = tuner.fit()\n\n# Fractional values are also supported, (i.e., {\""cpu\"": 0.5}).\n# If you have 4 CPUs on your machine, this will run 8 concurrent trials at a time.\ntrainable_with_resources = tune.with_resources(trainable, {\""cpu\"": 0.5})\ntuner = "", ""start_index"": 1229, ""end_index"": 1523}]",datasets/raydocs_full/tune_tutorials_tune-resources.txt
How does Ray Tune handle trials if there are not enough resources available immediately?,"[{""content"": ""specified GPU and CPU as specified by tune.with_resources to each individual trial.\nEven if the trial cannot be scheduled right now, Ray Tune will still try to start the respective placement group. If not enough resources are available, this will trigger\nautoscaling behavior if you\u2019re "", ""start_index"": 2075, ""end_index"": 2361}]",datasets/raydocs_full/tune_tutorials_tune-resources.txt
How does fractional CPU allocation affect the number of concurrent trials in Ray Tune?,"[{""content"": ""supported, (i.e., {\""cpu\"": 0.5}).\n# If you have 4 CPUs on your machine, this will run 8 concurrent trials at a time.\ntrainable_with_resources = tune.with_resources(trainable, {\""cpu\"": 0.5})\ntuner = "", ""start_index"": 1327, ""end_index"": 1523}]",datasets/raydocs_full/tune_tutorials_tune-resources.txt
What can you do with the 'result' parameter in the RLlibCallback.on_train_result method?,"[{""content"": ""metrics after traing results are available.\nresult \u2013 Dict of results returned from Algorithm.train() call.\nYou can mutate this object to add additional metrics.\nkwargs \u2013 Forward "", ""start_index"": 389, ""end_index"": 567}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result.txt
What parameters can you modify within the RLlibCallback.on_train_result method?,"[{""content"": ""None[source]#\nCalled at the end of Algorithm.train().\n\nParameters:\n\nalgorithm \u2013 Current Algorithm instance.\nmetrics_logger \u2013 The MetricsLogger object inside the Algorithm. Can be\nused to log custom "", ""start_index"": 191, ""end_index"": 389}, {""content"": ""m.train() call.\nYou can mutate this object to add additional metrics.\nkwargs \u2013 Forward compatibility placeholder.\n\n\n\n\n\n"", ""start_index"": 480, ""end_index"": 663}, {""content"": """", ""start_index"": 869, ""end_index"": 1047}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result.txt
What is achieved by utilizing the RLlibCallback.on_train_result method in the Algorithm train process?,"[{""content"": ""None[source]#\nCalled at the end of Algorithm.train().\n\nParameters:\n\nalgorithm \u2013 Current Algorithm "", ""start_index"": 191, ""end_index"": 289}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.callbacks.callbacks.RLlibCallback.on_train_result.txt
What are the necessary steps to recreate an entire Algorithm instance from a checkpoint?,"[{""content"": ""examples:\n\n\n\nCreate a new Algorithm from a checkpoint\nTo recreate an entire Algorithm\ninstance from a checkpoint, you can do the following:\n# Import the correct class to create from scratch using the checkpoint.\nfrom ray.rllib.algorithms.algorithm import Algorithm\n\n# Use the already existing checkpoint in `checkpoint_dir`.\nnew_ppo = Algorithm.from_checkpoint(checkpoint_dir)\n# Confirm the `new_ppo` matches the originally checkpointed one.\nassert new_ppo.config.env == \""Pendulum-v1\""\n\n# Continue training.\nnew_ppo.train()\n\n\n\n\n\nCreate a new RLModule from an Algorithm checkpoint\nCreating "", ""start_index"": 9100, ""end_index"": 9688}]",datasets/raydocs_full/rllib_checkpoints.txt
What information is stored in the class_and_ctor_args.pkl file used by RLlib?,"[{""content"": ""    \""ray_commit\"": ..,\n    \""checkpoint_version\"": \""2.1\""\n}\n\n\n\nThe class_and_ctor_args.pkl file stores meta information needed to construct a \u201cfresh\u201d object, without any particular state.\nThis "", ""start_index"": 6616, ""end_index"": 6805}, {""content"": ""information, as the filename suggests, contains the class of the saved object and its constructor arguments and keyword arguments.\nRLlib uses this file to create the initial new object when calling "", ""start_index"": 6805, ""end_index"": 7003}]",datasets/raydocs_full/rllib_checkpoints.txt
What file format does RLlib use to store the state information of saved objects?,"[{""content"": ""initial new object when calling from_checkpoint().\nFinally, the .._state.[pkl|msgpack] file contains the pickled or msgpacked state dict of the saved object.\nRLlib obtains this state dict, "", ""start_index"": 6971, ""end_index"": 7160}]",datasets/raydocs_full/rllib_checkpoints.txt
What types of machine learning applications are demonstrated in Ray Serve tutorials?,"[{""content"": ""integrate different modeling frameworks.ML ApplicationsServe ML ModelsServe a Stable Diffusion "", ""start_index"": 92, ""end_index"": 187}, {""content"": ""quest and Response StreamingAI AcceleratorsServe an Inference Model on AWS NeuronCores Using FastA"", ""start_index"": 279, ""end_index"": 377}, {""content"": ""model on Intel Gaudi AcceleratorIntegrationsScale a Gradio App with Ray ServeServe a Text Generato"", ""start_index"": 466, ""end_index"": 564}, {""content"": ""ApplicationsServe DeepSeek\n"", ""start_index"": 653, ""end_index"": 851}]",datasets/raydocs_full/serve_examples.txt
What types of AI accelerators does Ray Serve support for model serving?,"[{""content"": ""and Response StreamingAI AcceleratorsServe an Inference Model on AWS NeuronCores Using FastAPIServe "", ""start_index"": 285, ""end_index"": 385}, {""content"": "" DeepSeek\n"", ""start_index"": 670, ""end_index"": 766}, {""content"": """", ""start_index"": 1055, ""end_index"": 1249}]",datasets/raydocs_full/serve_examples.txt
What specific models does Ray Serve demonstrate for serving?,"[{""content"": ""integrate different modeling frameworks.ML ApplicationsServe ML ModelsServe a Stable Diffusion "", ""start_index"": 92, ""end_index"": 187}, {""content"": ""quest and Response StreamingAI AcceleratorsServe an Inference Model on AWS NeuronCores Using FastA"", ""start_index"": 279, ""end_index"": 377}, {""content"": ""model on Intel Gaudi AcceleratorIntegrationsScale a Gradio App with Ray ServeServe a Text Generato"", ""start_index"": 466, ""end_index"": 564}, {""content"": ""ApplicationsServe DeepSeek\n"", ""start_index"": 653, ""end_index"": 851}]",datasets/raydocs_full/serve_examples.txt
What are the functions available to manage a placement group in Ray?,"[{""content"": ""Group#\n\n\nray.util.placement_group\nAsynchronously creates a PlacementGroup.\n\nray.util.placement_group.get_placement_group\nGet a placement group object with a "", ""start_index"": 304, ""end_index"": 461}, {""content"": ""ously remove placement group.\n\nray.util.get_current_placement_group\nGet the current placement group which a task or actor is using.\n\n\n\n\n\n"", ""start_index"": 667, ""end_index"": 843}, {""content"": """", ""start_index"": 1206, ""end_index"": 1357}, {""content"": """", ""start_index"": 1826, ""end_index"": 1995}, {""content"": """", ""start_index"": 2516, ""end_index"": 2715}]",datasets/raydocs_full/ray-core_api_scheduling.txt
What are the types of scheduling strategies available in Ray?,"[{""content"": ""\n\nScheduling API#\n\nScheduling Strategy#\n\n\nray.util.scheduling_strategies.PlacementGroupSchedulingStrategy\nPlacement group based scheduling strategy.\n\nray.util.scheduling_strategies.NodeAffinitySchedulingStrategy\nStatic "", ""start_index"": 0, ""end_index"": 219}, {""content"": ""scheduling strategy.\n\nray.util.scheduling_strategies.NodeAffinitySchedulingStrategy\nStatic scheduling strategy used to run a task or actor on a particular node.\n\n\n\n\n\nPlacement "", ""start_index"": 128, ""end_index"": 304}]",datasets/raydocs_full/ray-core_api_scheduling.txt
What functions are utilized to check or change the state of placement groups in Ray?,"[{""content"": ""group.\n\nray.util.placement_group_table\nGet the state of the placement group from GCS.\n\nray.util.remove_placement_group\nAsynchronously remove placement "", ""start_index"": 539, ""end_index"": 690}, {""content"": """", ""start_index"": 1159, ""end_index"": 1328}]",datasets/raydocs_full/ray-core_api_scheduling.txt
What are the two methods of defining a trainable in Ray Tune?,"[{""content"": ""Trainables#\nIn short, a Trainable is an object that you can pass into a Tune run.\nRay Tune has two ways of defining a trainable, namely the Function API\nand the Class API.\nBoth are valid ways of "", ""start_index"": 902, ""end_index"": 1097}]",datasets/raydocs_full/tune_key-concepts.txt
What is the default scheduler used by Tune if none is specified?,"[{""content"": ""whether to stop the trial early or not.\nIf you don\u2019t specify a scheduler, Tune will use a first-in-first-out (FIFO) scheduler by default, which simply\npasses through the trials selected by your search algorithm in the order they were picked and does not perform any early stopping.\nIn "", ""start_index"": 11031, ""end_index"": 11316}]",datasets/raydocs_full/tune_key-concepts.txt
What are the different types of search space sampling methods provided by Tune?,"[{""content"": ""space.\nA search space defines valid values for your hyperparameters and can specify\nhow these values are sampled (e.g. from a uniform distribution or a normal\ndistribution).\nTune offers various "", ""start_index"": 3424, ""end_index"": 3618}, {""content"": ""tput of a trial run:\n== Status ==\nMemory usage on this node: 11.4/16.0 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 1/12 CPUs, 0/0 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects\nResult logdir: /Users/foo/ray_results/myexp\nNumber of trials: 1 (1 RUNNING)\n+----------------------+----------+---------------------+-----------+--------+--------+----------------+-------+\n| Trial name           | status   | loc                 |         a |      b |  score | total time (s) |  iter |\n|----------------------+----------+---------------------+-----------+--------+--------+"", ""start_index"": 5984, ""end_index"": 6563}, {""content"": ""w of all available search algorithms in Tune:\n\n\n\n\n\n\n\n\nSearchAlgorithm\nSummary\nWebsite\nCode Example\n\n\n\nRandom search/grid search\nRandom search/grid search\n\ntune_basic_example\n\nAxSearch\nBa"", ""start_index"": 9123, ""end_index"": 9309}, {""content"": ""udes a distributed implementation of Population Based Training (PBT)\nand Population Based Bandits (P"", ""start_index"": 12448, ""end_index"": 12548}]",datasets/raydocs_full/tune_key-concepts.txt
What is the purpose of the ReplayBuffer.set_state method?,"[{""content"": ""\n\nray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state#\n\n\nReplayBuffer.set_state(state: Dict[str, Any]) \u2192 None[source]#\nRestores all local state to the provided "", ""start_index"": 0, ""end_index"": 176}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.txt
How can the new state for ReplayBuffer.set_state be acquired?,"[{""content"": ""state.\n\nParameters:\nstate \u2013 The new state to set this buffer. Can be obtained by calling\nself.get_state().\n\n\nDeveloperAPI: This API may change across minor Ray releases.\n\n\n"", ""start_index"": 176, ""end_index"": 357}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.txt
What are the stability implications of using the ReplayBuffer.set_state API in Ray?,"[{""content"": ""calling\nself.get_state().\n\n\nDeveloperAPI: This API may change across minor Ray releases.\n\n\n"", ""start_index"": 257, ""end_index"": 357}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.txt
How is a Python function parallelized across a Ray cluster?,"[{""content"": ""the first use of a Ray remote API.\n\n\n\nRunning a Task#\nTasks are the simplest way to parallelize your Python functions across a Ray cluster. To create a task:\n\nDecorate your function with @ray.remote to indicate it should run remotely\nCall the function with .remote() instead of a normal function call\nUse ray.get() to retrieve the result from the returned future (Ray object reference)\n\nHere\u2019s a simple example:\n# Define the square task.\n@ray.remote\ndef square(x):\n    "", ""start_index"": 904, ""end_index"": 1373}]",datasets/raydocs_full/ray-core_walkthrough.txt
What are the steps to initialize Ray in a Python application?,"[{""content"": ""using multiple GPUs.\nSee Ray Compiled Graph for more details.\n\n\nGetting Started#\nTo get started, install Ray using pip install -U ray. For additional installation options, see Installing Ray.\nThe first step is to import and initialize Ray:\nimport ray\n\nray.init()\n\n\n\nNote\nIn recent versions of "", ""start_index"": 560, ""end_index"": 853}, {""content"": ""e the result from the returned future (Ray object reference)\n\nHere\u2019s a simple example:\n# Define the square task.\n@ray.remote\ndef square(x):\n    return x * x\n\n# Launch four parallel square tasks.\nf"", ""start_index"": 1229, ""end_index"": 1425}]",datasets/raydocs_full/ray-core_walkthrough.txt
What are some specific examples of tasks and actors in Ray?,"[{""content"": ""reference)\n\nHere\u2019s a simple example:\n# Define the square task.\n@ray.remote\ndef square(x):\n    return x * x\n\n# Launch four parallel square tasks.\nfutures = [square.remote(i) for i in range(4)]\n\n# Retrieve results.\nprint(ray.get(futures))\n# -> [0, 1, 4, 9]\n\n\n\n\nCalling an "", ""start_index"": 1279, ""end_index"": 1549}, {""content"": ""tion.\n\nHere\u2019s an example showing these techniques:\nimport numpy as np\n\n# Define a task that sums the values in a matrix.\n@ray.remote\ndef sum_matrix(matrix):\n    return np.sum(matrix)\n\n# Call the task with a literal argument value.\nprint(ray.get(sum_matrix.remote(np.ones((100, 100)))))\n# -> 10000.0\n\n# Put a large array into the object store.\nmatrix_ref = ray.put(np.ones((1000, 1000)))\n\n# Call the task with the object reference as an argument.\nprint(ray.get(sum_matrix.remote(matrix_ref)))\n# -> 1000000.0\n\n\n\n\nNext Steps#\n\nTip\nTo monitor your application\u2019s performance and resource usage"", ""start_index"": 3174, ""end_index"": 3762}]",datasets/raydocs_full/ray-core_walkthrough.txt
What happens when policies are no longer in the list or the callable returns False during a remove_policy operation?,"[{""content"": ""episode.\npolicies_to_train \u2013 An optional list of policy IDs to be trained\nor a callable taking PolicyID and SampleBatchType and\nreturning a bool (trainable or not?).\nIf None, will keep the existing setup in place. Policies,\nwhose IDs are not in the list (or for which the callable\nreturns "", ""start_index"": 748, ""end_index"": 1037}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.remove_policy.txt
What parameters are used in the Algorithm.remove_policy method?,"[{""content"": ""evaluation_workers=-1, remove_from_learners=-1) \u2192 None[source]#\nRemoves a policy from this Algorithm.\n\nParameters:\n\npolicy_id \u2013 ID of the policy to be removed.\npolicy_mapping_fn \u2013 An "", ""start_index"": 387, ""end_index"": 570}, {""content"": ""chType and\nreturning a bool (trainable or not?).\nIf None, will keep the existing setup in place. Policies,\nwhose IDs are not in the list (or for which the callable\nreturns False) will not be "", ""start_index"": 865, ""end_index"": 1056}, {""content"": """", ""start_index"": 1613, ""end_index"": 1802}, {""content"": """", ""start_index"": 2650, ""end_index"": 2832}, {""content"": """", ""start_index"": 3775, ""end_index"": 4067}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.remove_policy.txt
What are the default values for parameters in the Algorithm.remove_policy method?,"[{""content"": ""\n\nray.rllib.algorithms.algorithm.Algorithm.remove_policy#\n\n\nAlgorithm.remove_policy(policy_id: str = 'default_policy', *, policy_mapping_fn: Callable[[Any], str] | None = None, policies_to_train: Collection[str] | Callable[[str, SampleBatch | MultiAgentBatch | Dict[str, Any] | None], bool] | None = None, remove_from_env_runners: bool = True, remove_from_eval_env_runners: bool = True, evaluation_workers=-1, remove_from_learners=-1) \u2192 None[source]#\nRemoves a policy from this "", ""start_index"": 0, ""end_index"": 478}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.remove_policy.txt
What are the issues associated with using global variables in multiple processes as illustrated in the provided coding example?,"[{""content"": ""places where the state needs to be modified or accessed.\nNote that using class variables to manage state between instances of the same class is not supported.\nEach actor instance is instantiated in its own process, so each actor will have its own copy of the class variables.\n\nCode "", ""start_index"": 584, ""end_index"": 866}, {""content"": ""r):\n        self.global_var_actor = global_var_actor\n\n    def f(self):\n        return ray.get(self.global_var_actor.get_global_var.remote()) + 3\n\n\nglobal_var_actor = GlobalVarActor.remote()\nactor = Actor.remote(global_var_actor)\nray.get(global_var_actor.set_global_var.remote(4))\n# Th"", ""start_index"": 1547, ""end_index"": 1831}]",datasets/raydocs_full/ray-core_patterns_global-variables.txt
What is the recommended method to manage global variables across different processes in the given text?,"[{""content"": ""modify global variables\nin one process, changes are not reflected in other processes.\nThe solution is to use an actor\u2019s instance variables to hold the global state and pass the actor handle to places where the state needs to be modified or accessed.\nNote that using class variables to manage "", ""start_index"": 391, ""end_index"": 683}, {""content"": ""et(self.global_var_actor.get_global_var.remote()) + 3\n\n\nglobal_var_actor = GlobalVarActor.remote()\nactor = Actor.remote(global_var_actor)\nray.get(global_var_actor.set_global_var.remote(4))\n# This returns 7 correctly.\nassert ray.get(actor.f.remote()) == 7\n\n\n\n\n"", ""start_index"": 1638, ""end_index"": 2335}]",datasets/raydocs_full/ray-core_patterns_global-variables.txt
"How do Ray drivers, tasks, and actors operate concerning process allocation and address spaces?","[{""content"": ""an actor and pass the actor handle to other tasks and actors.\nRay drivers, tasks and actors are running in\ndifferent processes, so they don\u2019t share the same address space.\nThis means that if you "", ""start_index"": 196, ""end_index"": 391}]",datasets/raydocs_full/ray-core_patterns_global-variables.txt
What is the minimum version required to install Mars for use with Ray?,"[{""content"": ""reuse failover and\npipeline capabilities provided by ray futures.\n\nInstallation#\nYou can simply install Mars via pip:\npip install pymars>=0.8.3\n\n\n\n\nGetting started#\nIt\u2019s easy to run Mars jobs on "", ""start_index"": 483, ""end_index"": 678}]",datasets/raydocs_full/ray-more-libs_mars-on-ray.txt
What are the two ways to start a Mars on Ray runtime as described in the text?,"[{""content"": ""a Ray cluster.\nStarting a new Mars on Ray runtime locally via:\nimport ray\nray.init()\nimport mars\nmars.new_ray_session()\nimport mars.tensor as mt\nmt.random.RandomState(0).rand(1000_0000, 5).sum().execute()\n\n\nOr connecting to a Mars on Ray runtime which is already initialized:\nimport "", ""start_index"": 678, ""end_index"": 961}, {""content"": ""ct.readthedocs.io/en/latest/installation/ray.html#mars-ray for more information.\n\n\n"", ""start_index"": 1542, ""end_index"": 1732}]",datasets/raydocs_full/ray-more-libs_mars-on-ray.txt
How can a Ray Dataset be converted into a Mars DataFrame?,"[{""content"": ""dataset\nimport ray\n# ds = md.to_ray_dataset(df)\nds = ray.data.from_mars(df)\nprint(ds.schema(), ds.count())\nds.filter(lambda row: row[\""a\""] > 0.5).show(5)\n# Convert ray dataset to mars dataframe\n# df2 = md.read_ray_dataset(ds)\ndf2 = ds.to_mars()\nprint(df2.head(5).execute())\n\n\nRefer to Mars on "", ""start_index"": 1227, ""end_index"": 1519}]",datasets/raydocs_full/ray-more-libs_mars-on-ray.txt
What specific caution does Comet provide for users running experiments in a Jupyter environment?,"[{""content"": ""COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\nCOMET "", ""start_index"": 3081, ""end_index"": 3260}, {""content"": ""rics and code are logged before exiting.\nCOMET ERROR: The given API key abc is invalid, please check it against the dashboard. Your experiment would not be logged \nFor more details, please re"", ""start_index"": 3598, ""end_index"": 3789}, {""content"": ""e_since_restore: 0.000125885009765625\n  time_this_iter_s: 0.000125885009765625\n  time_total_s: 0.000125885009765625\n  timestamp: 1658500887\n  timesteps_since_restore: 0\n  training_iteration: "", ""start_index"": 4500, ""end_index"": 4691}]",datasets/raydocs_full/tune_examples_tune-comet.txt
What resources were requested during the scheduling process described in the text?,"[{""content"": ""GiBUsing FIFO scheduling algorithm.Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.5 GiB heap, "", ""start_index"": 2051, ""end_index"": 2146}]",datasets/raydocs_full/tune_examples_tune-comet.txt
"What are the results logged for train_function_5bf98_00000 including its hostname, process ID (pid), node IP, trial ID?","[{""content"": ""hostname: Kais-MacBook-Pro.local\n  iterations_since_restore: 1\n  loss: 1.1009860426725162\n  node_ip: 127.0.0.1\n  pid: 48140\n  time_since_restore: 0.000125885009765625\n  time_this_iter_s: 0.000125885009765625\n  time_total_s: 0.000125885009765625\n  timestamp: 1658500887\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: 5bf98_00000\n  warmup_time: "", ""start_index"": 4371, ""end_index"": 4732}]",datasets/raydocs_full/tune_examples_tune-comet.txt
What is the training iteration number for the experiment trial with trial_id a052a214?,"[{""content"": ""timesteps_since_restore: 0\n  timesteps_total: 0\n  training_iteration: 4\n  trial_id: a052a214\n  "", ""start_index"": 21085, ""end_index"": 21180}]",datasets/raydocs_full/tune_examples_bohb_example.txt
What are the set hyperparameters that minimized the mean loss for the objective previously defined?,"[{""content"": ""results.get_best_result().config)\n\n\n\n\nBest hyperparameters found were:  {'steps': 100, 'width': 3.7250202606878258, 'height': -57.97769618290691, 'activation': 'tanh'}\n\n\n\n\n\nOptional: Passing the "", ""start_index"": 28465, ""end_index"": 28660}]",datasets/raydocs_full/tune_examples_bohb_example.txt
What is the mean loss for the experiment conducted on 2022-07-22 for the trial_id identified as 2397442c?,"[{""content"": ""objective_2397442c:\n  date: 2022-07-22_15-11-27\n  done: false\n  episodes_total: 0\n  experiment_id: 1a4ebf62df50443492dc6df792fcb67a\n  hostname: Kais-MacBook-Pro.local\n  iterations: 0\n  iterations_since_restore: 1\n  mean_loss: 14.284216630043918\n  neg_mean_loss: -14.284216630043918\n  node_ip: 127.0.0.1\n  pid: 45401\n  time_since_restore: 0.1044008731842041\n  time_this_iter_s: 0.1044008731842041\n  time_total_s: 0.6395547389984131\n  timestamp: 1658499087\n  timesteps_since_restore: 0\n  timesteps_total: 0\n  training_iteration: 1\n  trial_id: 2397442c\n  "", ""start_index"": 50334, ""end_index"": 50886}]",datasets/raydocs_full/tune_examples_bohb_example.txt
What are the stability guarantees for v1 APIs in the KubeRay project?,"[{""content"": ""API reference.\n\nKubeRay API compatibility and guarantees#\nv1 APIs in the KubeRay project are stable and suitable for production environments.\nFields in the v1 APIs will never be removed to maintain "", ""start_index"": 189, ""end_index"": 387}, {""content"": ""lds removed from v1.\nHowever, KubeRay maintainers preserve the right to mark fields as deprecated and remove\nfunctionality associated with deprecated fields after a minimum of two minor release"", ""start_index"": 478, ""end_index"": 671}]",datasets/raydocs_full/cluster_kubernetes_references.txt
What steps can be taken to better understand RayCluster configuration?,"[{""content"": ""\n\nAPI Reference#\nTo learn about RayCluster configuration, we recommend taking a look at\nthe configuration guide.\nFor comprehensive coverage of all supported RayCluster fields,\nrefer to the "", ""start_index"": 0, ""end_index"": 189}, {""content"": ""configuration guide.\nFor comprehensive coverage of all supported RayCluster fields,\nrefer to the API reference.\n\nKubeRay API compatibility and guarantees#\nv1 APIs in the KubeRay project are stable "", ""start_index"": 92, ""end_index"": 289}]",datasets/raydocs_full/cluster_kubernetes_references.txt
What may trigger changes in the API behavior for KubeRay?,"[{""content"": ""remove\nfunctionality associated with deprecated fields after a minimum of two minor releases.\nIn addition, some definitions of the API may see small changes in behavior. For example,\nthe "", ""start_index"": 580, ""end_index"": 767}, {""content"": """", ""start_index"": 1257, ""end_index"": 1539}]",datasets/raydocs_full/cluster_kubernetes_references.txt
What is the dtype and shape of the 'image' column in the dataset schema after using read_tfrecords()?,"[{""content"": ""  ray.data.read_tfrecords(\n        \""s3://anonymous@air-example-data/cifar-10/tfrecords\""\n    )\n    .map(decode_bytes)\n)\n\nprint(ds.schema())\n\n\nColumn  Type\n------  ----\nimage   "", ""start_index"": 1739, ""end_index"": 1914}]",datasets/raydocs_full/data_working-with-images.txt
What methods are used to load images stored in different data formats?,"[{""content"": ""\n\n\n\n\n\nNumPy\nTo load images stored in NumPy format, call read_numpy().\nimport ray\n\nds = "", ""start_index"": 701, ""end_index"": 788}, {""content"": ""int64_list {\n                value: 3\n            }\n        }\n    }\n}\n\n\nTo load examples stored in this format, call read_tfrecords().\nThen, call map() to decode the raw image bytes.\nimport io\nfrom "", ""start_index"": 1282, ""end_index"": 1480}, {""content"": "" 0, 255)\n    return batch\n\nds = (\n    ray.data.read_images(\""s3://"", ""start_index"": 2594, ""end_index"": 2659}]",datasets/raydocs_full/data_working-with-images.txt
What are the common properties of images in the CIFAR-10 dataset as used in the examples read using ray.data.read_parquet()?,"[{""content"": ""in Parquet files, call ray.data.read_parquet().\nimport ray\n\nds = ray.data.read_parquet(\""s3://anonymous@air-example-data/cifar-10/parquet\"")\n\nprint(ds.schema())\n\n\nColumn  Type\n------  ----\nimage   numpy.ndarray(shape=(32, 32, 3), dtype=uint8)\nlabel   int64\n\n\n\n\nFor "", ""start_index"": 2013, ""end_index"": 2276}]",datasets/raydocs_full/data_working-with-images.txt
What happens if a user-defined schema is passed into input_read_schema?,"[{""content"": ""'unroll_id': 'unroll_id'}#\nThis is the default schema used if no input_read_schema is set in\nthe config. If a user passes in a schema into input_read_schema\nthis user-defined schema has to comply with the keys of SCHEMA,\nwhile values correspond to the columns in the user\u2019s dataset. Note\nthat only the user-defined values will be overridden while all\nother values from SCHEMA remain as defined here.\n\n\n"", ""start_index"": 380, ""end_index"": 866}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_prelearner.SCHEMA.txt
What entries are included in the default SCHEMA for ray.rllib.offline.offline_prelearner?,"[{""content"": ""\n\nray.rllib.offline.offline_prelearner.SCHEMA#\n\n\nray.rllib.offline.offline_prelearner.SCHEMA = {'actions': 'actions', 'agent_id': 'agent_id', 'agent_index': 'agent_index', 'dones': 'dones', 'eps_id': 'eps_id', 'infos': 'infos', 'module_id': 'module_id', 'new_obs': 'new_obs', 'obs': 'obs', 'rewards': 'rewards', 't': 't', 'terminateds': 'terminateds', 'truncateds': 'truncateds', 'unroll_id': 'unroll_id'}#\nThis is the default schema used if no input_read_schema is set in\nthe "", ""start_index"": 0, ""end_index"": 477}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_prelearner.SCHEMA.txt
What occurs if no input_read_schema is set in the config for ray.rllib.offline.offline_prelearner?,"[{""content"": ""'unroll_id': 'unroll_id'}#\nThis is the default schema used if no input_read_schema is set in\nthe "", ""start_index"": 380, ""end_index"": 477}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.offline.offline_prelearner.SCHEMA.txt
What types of colors are specified in the Color model?,"[{""content"": "" Color(BaseModel):\n    colors: List[Literal[\""cyan\"", \""magenta\"", \""yellow\""]]\n\n# Request structured "", ""start_index"": 11913, ""end_index"": 12009}]",datasets/raydocs_full/serve_llm_serving-llms.txt
What are the minimum and maximum replicas specified for the Qwen model deployment?,"[{""content"": ""autoscaling_config=dict(\n            min_replicas=1,\n            max_replicas=2,\n        )\n    ),\n  "", ""start_index"": 10510, ""end_index"": 10610}]",datasets/raydocs_full/serve_llm_serving-llms.txt
How can model weights be stored using remote storage options like S3 or GCS?,"[{""content"": ""flush=True)\n\n\n\n\n\n\nUsing remote storage for model weights#\nYou can use remote storage (S3 and GCS) to store your model weights instead of\ndownloading them from Hugging Face.\nFor example, if you have "", ""start_index"": 14250, ""end_index"": 14448}]",datasets/raydocs_full/serve_llm_serving-llms.txt
What are the fault tolerance features available for Ray generator tasks?,"[{""content"": ""tolerance#\nFault tolerance features work with\nRay generator tasks and actor tasks. For example;\n\nTask fault tolerance features: max_retries, retry_exceptions\nActor fault tolerance features: max_restarts, max_task_retries\nObject fault tolerance features: object reconstruction\n\n\n\nCancellation#\nThe ray.cancel() function works with both Ray generator tasks and "", ""start_index"": 6244, ""end_index"": 6603}]",datasets/raydocs_full/ray-core_ray-generator.txt
How can the Ray generator be utilized with asyncio?,"[{""content"": ""print(ray.get(ref))\n\n\n\n\n\nUsing the Ray generator with asyncio#\nThe returned ObjectRefGenerator is also compatible with asyncio. You can\nuse __anext__ or async for loops.\nimport "", ""start_index"": 5165, ""end_index"": 5342}, {""content"": ""s the next object reference immediately without blocking. See the example below for more details.\n@ray.remote\ndef task():\n    for i in range(5):\n        time.sleep(5)\n        yield i\n\ngen = task.remote()\n\n# Because it takes 5 seconds to make the first yield,\n# with 0 timeout, the generato"", ""start_index"": 8211, ""end_index"": 8500}]",datasets/raydocs_full/ray-core_ray-generator.txt
What are the different ways to handle Ray object references in a Python generator?,"[{""content"": ""range(5):\n        time.sleep(5)\n        yield i\n\n\n\nThe Ray generator task returns an ObjectRefGenerator object, which is\ncompatible with generator and async generator APIs. You can access the\nnext, __iter__, __anext__, __aiter__ APIs from the class.\nWhenever a task invokes yield, "", ""start_index"": 1909, ""end_index"": 2190}, {""content"": ""I blocks the thread until the task generates a next object reference with yield.\nSince the ObjectRefGenerator is just a Python generator, you can also use a for loop to\niterate object referenc"", ""start_index"": 2482, ""end_index"": 2674}, {""content"": ""ling#\nIf a generator task has a failure (by an application exception or system error such as an unexpected node failure),\nthe next(gen) returns an object reference that contains an excep"", ""start_index"": 3247, ""end_index"": 3433}, {""content"": ""ject reference that contains the system level exception\nat any time without an ordering guarantee.\nI"", ""start_index"": 4198, ""end_index"": 4298}]",datasets/raydocs_full/ray-core_ray-generator.txt
What are the advanced use cases for using Ray Core?,"[{""content"": ""Ray\n\n\n\n\n\nAdvanced#\n\n\nBuild Simple AutoML for Time Series Using Ray\n\nBuild Batch Prediction Using "", ""start_index"": 338, ""end_index"": 435}, {""content"": """", ""start_index"": 676, ""end_index"": 773}, {""content"": """", ""start_index"": 1111, ""end_index"": 1209}, {""content"": """", ""start_index"": 1546, ""end_index"": 1644}, {""content"": """", ""start_index"": 1981, ""end_index"": 2179}]",datasets/raydocs_full/ray-core_examples_overview.txt
What are some beginner use cases for using Ray Core?,"[{""content"": ""cases.\n\nBeginner#\n\n\nA Gentle Introduction to Ray Core by Example\n\nUsing Ray for Highly "", ""start_index"": 79, ""end_index"": 166}, {""content"": "" Highly Parallelizable Tasks\n\nMonte Carlo Estimation of \u03c0\n\n\n\n\n\nIntermediate#\n\n\nRunning a Simple MapReduce Example with Ray Core\n\nSpeed Up Your Web Crawler by Parallelizing it with Ray\n\n"", ""start_index"": 158, ""end_index"": 343}, {""content"": ""izing it with Ray\n\n\n\n\n\nAdvanced#\n\n\nBuild Simple AutoML for Time Series Using Ray\n\nBuild Batch Pred"", ""start_index"": 324, ""end_index"": 422}]",datasets/raydocs_full/ray-core_examples_overview.txt
What are some intermediate use cases for using Ray Core?,"[{""content"": ""Parallelizable Tasks\n\nMonte Carlo Estimation of \u03c0\n\n\n\n\n\nIntermediate#\n\n\nRunning a Simple MapReduce Example with Ray Core\n\nSpeed Up Your Web Crawler by Parallelizing it with "", ""start_index"": 166, ""end_index"": 338}, {""content"": ""sing Ray\n\nBuild a Simple Parameter Server Using Ray\n\nSimple Parallel Model Selection\n\nLearning to Play Pong\n\n\n\n\n\n"", ""start_index"": 430, ""end_index"": 601}]",datasets/raydocs_full/ray-core_examples_overview.txt
What signals can be used to gracefully terminate a Ray Tune training session?,"[{""content"": ""could result\nin resuming with stale state.\n\nRay Tune also accepts the SIGUSR1 signal to interrupt training gracefully. This\nshould be used when running Ray Tune in a remote Ray task\nas Ray will "", ""start_index"": 1592, ""end_index"": 1786}, {""content"": ""mplest way is to use metric-based criteria. These are a fixed set of thresholds that determine when the experiment should stop.\nYou can implement the stopping criteria using either "", ""start_index"": 1976, ""end_index"": 2157}]",datasets/raydocs_full/tune_tutorials_tune-stopping.txt
How can stopping criteria be implemented using a dictionary in Ray Tune?,"[{""content"": ""stopping criteria using either a dictionary, a function, or a custom Stopper.\n\n\n\nDictionary\nIf a dictionary is passed in, the keys may be any field in the return result of session.report in the\nFunction API or step() in the Class API.\n\nNote\nThis includes auto-filled metrics such as training_iteration.\n\nIn the example below, each trial will be stopped either when it completes 10 iterations or when it\nreaches a mean accuracy of 0.8 or more.\nThese metrics are assumed to be increasing, so the trial will stop once the reported metric has exceeded the threshold specified in the dictionary.\nfrom ray import tune\n\ntuner = tune.Tuner(\n    my_trainable,\n    "", ""start_index"": 2126, ""end_index"": 2781}]",datasets/raydocs_full/tune_tutorials_tune-stopping.txt
What conditions determine when a trial should be stopped in the provided Ray Tune example?,"[{""content"": ""trial should be stopped and False otherwise).\nIn the example below, each trial will be stopped either when it completes 10 iterations or when it\nreaches a mean accuracy of 0.8 or more.\nfrom ray "", ""start_index"": 3090, ""end_index"": 3284}]",datasets/raydocs_full/tune_tutorials_tune-stopping.txt
What does the function ray.rllib.utils.framework.try_import_torch return if torch is successfully imported?,"[{""content"": ""Whether to raise an error if torch cannot be imported.\n\nReturns:\nTuple consisting of the torch- AND torch.nn modules.\n\nRaises:\nImportError \u2013 If error=True and PyTorch is not installed.\n\n\n\n\n"", ""start_index"": 198, ""end_index"": 398}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.framework.try_import_torch.txt
Under which condition does the ray.rllib.utils.framework.try_import_torch function raise an ImportError?,"[{""content"": ""\n\nray.rllib.utils.framework.try_import_torch#\n\n\nray.rllib.utils.framework.try_import_torch(error: bool = False)[source]#\nTries importing torch and returns the module (or None).\n\nParameters:\nerror \u2013 Whether to raise an error if torch cannot be imported.\n\nReturns:\nTuple consisting of the torch- AND torch.nn modules.\n\nRaises:\nImportError \u2013 If error=True and PyTorch is not installed.\n\n\n\n\n"", ""start_index"": 0, ""end_index"": 398}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.framework.try_import_torch.txt
What parameter controls whether an error is raised in the ray.rllib.utils.framework.try_import_torch function if torch cannot be imported?,"[{""content"": ""bool = False)[source]#\nTries importing torch and returns the module (or None).\n\nParameters:\nerror \u2013 Whether to raise an error if torch cannot be imported.\n\nReturns:\nTuple consisting of the torch- AND "", ""start_index"": 98, ""end_index"": 298}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.framework.try_import_torch.txt
What is the topic of the provided text?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModule.model_config#\n\n\nRLModule.model_config#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule.model_config.txt
Where is the model_config referenced within the RLModule?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModule.model_config#\n\n\nRLModule.model_config#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule.model_config.txt
What information can be found regarding the RLModule model configuration?,"[{""content"": ""\n\nray.rllib.core.rl_module.rl_module.RLModule.model_config#\n\n\nRLModule.model_config#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.rl_module.RLModule.model_config.txt
What is the function referenced in the text?,"[{""content"": ""\n\nray.rllib.utils.numpy.concat_aligned#\n\n\nray.rllib.utils.numpy.concat_aligned(*args, **kwargs)#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.concat_aligned.txt
What method signature is described in the text?,"[{""content"": ""\n\nray.rllib.utils.numpy.concat_aligned#\n\n\nray.rllib.utils.numpy.concat_aligned(*args, **kwargs)#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.concat_aligned.txt
Which module does the function concat_aligned belong to?,"[{""content"": ""\n\nray.rllib.utils.numpy.concat_aligned#\n\n\nray.rllib.utils.numpy.concat_aligned(*args, **kwargs)#\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.numpy.concat_aligned.txt
What is the primary function of the Repeater in hyperparameter tuning?,"[{""content"": ""suggestions.\n\n\n\n\n\nRepeated Evaluations (tune.search.Repeater)#\nUse ray.tune.search.Repeater to average over multiple evaluations of the same\nhyperparameter configurations. This is useful in cases where the evaluated\ntraining procedure has high variance (i.e., in reinforcement "", ""start_index"": 4227, ""end_index"": 4504}]",datasets/raydocs_full/tune_api_suggestion.txt
What are the functionalities of the ConcurrencyLimiter in hyperparameter tuning?,"[{""content"": ""of same parameters.\n\n\n\n\n\nConcurrencyLimiter (tune.search.ConcurrencyLimiter)#\nUse ray.tune.search.ConcurrencyLimiter to limit the amount of concurrency when using a search algorithm.\nThis is useful when a given optimization algorithm does not parallelize very well (like "", ""start_index"": 4985, ""end_index"": 5256}]",datasets/raydocs_full/tune_api_suggestion.txt
How can Ray Tune algorithms' states be saved and reused during tuning?,"[{""content"": ""= tuner.fit()\n\n\n\nSaving and Restoring Tune Search Algorithms#\nCertain search algorithms have save/restore implemented,\nallowing reuse of searchers that are fitted on the results of multiple "", ""start_index"": 863, ""end_index"": 1053}, {""content"": ""   train_fn,\n    tune_config=tune.TuneConfig(\n        num_samples=5,\n        search_alg=search"", ""start_index"": 1948, ""end_index"": 2042}, {""content"": ""Optimization to improve the hyperparameter search.\nIt is available from the HpBandSter library.\nImpo"", ""start_index"": 3185, ""end_index"": 3285}]",datasets/raydocs_full/tune_api_suggestion.txt
How do DeploymentHandles manage multiple stages of a pipeline in Ray Serve?,"[{""content"": ""DeploymentHandle calls#\nRay Serve can directly pass the DeploymentResponse object that a DeploymentHandle returns, to another DeploymentHandle call to chain together multiple stages of a pipeline.\nYou don\u2019t need to await the first response, Ray Serve\nmanages the await behavior under the hood. When the first call finishes, Ray Serve passes the output of the first call, instead of the DeploymentResponse object, directly to the second call.\nFor example, the code sample below "", ""start_index"": 5642, ""end_index"": 6119}]",datasets/raydocs_full/serve_model_composition.txt
What are the primary functionalities of the LanguageClassifier and its associated responders in the Ray Serve example?,"[{""content"": ""ray.serve.handle import DeploymentHandle\n 4\n 5\n 6@serve.deployment\n 7class LanguageClassifer:\n 8    def __init__(\n 9        self, spanish_responder: DeploymentHandle, french_responder: DeploymentHandle\n10    ):\n11        self.spanish_responder = spanish_responder\n12        self.french_responder = french_responder\n13\n14    async def __call__(self, http_request):\n15        request = await http_request.json()\n16        language, name = request[\""language\""], request[\""name\""]\n17\n18        if language == \""spanish\"":\n19            response = self.spanish_responder.say_hello.remote(name)\n20        elif language == \""french\"":\n21            response = self.french_responder.say_hello.remote(name)\n22        else:\n23            return \""Please try again.\""\n24\n25        return await response\n26\n27\n28@serve.deployment\n29class SpanishResponder:\n30    def say_hello(self, name: str):\n31        return f\""Hola {name}\""\n32\n33\n34@serve.deployment\n35class FrenchResponder:\n36    def say_hello(self, name: str):\n37        return f\""Bonjour {name}\""\n38\n39\n40spanish_responder = SpanishResponder.bind()\n41french_responder = FrenchResponder.bind()\n42language_classifier = LanguageClassifer.bind(spanish_responder, french_responder)\n\n\nIn line 42, the LanguageClassifier deployment takes in the spanish_responder and french_responder as constructor arguments. At runtime, Ray Serve converts these arguments into DeploymentHandles. LanguageClassifier can then call the spanish_responder and french_responder\u2019s deployment methods using this handle.\nFor example, the LanguageClassifier\u2019s __call__ method uses the HTTP request\u2019s values to decide whether to respond in Spanish or French. It then forwards the request\u2019s name to the spanish_responder or the french_responder on lines 19 and 21 using the DeploymentHandles. The format of the calls is as "", ""start_index"": 1340, ""end_index"": 3161}, {""content"": ""ymentHandles. LanguageClassifier can then call the spanish_responder and french_responder\u2019s deployment methods using this handle.\nFor example, the LanguageClassifier\u2019s __call__ met"", ""start_index"": 2732, ""end_index"": 2912}, {""content"": ""ng for the remote method call to finish.\nYou should use awai"", ""start_index"": 4304, ""end_index"": 4364}]",datasets/raydocs_full/serve_model_composition.txt
What are the required steps to execute a DeploymentHandle call asynchronously using Ray Serve?,"[{""content"": ""number of arguments or keyword arguments here.\n\nThis call returns a DeploymentResponse object, which is a reference to the result, rather than the result itself.\nThis pattern allows the call to "", ""start_index"": 3547, ""end_index"": 3741}, {""content"": ""composition, you can avoid application-level bottlenecks when serving models and business logic steps that use different types and amounts of resources.\n\n\n\nChaining DeploymentHandle call"", ""start_index"": 5477, ""end_index"": 5663}]",datasets/raydocs_full/serve_model_composition.txt
What is the primary purpose of defining a grid sweep in the trial space for Ray Tune?,"[{""content"": ""\""other_data\"": ...}\n\n\nStep 2: Next, define the space of trials to run. Here, we define a simple grid sweep from 0..NUM_MODELS, which will generate the config dicts to be passed to each model function. "", ""start_index"": 1381, ""end_index"": 1581}]",datasets/raydocs_full/tune_tutorials_tune-run.txt
How is the status summary printed by Tune described?,"[{""content"": ""results. They will look something like this. Tune periodically prints a status summary to stdout "", ""start_index"": 3078, ""end_index"": 3175}]",datasets/raydocs_full/tune_tutorials_tune-run.txt
How does Ray Tune manage the parallelism based on resource availability?,"[{""content"": ""range(NUM_MODELS)\n    ])\n}\n\n\nStep 3: Optionally, configure the resources allocated per trial. Tune uses this resources allocation to control the parallelism. For example, if each trial was configured to use 4 CPUs, and the cluster had only 32 CPUs, then Tune will limit the number of concurrent trials to 8 to avoid overloading the cluster. For more information, see A Guide To "", ""start_index"": 2037, ""end_index"": 2415}]",datasets/raydocs_full/tune_tutorials_tune-run.txt
What are the functions of the ray.runtime_env.RuntimeEnv and ray.runtime_env.RuntimeEnvConfig classes?,"[{""content"": ""\n\nRuntime Env API#\n\n\nray.runtime_env.RuntimeEnvConfig\nUsed to specify configuration options for a runtime environment.\n\nray.runtime_env.RuntimeEnv\nThis class is used to define a runtime environment "", ""start_index"": 0, ""end_index"": 198}, {""content"": ""runtime environment.\n\nray.runtime_env.RuntimeEnv\nThis class is used to define a runtime environment for a job, task, or actor.\n\n\n\n\n"", ""start_index"": 98, ""end_index"": 298}]",datasets/raydocs_full/ray-core_api_runtime-env.txt
What is the purpose of the ray.runtime_env.RuntimeEnv class?,"[{""content"": ""runtime environment.\n\nray.runtime_env.RuntimeEnv\nThis class is used to define a runtime environment for a job, task, or actor.\n\n\n\n\n"", ""start_index"": 98, ""end_index"": 298}]",datasets/raydocs_full/ray-core_api_runtime-env.txt
What are the specific uses of ray.runtime_env.RuntimeEnvConfig?,"[{""content"": ""\n\nRuntime Env API#\n\n\nray.runtime_env.RuntimeEnvConfig\nUsed to specify configuration options for a runtime environment.\n\nray.runtime_env.RuntimeEnv\nThis class is used to define a runtime environment "", ""start_index"": 0, ""end_index"": 198}]",datasets/raydocs_full/ray-core_api_runtime-env.txt
What is the procedure for handling interruptions in the Ray Train job driver process?,"[{""content"": ""training.#\n\n\n\n\n\nJob Driver Fault Tolerance#\nJob driver fault tolerance is to handle cases where the Ray Train driver process is interrupted.\nThe Ray Train driver process is the process that calls trainer.fit() and is usually located on the head node of the cluster.\nThe driver process may be interrupted due to one of the following reasons:\n\nThe run is manually interrupted by a user (e.g., Ctrl+C).\nThe node where the driver process is running (head node) crashes (e.g., out of memory, out of disk).\nThe entire cluster goes down (e.g., network error affecting all nodes).\n\nIn these cases, "", ""start_index"": 5171, ""end_index"": 5761}, {""content"": ""e located at the storage path.\nRay Train fetches the latest checkpoint information from storage and passes it to the newly launched worker processes to resume training.\nTo find this run state, Ray Train relies on passing in the same RunConfig(storage_path, name) pair as the previous run.\nIf the storage_path or name do not match, Ray Train will not be able to find the previous run state and will start a new run from scratch.\n\nWarning\nIf name is reused unintentionally, Ray Train will f"", ""start_index"": 6028, ""end_index"": 6516}, {""content"": ""model.load_state_dict(torch.load(...))\n            ...\n\n    # [2] Checkpoint saving and reporting logic.\n    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n        # torch.save(...)\n        ray.train.report(\n            {\""loss\"": 0.1},\n            checkpoint=ray.train.Checkpoint.from_"", ""start_index"": 7275, ""end_index"": 7571}]",datasets/raydocs_full/train_user-guides_fault-tolerance.txt
What levels of fault tolerance does Ray Train provide?,"[{""content"": ""e deprecation and migration.\n\nRay Train provides fault tolerance at three levels:\n\nWorker process fault tolerance handles errors that happen to one or more Train worker processes while they are executing the user defined training function.\nWorker node fault tolerance handles node failures "", ""start_index"": 343, ""end_index"": 633}, {""content"": ""executing the user defined training function.\nWorker node fault tolerance handles node failures that may occur during training.\nJob driver fault tolerance handles the case where Ray Train driver "", ""start_index"": 537, ""end_index"": 732}, {""content"": ""user guide covers how to configure and use these fault tolerance mechanisms.\n\nWorker Process and Node Fault Tolerance#\nWorker process failures are errors that occur within the user defined training f"", ""start_index"": 827, ""end_index"": 1026}]",datasets/raydocs_full/train_user-guides_fault-tolerance.txt
How does Ray Train handle worker process failures during training?,"[{""content"": ""vides fault tolerance at three levels:\n\nWorker process fault tolerance handles errors that happen to one or more Train worker processes while they are executing the user defined training function.\nWorker node fault tolerance handles node failures that may occur during training.\nJob "", ""start_index"": 386, ""end_index"": 669}, {""content"": ""use these fault tolerance mechanisms.\n\nWorker Process and Node Fault Tolerance#\nWorker process failures are errors that occur within the user defined training function of a training worker,\nsuch "", ""start_index"": 866, ""end_index"": 1061}, {""content"": "" a worker process or node failure is considered a retry. The\nnumber of retries is configurable through the max_failures attribute of the\nFailureConfig argument set in the RunConfig\npassed to the Trainer. By default, worker fault tolerance is disabled with max_failures=0.\nimport ray.train\n\n# Tries"", ""start_index"": 1922, ""end_index"": 2219}]",datasets/raydocs_full/train_user-guides_fault-tolerance.txt
What are the API changes needed to run Tune with the class-based WandbTrainable?,"[{""content"": ""ne.uniform(0.2, 0.8),\n        },\n    )\n    tuner.fit()\n\n\n\n\nFinally, you can also define a class-based Tune Trainable by using the setup_wandb in the setup() method and storing the run "", ""start_index"": 3248, ""end_index"": 3432}, {""content"": ""object as an attribute. Please note that with the class trainable, you have to pass the trial id, "", ""start_index"": 3432, ""end_index"": 3530}, {""content"": ""n differs from tune_decorated above only in the first argument we pass to\nTuner():\n\n\ndef tune_trainable():\n    \""\""\""Example for using a WandTrainableMixin with the class API\""\""\""\n    tuner ="", ""start_index"": 4303, ""end_index"": 4489}]",datasets/raydocs_full/tune_examples_tune-wandb.txt
What information is provided regarding the final log in the Wandb session?,"[{""content"": ""        1.728275.2814  \n\n\n\n\n\n(train_function_wandb pid=14647) 2022-11-02 16:03:17,149\tINFO wandb.py:282 -- Already logged into W&B.\n\n\n\nTrial Progress\n\n\nTrial name                      date   "", ""start_index"": 9306, ""end_index"": 9497}]",datasets/raydocs_full/tune_examples_tune-wandb.txt
"What memory status is reported for the Tune system as of November 2nd, 2022?","[{""content"": ""Status\n\n\nCurrent time:2022-11-02 16:03:22\nRunning for: 00:00:08.49        \nMemory:      9.9/16.0 "", ""start_index"": 8448, ""end_index"": 8545}]",datasets/raydocs_full/tune_examples_tune-wandb.txt
What is the sequence of operations performed on an image during the prediction process in the given text?,"[{""content"": ""image = await self._image_downloader.remote(image_data.url)\n\n        # Preprocess image\n        "", ""start_index"": 13054, ""end_index"": 13150}, {""content"": ""ch.no_grad():\n            output = self.model(input_batch)\n\n        probabilities = torch.nn.fu"", ""start_index"": 13248, ""end_index"": 13343}, {""content"": ""top5_prob, top5_catid = torch.topk(probabilities, 5)\n        "", ""start_index"": 13624, ""end_index"": 13685}, {""content"": ""_bytes = requests.get(image_url).content\n        return Image.open(BytesIO(image_bytes)).conve"", ""start_index"": 14061, ""end_index"": 14155}]",datasets/raydocs_full/serve_advanced-guides_grpc-guide.txt
What is the output of the Streaming method in the GrpcDeployment class when invoked with a UserDefinedMessage?,"[{""content"": "" )\n        return user_response\n\n    def Streaming(\n        self, user_message: UserDefinedMessage\n    ) -> Generator[UserDefinedResponse, None, None]:\n        for i in range(10):\n            greeting = f\""{i}: Hello {user_message.name} from {user_message.origin}\""\n            num = user_message.num * 2 + i\n            user_response = UserDefinedResponse(\n                greeting=greeting,\n                num=num,\n            )\n            yield user_response\n\n         "", ""start_index"": 5871, ""end_index"": 6343}]",datasets/raydocs_full/serve_advanced-guides_grpc-guide.txt
What components are involved in the functionality of the ImageClassifier deployment?,"[{""content"": ""DeploymentHandle\n\n\n@serve.deployment\nclass ImageClassifier:\n    def __init__(\n        self,\n        _image_downloader: DeploymentHandle,\n        _data_preprocessor: DeploymentHandle,\n    ):\n        "", ""start_index"": 12141, ""end_index"": 12339}, {""content"": ""f.categories[top5_catid[i]])\n            image_probabilities.append(top5_prob[i].item())\n\n        re"", ""start_index"": 13756, ""end_index"": 13856}, {""content"": ""ion\"", \""app2\""),)  # Make sure application metadata is passed.\n\nresponse, call = stub.Predict"", ""start_index"": 15471, ""end_index"": 15562}]",datasets/raydocs_full/serve_advanced-guides_grpc-guide.txt
What key functions can Serve perform for large language model applications?,"[{""content"": ""developer\n\n\n\n\nServe enables you to rapidly prototype, develop, and deploy scalable LLM applications to production. Many large language model (LLM) applications combine prompt preprocessing, vector database lookups, LLM API calls, and response validation. Because Serve supports any arbitrary Python code, you can write all these steps as a single Python module, enabling rapid development and easy testing. You can then quickly deploy your Ray Serve LLM application to production, and each application step can independently autoscale to efficiently accommodate user traffic without wasting resources. In order to improve performance of your LLM applications, Ray Serve has features for batching and can integrate with any model optimization technique. Ray Serve also supports streaming responses, a key feature for chatbot-like applications.\n\n\n\nHow does Serve compare to "", ""start_index"": 9541, ""end_index"": 10413}]",datasets/raydocs_full/serve_index.txt
What are the advantages of using Ray Serve for machine learning applications?,"[{""content"": ""often important to combine machine learning with business logic and traditional web serving logic such as database queries.\nRay Serve is unique in that it allows you to build and deploy an end-to-end distributed serving application in a single framework.\nYou can combine multiple ML models, business logic, and expressive HTTP handling using Serve\u2019s FastAPI integration (see FastAPI HTTP Deployments) to build your entire application as one Python program.\n\n\n\nCombine multiple "", ""start_index"": 5425, ""end_index"": 5902}, {""content"": ""epts or cloud configurations to use Serve.\n\n\n\nML engineer\n\n\n\n\nServe helps you scale out your deployment and runs them reliably and efficiently to save costs. With Serve\u2019s first-class model composition API, you can combine models together with business logic and build end-to-end user-facing applications. Additionally, Serve runs natively on Kubernetes with minimal operation overhead.\n\n\n\nML platform engineer\n\n\n\n\nServe specializes in scalable and reliable ML model serving. As such, it can be an important plug-and-play component of your ML platform stack.\nServe supports arbitrary Python code and therefore integrates well with the MLOps ecosystem. You can use it with model optimizers (ONNX, TVM), model monitoring systems (Seldon Alibi, Arize), model registries (MLFlow, Weights and Biases), machine learning frameworks (XGBoost, Scikit-learn), data app UIs (Gradio, Streamli"", ""start_index"": 8611, ""end_index"": 9490}]",datasets/raydocs_full/serve_index.txt
How does Serve aid in model serving scalability and cost efficiency?,"[{""content"": ""number of built-in primitives to help make your ML serving application efficient.\nIt supports dynamically scaling the resources for a model up and down by adjusting the number of replicas, batching requests to take advantage of efficient vectorized operations (especially important on GPUs), and a flexible resource allocation model that enables you to serve many models on limited "", ""start_index"": 7145, ""end_index"": 7527}, {""content"": ""elps you scale out your deployment and runs them reliably and efficiently to save costs. With Serve\u2019s first-class model composition API, you can combine models together with business logic a"", ""start_index"": 8680, ""end_index"": 8870}, {""content"": "".\nCompared to these other offerings, Ray Serve lacks the functionality for\nmanaging the lifecycle of your models, visualizing their performance, etc. Ray\nServe primarily focuses on model serving and providing the primitives for you to\nbuild your own ML platform on top.\n\n\n\nSeldon, KServe, Cortex\n\n\n\n\nYou can develop Ray Serve on your laptop, deploy it on a dev box, and scale it out\nto "", ""start_index"": 11369, ""end_index"": 11755}]",datasets/raydocs_full/serve_index.txt
What can the return value of get_default_learner_class() be?,"[{""content"": ""input framework.\n\nReturns:\nThe Learner class to use for this algorithm either as a class type or as\na string (e.g. \u201cray.rllib.algorithms.ppo.ppo_learner.PPOLearner\u201d).\n\n\n\n\n"", ""start_index"": 294, ""end_index"": 491}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.txt
What purpose does the get_default_learner_class() serve in the context of AlgorithmConfig?,"[{""content"": "".get_default_learner_class() \u2192 Type[Learner] | str[source]#\nReturns the Learner class to use for this algorithm.\nOverride this method in the sub-class to return the Learner class type given\nthe "", ""start_index"": 100, ""end_index"": 294}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.txt
What is returned by the get_default_learner_class method in AlgorithmConfig?,"[{""content"": ""input framework.\n\nReturns:\nThe Learner class to use for this algorithm either as a class type or as\na string (e.g. \u201cray.rllib.algorithms.ppo.ppo_learner.PPOLearner\u201d).\n\n\n\n\n"", ""start_index"": 294, ""end_index"": 491}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.txt
What types of APIs does the Ray Workflows include?,"[{""content"": ""\n\nRay Workflows API#\n\n\nWorkflow Execution API\nWorkflow Management API\n\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/workflows_api_api.txt
What are the specific components of the Ray Workflows API?,"[{""content"": ""\n\nRay Workflows API#\n\n\nWorkflow Execution API\nWorkflow Management API\n\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/workflows_api_api.txt
What are the two primary features of the Ray Workflows API?,"[{""content"": ""\n\nRay Workflows API#\n\n\nWorkflow Execution API\nWorkflow Management API\n\n\n\n"", ""start_index"": 0, ""end_index"": 100}]",datasets/raydocs_full/workflows_api_api.txt
What does the MultiRLModule.setup function do?,"[{""content"": ""\n\nray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup#\n\n\nMultiRLModule.setup()[source]#\nSets up the underlying, individual RLModules.\n\n\n"", ""start_index"": 0, ""end_index"": 200}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.txt
What is the purpose of the MultiRLModule.setup function?,"[{""content"": ""\n\nray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup#\n\n\nMultiRLModule.setup()[source]#\nSets up the underlying, individual RLModules.\n\n\n"", ""start_index"": 0, ""end_index"": 200}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.txt
What occurs during the MultiRLModule.setup process?,"[{""content"": ""s up the underlying, individual RLModules.\n\n\n"", ""start_index"": 100, ""end_index"": 200}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.txt
What are the potential overheads associated with repeatedly starting and stopping a Ray cluster?,"[{""content"": ""tearDown(self):\n        ray.shutdown()\n\n\nHowever, starting and stopping a Ray cluster can actually incur a non-trivial amount of latency. For example, on a typical Macbook Pro laptop, starting and stopping can take nearly 5 seconds:\npython -c 'import ray; ray.init(); ray.shutdown()'  3.93s user 1.23s system 116% cpu 4.420 total\n\n\nAcross 20 tests, this ends up being 90 seconds of added "", ""start_index"": 1335, ""end_index"": 1723}]",datasets/raydocs_full/ray-contribute_testing-tips.txt
What are the key strategies discussed for handling resources in Ray testing?,"[{""content"": ""practices for Ray programs.\n\n\nTip 1: Fixing the resource quantity with ray.init(num_cpus=...)\nTip "", ""start_index"": 181, ""end_index"": 279}, {""content"": "" and GPUs on your local machine/cluster.\nHowever, your testing environment may have a significantly lower number of resources. For example, the TravisCI build environment only has 2 cores\nIf tests are written to depend on ray.init(), they may be implicitly written in a way that relies on a la"", ""start_index"": 557, ""end_index"": 850}, {""content"": ""ts are written to depend on ray.init(), they may be implicitly written in a way that r"", ""start_index"": 751, ""end_index"": 837}, {""content"": ""el global variables.\n\n\n\nTip 3: Create a mini-cluster with ray.cluster_utils.Cluster#\nIf writing an"", ""start_index"": 2389, ""end_index"": 2487}]",datasets/raydocs_full/ray-contribute_testing-tips.txt
What are the testing tips provided for Ray programs?,"[{""content"": ""for Ray programs.\n\n\nTip 1: Fixing the resource quantity with ray.init(num_cpus=...)\nTip 2: Sharing "", ""start_index"": 191, ""end_index"": 290}, {""content"": ""r\nTip 4: Be careful when running tests in parallel\n\n\n\nTip 1: Fixing the res"", ""start_index"": 389, ""end_index"": 464}, {""content"": "" number of resources. For example, the TravisCI build environment only has 2 cores\nIf tests are "", ""start_index"": 662, ""end_index"": 758}, {""content"": """", ""start_index"": 4385, ""end_index"": 4478}]",datasets/raydocs_full/ray-contribute_testing-tips.txt
How does the process_rewards function handle game boundaries?,"[{""content"": ""sum, since this was a game boundary (pong specific!).\n        if r[t] != 0:\n            running_add "", ""start_index"": 2988, ""end_index"": 3088}]",datasets/raydocs_full/ray-core_examples_plot_pong_example.txt
How are gradients applied to model parameters using RMSProp in the provided code?,"[{""content"": ""gradients to the model parameters with RMSProp.\""\""\""\n        for k, v in self.weights.items():\n       "", ""start_index"": 6143, ""end_index"": 6243}, {""content"": ""* 2\n            self.weights[k] += lr * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n\n\ndef zero_grads(gr"", ""start_index"": 6342, ""end_index"": 6440}, {""content"": ""nsible for taking a model and an env\nand performing a rollout + computing a gradient u"", ""start_index"": 6639, ""end_index"": 6725}]",datasets/raydocs_full/ray-core_examples_plot_pong_example.txt
"What are the defined roles of gamma, decay_rate, and learning_rate in the provided text?","[{""content"": ""neurons.\ngamma = 0.99  # The discount factor for reward.\ndecay_rate = 0.99  # The decay factor for "", ""start_index"": 1635, ""end_index"": 1734}, {""content"": ""MSProp leaky sum of grad^2.\nD = 80 * 80  # The input dimensionality: 80x80 grid.\nlearning_rate = 1e-4  # Magnitude of the update.\n\n\n\n\n\n\nHelper Functions#\nWe first define a few helper functions:\n\nPr"", ""start_index"": 1735, ""end_index"": 1932}, {""content"": ""rocessing: The preprocess function will\npreprocess the original 210x160x3 uint8 frame into a one-dimensional 6400\nfloat vector.\nReward Processing: The process_rewards function will cal"", ""start_index"": 1934, ""end_index"": 2118}]",datasets/raydocs_full/ray-core_examples_plot_pong_example.txt
What are the stability levels for PublicAPI in Ray?,"[{""content"": ""which are decorated/labeled accordingly.\nAn API can be labeled:\n\nPublicAPI, which means the API is exposed to end users. PublicAPI has three sub-levels (alpha, beta, stable), as described below.\nDeveloperAPI, which means the API is explicitly exposed to advanced Ray users and library "", ""start_index"": 98, ""end_index"": 383}]",datasets/raydocs_full/ray-contribute_stability.txt
What are the expectations for Ray's alpha and beta API components regarding stability and change?,"[{""content"": ""alpha component undergoes rapid iteration with a known set of users who\nmust be tolerant of change. The number of users should be a\ncurated, manageable set, such that it is feasible to communicate with all\nof them individually.\nBreaking changes must be both allowed and expected in alpha components, and\nusers must have no expectation of stability.\n\n\nBeta#\nA beta component must be "", ""start_index"": 581, ""end_index"": 963}, {""content"": ""fetime of the major\nAPI version. Because users expect such stability from components marked stable,\nthere must be no breaking changes to these components within a major version\n(excluding extraordinary circumstances).\n\nDocstrings#\n\n\nray.util.annotations.PublicAPI(*args, **kwargs)[source]#\nAnnotation for documenting public APIs.\nPublic APIs are classes and methods exposed to end users of Ray.\nIf stability=\""alpha\"", the API can be used by advanced users who are\ntolerant to and expect breaking changes.\nIf stability=\""beta\"", the API is still public and c"", ""start_index"": 1544, ""end_index"": 2098}]",datasets/raydocs_full/ray-contribute_stability.txt
How are deprecated Ray APIs managed?,"[{""content"": ""developers\nDeprecated, which may be removed in future releases of Ray.\n\nRay\u2019s PublicAPI stability "", ""start_index"": 383, ""end_index"": 481}, {""content"": ""eprecated\n>>> @Deprecated\n... def func(x):\n...     return x\n\n\n>>> @Deprecated(message=\""g() is deprecated because the API is error \""\n...   \""prone. Please call h() instead.\"")\n... def g(y):\n..."", ""start_index"": 3446, ""end_index"": 3636}, {""content"": """", ""start_index"": 6699, ""end_index"": 6891}]",datasets/raydocs_full/ray-contribute_stability.txt
What is the status of the new API stack introduced in Ray 2.40?,"[{""content"": ""\n\nEnv Utils#\n\nNote\nRay 2.40 uses RLlib\u2019s new API stack by default.\nThe Ray team has mostly "", ""start_index"": 0, ""end_index"": 91}, {""content"": ""completed transitioning algorithms, example scripts, and\ndocumentation to the new code base.\nIf "", ""start_index"": 91, ""end_index"": 187}]",datasets/raydocs_full/rllib_package_ref_env_utils.txt
What is the purpose of the functions try_import_open_spiel and try_import_pyspiel in RLlib?,"[{""content"": ""becoming stable.\n\ntry_import_open_spiel\nTries importing open_spiel and returns the module (or None).\n\ntry_import_pyspiel\nTries importing pyspiel and returns the module (or None).\n\n\n\n\n\n"", ""start_index"": 380, ""end_index"": 574}, {""content"": """", ""start_index"": 854, ""end_index"": 954}]",datasets/raydocs_full/rllib_package_ref_env_utils.txt
What instructions exist for users still on Ray's old API stack?,"[{""content"": ""you\u2019re still using the old API stack, see New API stack migration guide for details on how to migrate.\n\n\nrllib.env.utils#\n\n\nRLlink\nPublicAPI (alpha): This API is in alpha and may change before "", ""start_index"": 187, ""end_index"": 380}]",datasets/raydocs_full/rllib_package_ref_env_utils.txt
What commands are available for managing Ray Clusters on VMs?,"[{""content"": ""Commands\nLaunching a cluster (ray up)\nUpdating an existing cluster (ray up)\nRunning shell commands "", ""start_index"": 182, ""end_index"": 281}, {""content"": "" to a running cluster (ray attach)\nSynchronizing files from the cluster (ray rsync-up/down)\nMonitor"", ""start_index"": 364, ""end_index"": 463}, {""content"": """", ""start_index"": 645, ""end_index"": 742}, {""content"": """", ""start_index"": 926, ""end_index"": 1023}, {""content"": """", ""start_index"": 1304, ""end_index"": 1401}]",datasets/raydocs_full/cluster_vms_references_index.txt
What functionalities does the Cluster Launcher provide for Ray Clusters on VMs?,"[{""content"": ""virtual machines.\n\nReference documentation for Ray Clusters on VMs:\n\nCluster Launcher Commands\nLaunching a cluster (ray up)\nUpdating an existing cluster (ray up)\nRunning shell commands "", ""start_index"": 96, ""end_index"": 281}, {""content"": ""ds on the cluster (ray exec)\nRunning Ray scripts on the cluster (ray submit)\nAttaching to a running"", ""start_index"": 278, ""end_index"": 377}, {""content"": ""toring cluster status (ray dashboard/status)\nCommon Workflow: Syncing git branches\n\n\nCluster YAML Configuration Options\nSyntax\nCustom types\nProperties and Definitions\nExamples\n\n\n\n\n\n"", ""start_index"": 460, ""end_index"": 656}, {""content"": """", ""start_index"": 741, ""end_index"": 838}, {""content"": """", ""start_index"": 1022, ""end_index"": 1216}]",datasets/raydocs_full/cluster_vms_references_index.txt
What are the key activities that can be performed on a Ray Cluster using VMs?,"[{""content"": ""Commands\nLaunching a cluster (ray up)\nUpdating an existing cluster (ray up)\nRunning shell commands "", ""start_index"": 182, ""end_index"": 281}, {""content"": "" to a running cluster (ray attach)\nSynchronizing files from the cluster (ray rsync-up/down)\nMonitor"", ""start_index"": 364, ""end_index"": 463}, {""content"": ""luster YAML Configuration Options\nSyntax\nCustom types\nProperties and Definitions\nExamples\n\n\n\n\n\n"", ""start_index"": 546, ""end_index"": 742}, {""content"": """", ""start_index"": 827, ""end_index"": 924}, {""content"": """", ""start_index"": 1108, ""end_index"": 1302}]",datasets/raydocs_full/cluster_vms_references_index.txt
What are the default settings for the MultiAgentReplayBuffer parameters?,"[{""content"": ""ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer(capacity: int = 10000, storage_unit: str = 'timesteps', num_shards: int = 1, replay_mode: str = 'independent', "", ""start_index"": 91, ""end_index"": 282}, {""content"": "" = 0, replay_zero_init_states: bool = True, underlying_buffer_config: dict = None, **kwargs)[source]#\nBases: ReplayBuffer\nA replay buffer shard for multiagent setups.\nThis bu"", ""start_index"": 373, ""end_index"": 547}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.txt
What are the primary functions of the MultiAgentReplayBuffer class methods?,"[{""content"": ""instance.\n\nadd\nAdds a batch to the appropriate policy's replay buffer.\n\napply\nCalls the given "", ""start_index"": 828, ""end_index"": 922}, {""content"": """", ""start_index"": 1750, ""end_index"": 1826}, {""content"": """", ""start_index"": 2847, ""end_index"": 2945}, {""content"": """", ""start_index"": 4042, ""end_index"": 4142}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.txt
How is the MultiAgentReplayBuffer designed to facilitate parallel operations?,"[{""content"": ""**kwargs)[source]#\nBases: ReplayBuffer\nA replay buffer shard for multiagent setups.\nThis buffer is meant to be run in parallel to distribute experiences\nacross num_shards shards. Unlike simpler "", ""start_index"": 456, ""end_index"": 650}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.txt
What are the responsibilities of the Algorithm class in RLlib?,"[{""content"": ""migrate.\n\nThe Algorithm class is the highest-level API in RLlib responsible for WHEN and WHAT of RL "", ""start_index"": 282, ""end_index"": 382}, {""content"": "" to train and evaluate policies, save an experiment\u2019s progress and restore from\na prior saved"", ""start_index"": 664, ""end_index"": 757}]",datasets/raydocs_full/rllib_package_ref_algorithm.txt
How can users interact with RLlib's algorithms using the Algorithm class?,"[{""content"": ""network update, and so on.\nThe HOW will be delegated to components such as RolloutWorker, etc..\nIt is the main entry point for RLlib users to interact with RLlib\u2019s algorithms.\nIt allows you to train "", ""start_index"": 475, ""end_index"": 674}, {""content"": ""er.#\n\n\n\nBuilding Custom Algorithm Classes#\n\nWarning\nAs of Ray >= 1.9, it is no longer recommended to"", ""start_index"": 1149, ""end_index"": 1249}]",datasets/raydocs_full/rllib_package_ref_algorithm.txt
What are the key changes to building custom Algorithm classes as recommended from Ray version 1.9 onwards?,"[{""content"": ""Custom Algorithm Classes#\n\nWarning\nAs of Ray >= 1.9, it is no longer recommended to use the build_trainer() utility\nfunction for creating custom Algorithm sub-classes.\nInstead, follow the simple guidelines here for directly sub-classing from\nAlgorithm.\n\nIn order to create a custom "", ""start_index"": 1166, ""end_index"": 1448}]",datasets/raydocs_full/rllib_package_ref_algorithm.txt
What features does the OfflinePreLearner provide for data handling?,"[{""content"": ""config.\n\nOfflinePreLearner.__call__\nPrepares plain data batches for training with Learner's.\n\nOfflinePreLearner._map_to_episodes\nMaps a batch of data to "", ""start_index"": 1294, ""end_index"": 1447}, {""content"": """", ""start_index"": 2670, ""end_index"": 2836}, {""content"": """", ""start_index"": 4117, ""end_index"": 4299}]",datasets/raydocs_full/rllib_package_ref_offline.txt
What changes does Ray 2.40 introduce regarding its API?,"[{""content"": ""\n\nOffline RL API#\n\nNote\nRay 2.40 uses RLlib\u2019s new API stack by default.\nThe Ray team has mostly completed transitioning algorithms, example scripts, and\ndocumentation to the new code base.\nIf "", ""start_index"": 0, ""end_index"": 192}]",datasets/raydocs_full/rllib_package_ref_offline.txt
What configurations does AlgorithmConfig allow for Offline RL?,"[{""content"": ""migrate.\n\n\nConfiguring Offline RL#\n\n\nAlgorithmConfig.offline_data\nSets the config's offline data settings.\n\nAlgorithmConfig.learners\nSets LearnerGroup and Learner worker related "", ""start_index"": 286, ""end_index"": 464}, {""content"": ""e environment runner to record the single agent case.\n\n\n\n\n\nConstructing OfflineData#\n\n\nOfflineData\nPublicAPI (alpha): This API is in alpha and may change before becoming stable.\n\n"", ""start_index"": 669, ""end_index"": 848}, {""content"": ""r.\n\nOfflinePreLearner.__init__\n\n\n\n\n\n\nTransforming Data with an OfflinePreLearner#\n\n\nSCHEMA\nThis is the default schema used if no input_read_schema is set in the c"", ""start_index"": 1133, ""end_index"": 1295}]",datasets/raydocs_full/rllib_package_ref_offline.txt
What does the method Algorithm.train() return after execution?,"[{""content"": ""training\nprocess.\nnode_ip (str): Node ip of the machine hosting the training\nprocess.\n\n\nReturns:\nA dict that describes training progress.\n\n\n\n\n"", ""start_index"": 942, ""end_index"": 1141}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.train.txt
What fields are automatically filled by the Algorithm.train() method?,"[{""content"": ""results.\nThis method automatically fills the following fields in the result:\n\ndone (bool): training is terminated. Filled only if not provided.\ntime_this_iter_s (float): Time in seconds this iteration\ntook to run. This may be overridden in order to override the\nsystem-computed time "", ""start_index"": 188, ""end_index"": 471}, {""content"": ""emented\nafter step() is called.\npid (str): The pid of the training process.\ndate (str): A formatted date of when the result was processed.\ntimestamp (str): A UNIX timestamp of "", ""start_index"": 659, ""end_index"": 835}, {""content"": """", ""start_index"": 1306, ""end_index"": 1700}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.train.txt
What specific method does Algorithm.train() call internally during a training iteration?,"[{""content"": ""of training.\nCalls step() internally. Subclasses should override step()\ninstead to return "", ""start_index"": 98, ""end_index"": 188}]",datasets/raydocs_full/rllib_package_ref_doc_ray.rllib.algorithms.algorithm.Algorithm.train.txt
