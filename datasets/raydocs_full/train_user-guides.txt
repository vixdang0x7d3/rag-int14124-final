

Ray Train User Guides#


Data Loading and Preprocessing
Quickstart
Starting with PyTorch data
Splitting datasets
Random shuffling
Enabling reproducibility
Preprocessing structured data
Performance tips


Configuring Scale and GPUs
Increasing the number of workers
Using GPUs
Setting the resources per worker
(Deprecated) Trainer resources


Configuring Persistent Storage
Cloud storage (AWS S3, Google Cloud Storage)
Shared filesystem (NFS, HDFS)
Local storage
Custom storage
Overview of Ray Train outputs
Advanced configuration
Deprecated


Monitoring and Logging Metrics
How to obtain and aggregate results from different workers?
(Deprecated) Reporting free-floating metrics


Saving and Loading Checkpoints
Saving checkpoints during training
Configure checkpointing
Using checkpoints after training
Restore training state from a checkpoint


Experiment Tracking
Getting Started
Examples
Common Errors


Inspecting Training Results
Viewing metrics
Retrieving checkpoints
Accessing storage location
Viewing Errors
Finding results on persistent storage


Handling Failures and Node Preemption
Worker Process and Node Fault Tolerance
Job Driver Fault Tolerance
Fault Tolerance API Deprecations


Reproducibility
Hyperparameter Optimization
Quickstart
What does Ray Tune provide?
Configuring resources for multiple trials
Reporting metrics and checkpoints
Tuner(trainer) API Deprecation





